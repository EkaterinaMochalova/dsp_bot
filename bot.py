# ====== imports ======
import os, io, math, asyncio, logging, time, json, ssl, re
from pathlib import Path
from datetime import datetime
import random
from typing import Any
from geo_ai import find_poi_ai

import pandas as pd
import aiohttp

try:
    import certifi  # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
except Exception:
    certifi = None

# aiogram 3.x
from aiogram import Bot, Dispatcher, F, types, Router
from aiogram.types import Message, BufferedInputFile, BotCommand
from aiogram.filters import Command

# ====== logging ======
logging.basicConfig(level=logging.INFO)

# ====== ENV CONFIG ======
OBDSP_BASE = os.getenv("OBDSP_BASE", "https://obdsp.projects.eraga.net").strip()
OBDSP_TOKEN = os.getenv("OBDSP_TOKEN", "").strip()
OBDSP_AUTH_SCHEME = os.getenv("OBDSP_AUTH_SCHEME", "Bearer").strip()
OBDSP_CLIENT_ID = os.getenv("OBDSP_CLIENT_ID", "").strip()

try:
    TELEGRAM_OWNER_ID = int(os.getenv("TELEGRAM_OWNER_ID", "0"))
except Exception:
    TELEGRAM_OWNER_ID = 0

OBDSP_CA_BUNDLE = os.getenv("OBDSP_CA_BUNDLE", "").strip()
OBDSP_SSL_VERIFY = (os.getenv("OBDSP_SSL_VERIFY", "1") or "1").strip().lower()
OBDSP_SSL_NO_VERIFY = os.getenv("OBDSP_SSL_NO_VERIFY", "0").strip().lower() in {"1", "true", "yes", "on"}

# ====== PATHS & STATE ======
BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR / "data"
DATA_DIR.mkdir(parents=True, exist_ok=True)

# –ö—ç—à-–∫–∞—Ç–∞–ª–æ–≥ (–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å env-–ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π)
CACHE_DIR = Path(os.getenv("SCREENS_CACHE_DIR", "/tmp/omnika_cache"))
CACHE_DIR.mkdir(parents=True, exist_ok=True)

CACHE_CSV  = CACHE_DIR / "screens_cache.csv"
CACHE_META = CACHE_DIR / "screens_cache.meta.json"

SCREENS: pd.DataFrame | None = None
LAST_RESULT: pd.DataFrame | None = None
LAST_SELECTION_NAME = "last"
MAX_PLAYS_PER_HOUR = 6
LAST_SYNC_TS: float | None = None

# –ì–µ–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∏
DEFAULT_RADIUS: float = 2.0
USER_RADIUS: dict[int, float] = {}
PLAN_MAX_PLAYS_PER_HOUR = 40  # –ª–∏–º–∏—Ç –ø–æ–∫–∞–∑–æ–≤ –≤ —á–∞—Å –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è

# ===== Places / Geocoding config =====
GEOCODER_PROVIDER = (os.getenv("GEOCODER_PROVIDER") or "nominatim").lower()
GOOGLE_PLACES_KEY = os.getenv("GOOGLE_PLACES_KEY") or ""   # –µ—Å–ª–∏ –∑–∞—Ö–æ—á–µ—à—å Google
YANDEX_API_KEY    = os.getenv("YANDEX_API_KEY") or ""      # –µ—Å–ª–∏ –∑–∞—Ö–æ—á–µ—à—å –Ø–Ω–¥–µ–∫—Å
D2GIS_API_KEY     = os.getenv("D2GIS_API_KEY") or ""       # –µ—Å–ª–∏ –∑–∞—Ö–æ—á–µ—à—å 2–ì–ò–°

# --- Geo providers ---
import aiohttp, asyncio, json

OVERPASS_ENDPOINTS = [
    "https://overpass-api.de/api/interpreter",
    "https://overpass.kumi.systems/api/interpreter",
    "https://overpass.openstreetmap.ru/api/interpreter",
]
NOMINATIM_URL = "https://nominatim.openstreetmap.org/search"

# –ø–æ—Å–ª–µ–¥–Ω–µ–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ POI (–¥–ª—è /near_geo –±–µ–∑ —Ç–µ–∫—Å—Ç–∞)
LAST_POI: list[dict] = []

# ====== –ú–µ–Ω—é –∏ help ======
HELP = (
    "–ü—Ä–∏–≤–µ—Ç ‚ù§Ô∏è –Ø –ø–æ–º–æ–≥–∞—é –ø–æ–¥–±–∏—Ä–∞—Ç—å —Ä–µ–∫–ª–∞–º–Ω—ã–µ —ç–∫—Ä–∞–Ω—ã –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∫–∞–∑—ã.\n\n"
    "üìÑ –ß—Ç–æ–±—ã –±—ã–ª–æ –∏–∑ —á–µ–≥–æ –≤—ã–±–∏—Ä–∞—Ç—å, –æ—Ç–ø—Ä–∞–≤—å –º–Ω–µ —Ñ–∞–π–ª CSV/XLSX c –∫–æ–ª–æ–Ω–∫–∞–º–∏ –º–∏–Ω–∏–º—É–º: lat, lon.\n"
    "   –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è: screen_id, name, city, format, owner, minBid / min_bid.\n\n"

    "üí¨ –ü–æ–ø—Ä–æ–±—É–π –ø—Ä–æ—Å—Ç–æ —Å–ø—Ä–æ—Å–∏—Ç—å:\n"
    "   ‚Äî ¬´–ü–æ–¥–±–µ—Ä–∏ 10 —ç–∫—Ä–∞–Ω–æ–≤ –≤ –ú–æ—Å–∫–≤–µ¬ª\n"
    "   ‚Äî ¬´–°–ø–ª–∞–Ω–∏—Ä—É–π –∫–∞–º–ø–∞–Ω–∏—é –Ω–∞ 30 –±–∏–ª–±–æ—Ä–¥–∞—Ö –≤ –ú–æ—Å–∫–≤–µ, 7 –¥–Ω–µ–π, –±—é–¥–∂–µ—Ç 250000¬ª\n"
    "   ‚Äî ¬´–•–æ—á—É –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å 20 —Ñ–∞—Å–∞–¥–æ–≤ –≤ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–µ\n"
    "   –Ø –ø–æ—Å—Ç–∞—Ä–∞—é—Å—å –ø–æ–¥—Å–∫–∞–∑–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â—É—é –∫–æ–º–∞–Ω–¥—É.\n\n"

    "‚öôÔ∏è –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:\n"
    "‚Ä¢ /status ‚Äî —á—Ç–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏ —Å–∫–æ–ª—å–∫–æ —ç–∫—Ä–∞–Ω–æ–≤\n"
    "‚Ä¢ /radius 2 ‚Äî –∑–∞–¥–∞—Ç—å —Ä–∞–¥–∏—É—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–∫–º)\n"
    "‚Ä¢ /cache_info ‚Äî –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫—ç—à–∞\n"
    "‚Ä¢ /sync_api [—Ñ–∏–ª—å—Ç—Ä—ã] ‚Äî –ø–æ–¥—Ç—è–Ω—É—Ç—å –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å –∏–∑ API (–µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è)\n"
    "–ù–∞–ø—Ä–∏–º–µ—Ä: /sync_api city=–ú–æ—Å–∫–≤–∞ ‚Äî –ø–æ–¥—Ç—è–Ω—É—Ç—å —ç–∫—Ä–∞–Ω—ã –∏–∑ API —Ç–æ–ª—å–∫–æ –ø–æ –ú–æ—Å–∫–≤–µ\n"
    "‚Ä¢ /export_last ‚Äî –≤—ã–≥—Ä—É–∑–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω—é—é –≤—ã–±–æ—Ä–∫—É (CSV)\n\n"
    
    "üîé –í—ã–±—Ä–∞—Ç—å —ç–∫—Ä–∞–Ω—ã:\n"
    "‚Ä¢ /near <lat> <lon> [R] [filters] [fields=...] ‚Äî —ç–∫—Ä–∞–Ω—ã –≤ —Ä–∞–¥–∏—É—Å–µ\n"
    "–ù–∞–ø—Ä–∏–º–µ—Ä: /near 55.714349 37.553834 2 ‚Äî –≤—Å—ë –≤ —Ä–∞–¥–∏—É—Å–µ 2 –∫–º\n"
    "‚Ä¢ /pick_city <–ì–æ—Ä–æ–¥> <N> [filters] [mix=...] [fields=...] ‚Äî —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –ø–æ –≥–æ—Ä–æ–¥—É\n"
    "–ù–∞–ø—Ä–∏–º–µ—Ä: /pick_city –ú–æ—Å–∫–≤–∞ 20 format=billboard,supersite ‚Äî 20 –ë–ë –∏ –°–° –≤ –ú–æ—Å–∫–≤–µ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ\n"
    "‚Ä¢ /pick_at <lat> <lon> <N> [R] ‚Äî —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –≤ –∫—Ä—É–≥–µ\n\n"

    "üìä –ü—Ä–æ–≥–Ω–æ–∑—ã –∏ –ø–ª–∞–Ω—ã:\n"
    "‚Ä¢ /forecast [budget=...] [days=7] [hours_per_day=8] [hours=07-10,17-21]\n"
    "–ù–∞–ø—Ä–∏–º–µ—Ä: /forecast days=14 hours_per_day=10 ‚Äî –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ –±—é–¥–∂–µ—Ç—É –Ω–∞ 14 –¥–Ω–µ–π\n"
    "‚Ä¢ /plan budget=<—Å—É–º–º–∞> [city=...] [format=...] [owner=...] [n=...] [days=...] [hours_per_day=...] [top=1] ‚Äî —Å–ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –∫–∞–º–ø–∞–Ω–∏—é –ø–æ–¥ –±—é–¥–∂–µ—Ç\n"
    "–ù–∞–ø—Ä–∏–º–µ—Ä: /plan budget=200000 city=–ú–æ—Å–∫–≤–∞ n=10 days=10 hours_per_day=8 ‚Äî —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –≤—ã–±—Ä–∞—Ç—å 10 —ç–∫—Ä–∞–Ω–æ–≤ –∏ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å —Å–ª–æ—Ç—ã\n\n"

    "üß≠ –ü–æ–∏—Å–∫ —Ç–æ—á–µ–∫ –Ω–∞ –∫–∞—Ä—Ç–µ –∏ –ø–æ–¥–±–æ—Ä —Ä—è–¥–æ–º:\n"
    "‚Ä¢ /geo <–∑–∞–ø—Ä–æ—Å> [city=...] [limit=5] ‚Äî –Ω–∞–π—Ç–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É\n"
    "   –ü—Ä–∏–º–µ—Ä—ã:\n"
    "   /geo –¢–≤–æ–π –¥–æ–º city=–ú–æ—Å–∫–≤–∞\n"
    "   /geo Burger King city=–ú–æ—Å–∫–≤–∞ limit=15"
    "‚Ä¢ /near_geo [R] [fields=...] ‚Äî –ø–æ–¥–æ–±—Ä–∞—Ç—å —ç–∫—Ä–∞–Ω—ã –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–æ—á–µ–∫\n"
    "   –ü—Ä–∏–º–µ—Ä—ã:\n"
    "   /near_geo 2\n"
    "   /near_geo 1.5 fields=screen_id\n"
    "   /near_geo 2 query=\"–¢–≤–æ–π –¥–æ–º\" city=–ú–æ—Å–∫–≤–∞ limit=5\n\n"

    "üî§ –ö–∞–∫–∏–µ –µ—â—ë —Ñ–∏–ª—å—Ç—Ä—ã –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:\n"
    "   ‚Ä¢ format=billboard ‚Äî —Ç–æ–ª—å–∫–æ –ë–ë\n"
    "   ‚Ä¢ format=billboard,supersite | billboard;supersite | billboard|supersite ‚Äî –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–æ—Ä–º–∞—Ç–æ–≤\n"
    "   ‚Ä¢ owner=russ | owner=–†–ò–ú,–ü–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞ ‚Äî —Ñ–∏–ª—å—Ç—Ä –ø–æ –≤–ª–∞–¥–µ–ª—å—Ü—É (–ø–æ–¥—Å—Ç—Ä–æ–∫–∞, –±–µ–∑ —É—á—ë—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞)\n"
    "   ‚Ä¢ fields=screen_id | screen_id,format ‚Äî –∫–∞–∫–∏–µ –ø–æ–ª—è –≤—ã–≤–æ–¥–∏—Ç—å\n\n"
)

from aiogram.types import ReplyKeyboardMarkup, KeyboardButton

def _extract_screen_ids(frame: pd.DataFrame) -> list[str]:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –¥–æ—Å—Ç–∞—ë—Ç —Å–ø–∏—Å–æ–∫ screen_id –¥–∞–∂–µ –ø—Ä–∏ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö."""
    if "screen_id" not in frame.columns:
        return []
    ser = frame["screen_id"]
    if isinstance(ser, pd.DataFrame):   # –Ω–∞ —Å–ª—É—á–∞–π –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∫–æ–ª–æ–Ω–æ–∫
        ser = ser.iloc[:, 0]
    return [s for s in ser.astype(str).tolist() if s and s.lower() != "nan"]

def make_main_menu() -> ReplyKeyboardMarkup:
    return ReplyKeyboardMarkup(
        keyboard=[
            [KeyboardButton(text="/help"), KeyboardButton(text="/status"), KeyboardButton(text="/start")],
        ],
        resize_keyboard=True,
        input_field_placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: /pick_city –ú–æ—Å–∫–≤–∞ 20 ‚Äî —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ 20 —ç–∫—Ä–∞–Ω–æ–≤ –ø–æ –≥–æ—Ä–æ–¥—É"
    )

# ====== –ö—ç—à-–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ ======
def _cache_diag() -> str:
    try:
        can_write_dir = os.access(CACHE_DIR, os.W_OK)
        parent = CACHE_DIR.parent
        return (
            f"BASE_DIR={BASE_DIR} | CACHE_DIR={CACHE_DIR} "
            f"| exists={CACHE_DIR.exists()} | writable={can_write_dir} "
            f"| parent_writable={os.access(parent, os.W_OK)}"
        )
    except Exception as e:
        return f"diag_error={e}"

def save_screens_cache(df: pd.DataFrame) -> bool:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫—ç—à –Ω–∞ –¥–∏—Å–∫ (CSV + meta)."""
    global LAST_SYNC_TS
    try:
        if df is None or df.empty:
            logging.warning("save_screens_cache: –ø—É—Å—Ç–æ–π df ‚Äî —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –Ω–µ—á–µ–≥–æ")
            return False

        # —Ç–µ—Å—Ç –∑–∞–ø–∏—Å–∏
        try:
            (CACHE_DIR / ".write_test").write_text("ok", encoding="utf-8")
        except Exception as e:
            logging.error(f"write_test failed: {e} | {_cache_diag()}")
            return False

        df.to_csv(CACHE_CSV, index=False, encoding="utf-8-sig")

        LAST_SYNC_TS = time.time()
        meta = {"ts": LAST_SYNC_TS, "rows": int(len(df))}
        CACHE_META.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")

        logging.info(f"üíæ –ö—ç—à —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {len(df)} —Å—Ç—Ä–æ–∫ ‚Üí {CACHE_CSV} | {_cache_diag()}")
        return True
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫—ç—à–∞: {e} | {_cache_diag()}", exc_info=True)
        return False

def load_screens_cache() -> bool:
    """–ü—ã—Ç–∞–µ—Ç—Å—è –ø–æ–¥–Ω—è—Ç—å –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å –∏–∑ CSV. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True/False."""
    global SCREENS, LAST_SYNC_TS
    try:
        if not CACHE_CSV.exists():
            logging.info(f"–ö—ç—à CSV –Ω–µ –Ω–∞–π–¥–µ–Ω: {CACHE_CSV} | {_cache_diag()}")
            return False

        df = pd.read_csv(CACHE_CSV)
        if df is None or df.empty:
            logging.warning(f"–ö—ç—à CSV –ø—É—Å—Ç–æ–π: {CACHE_CSV}")
            return False

        SCREENS = df

        if CACHE_META.exists():
            meta = json.loads(CACHE_META.read_text(encoding="utf-8"))
            LAST_SYNC_TS = float(meta.get("ts")) if "ts" in meta else None
        else:
            LAST_SYNC_TS = None

        logging.info(f"Loaded screens cache: {len(SCREENS)} rows, ts={LAST_SYNC_TS} | {_cache_diag()}")
        return True
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫—ç—à–∞: {e} | {_cache_diag()}", exc_info=True)
        return False

# ====== –ì–µ–æ –∏ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã ======

# –õ—ë–≥–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º -> OSM —Ç–µ–≥–∏
_OSM_CATEGORY_HINTS = [
    # (–∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–∏–µ, —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤)
    (("amenity", "pharmacy"), ["–∞–ø—Ç–µ–∫–∞", "pharmacy"]),
    (("shop", "mall"), ["—Ç—Ü", "—Ç–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä", "–º–æ–ª–ª", "mall"]),
    (("shop", "doityourself"), ["—Ç–≤–æ–π –¥–æ–º", "leroy", "obi", "castorama"]),
    (("amenity", "hospital"), ["–±–æ–ª—å–Ω–∏—Ü–∞", "hospital"]),
    (("amenity", "university"), ["—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç", "university"]),
    (("amenity", "school"), ["—à–∫–æ–ª–∞", "school"]),
    (("amenity", "cinema"), ["–∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä", "cinema"]),
    (("amenity", "parking"), ["–ø–∞—Ä–∫–æ–≤–∫–∞", "parking"]),
]
def _detect_osm_category(q: str):
    t = (q or "").lower()
    for (k, v), words in _OSM_CATEGORY_HINTS:
        if any(w in t for w in words):
            return k, v
    return None

async def _nominatim_city_bbox(session: aiohttp.ClientSession, city: str, ssl):
    """–ü–æ–ª—É—á–∞–µ–º bbox –≥–æ—Ä–æ–¥–∞ —á–µ—Ä–µ–∑ Nominatim: (south, west, north, east)."""
    params = {
        "q": city,
        "format": "jsonv2",
        "limit": 1,
        "addressdetails": 0,
        "polygon_geojson": 0,
    }
    headers = {"User-Agent": "omniboard-bot/1.0"}
    async with session.get(NOMINATIM_URL, params=params, headers=headers, ssl=ssl) as r:
        data = await r.json()
    if not data:
        return None
    bbox = data[0].get("boundingbox")
    if not bbox or len(bbox) < 4:
        return None
    south, north, west, east = float(bbox[0]), float(bbox[1]), float(bbox[2]), float(bbox[3])
    # Nominatim –æ—Ç–¥–∞—ë—Ç [south, north, west, east]
    return (south, west, north, east)

def _build_overpass_query(q: str, bbox=None, limit=50):
    """
    –°—Ç—Ä–æ–∏–º Overpass QL.
    –ï—Å–ª–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é ‚Äî –¥–æ–±–∞–≤–∏–º (k=v).
    –ò–º—è –∏—â–µ–º –ø–æ regexp /name~/‚Ä¶/i (–±–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞).
    """
    # –†–µ–≥–µ–∫—Å –ø–æ –∏–º–µ–Ω–∏ (–≤—ã—Ä–µ–∂–µ–º –ª–∏—à–Ω–µ–µ, —ç–∫—Ä–∞–Ω–∏—Ä—É–µ–º —Ç–æ—á–∫—É –≤ "36.6")
    name_tokens = [t for t in re.split(r"\s+", q.strip()) if t]
    pattern = "|".join([re.escape(t).replace(r"\.", r"\.") for t in name_tokens])  # "36.6" -> "36\.6"
    name_clause = f'[name~"{pattern}",i]'

    kv = _detect_osm_category(q)  # –Ω–∞–ø—Ä–∏–º–µ—Ä ("amenity","pharmacy")
    kv_clause = ""
    if kv:
        kv_clause = f'[{kv[0]}="{kv[1]}"]'

    bbox_clause = ""
    if bbox and len(bbox) == 4:
        s, w, n, e = bbox
        bbox_clause = f"({s},{w},{n},{e})"

    # –ò—â–µ–º –ø–æ –≤—Å–µ–º —Ç–∏–ø–∞–º: —Ç–æ—á–∫–∏, –ø—É—Ç–∏, –æ—Ç–Ω–æ—à–µ–Ω–∏—è; –¥–ª—è ways/relations –±–µ—Ä—ë–º center
    ql = f"""
[out:json][timeout:25];
(
  node{kv_clause}{name_clause}{bbox_clause};
  way{kv_clause}{name_clause}{bbox_clause};
  relation{kv_clause}{name_clause}{bbox_clause};
);
out center {limit};
"""
    return ql

async def _overpass_search(session: aiohttp.ClientSession, query: str, city: str | None, limit: int, ssl):
    # –ü–æ–ª—É—á–∏–º bbox –≥–æ—Ä–æ–¥–∞ –¥–ª—è —Å—É–∂–µ–Ω–∏—è (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω city)
    bbox = None
    if city:
        try:
            bbox = await _nominatim_city_bbox(session, city, ssl)
        except Exception:
            bbox = None

    ql = _build_overpass_query(query, bbox=bbox, limit=limit)
    headers = {"User-Agent": "omniboard-bot/1.0"}
    for url in OVERPASS_ENDPOINTS:
        try:
            async with session.post(url, data=ql.encode("utf-8"), headers=headers, ssl=ssl, timeout=aiohttp.ClientTimeout(total=40)) as r:
                if r.status != 200:
                    continue
                data = await r.json()
        except Exception:
            continue

        els = data.get("elements", []) if isinstance(data, dict) else []
        pois = []
        seen = set()
        for el in els:
            tags = el.get("tags", {}) or {}
            name = tags.get("name") or tags.get("brand") or "(–±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è)"
            lat = el.get("lat")
            lon = el.get("lon")
            if lat is None or lon is None:
                center = el.get("center") or {}
                lat = center.get("lat")
                lon = center.get("lon")
            if lat is None or lon is None:
                continue
            key = (round(float(lat), 6), round(float(lon), 6), name)
            if key in seen:
                continue
            seen.add(key)
            pois.append({
                "name": name,
                "lat": float(lat),
                "lon": float(lon),
                "provider": "overpass",
                "raw": {"id": el.get("id"), "type": el.get("type"), "tags": tags}
            })
        if pois:
            # –æ—Ç—Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–º–µ–Ω–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            pois.sort(key=lambda x: x["name"].lower())
            return pois[:limit]
    return []

async def _nominatim_search(session: aiohttp.ClientSession, query: str, city: str | None, limit: int, ssl):
    q = f"{query}, {city}" if city else query
    params = {
        "q": q,
        "format": "jsonv2",
        "limit": limit,
        "addressdetails": 0,
    }
    headers = {"User-Agent": "omniboard-bot/1.0"}
    async with session.get(NOMINATIM_URL, params=params, headers=headers, ssl=ssl, timeout=aiohttp.ClientTimeout(total=30)) as r:
        data = await r.json()

    pois = []
    seen = set()
    for item in data or []:
        name = item.get("display_name") or "(–±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è)"
        lat = item.get("lat")
        lon = item.get("lon")
        if not lat or not lon:
            continue
        key = (round(float(lat), 6), round(float(lon), 6), name)
        if key in seen:
            continue
        seen.add(key)
        pois.append({
            "name": name,
            "lat": float(lat),
            "lon": float(lon),
            "provider": "nominatim"
        })
    return pois[:limit]

def _make_ssl_param_for_aiohttp():
    # —É —Ç–µ–±—è —É–∂–µ –µ—Å—Ç—å —ç—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è; –æ—Å—Ç–∞–≤–ª—è—é –∑–∞–≥–ª—É—à–∫—É —á—Ç–æ–±—ã –Ω–µ —Ä–æ–Ω—è—Ç—å –∏–º–ø–æ—Ä—Ç
    import ssl, os
    verify = (os.getenv("OBDSP_SSL_VERIFY","1") or "1").lower() in {"1","true","yes","on"}
    if verify:
        return None
    ctx = ssl.create_default_context()
    ctx.check_hostname = False
    ctx.verify_mode = ssl.CERT_NONE
    return ctx

async def geocode_query(query: str, city: str | None = None, limit: int = 10, provider: str = "nominatim"):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ POI: [{name, lat, lon, provider, raw?}]
    provider: 'nominatim' | 'overpass'
    """
    ssl_param = _make_ssl_param_for_aiohttp()
    async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=45)) as session:
        prov = (provider or "nominatim").lower().strip()
        if prov == "overpass":
            pois = await _overpass_search(session, query, city, limit, ssl_param)
            if pois:
                return pois
            # –µ—Å–ª–∏ –ø—É—Å—Ç–æ ‚Äî —Ñ–æ–ª–±—ç–∫ –Ω–∞ nominatim
            return await _nominatim_search(session, query, city, limit, ssl_param)
        else:
            pois = await _nominatim_search(session, query, city, limit, ssl_param)
            # –µ—Å–ª–∏ ¬´—Å–µ—Ç–µ–≤–æ–π¬ª –∑–∞–ø—Ä–æ—Å (–º–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–∂–∏–¥–∞–µ—Ç—Å—è), –∞ nominatim –¥–∞–ª 0‚Äì1 ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º Overpass
            expect_many = any(w in (query or "").lower() for w in ["–∞–ø—Ç–µ–∫–∞", "—Ç—Ü", "—Ç–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä", "36.6", "—Ç–≤–æ–π –¥–æ–º"])
            if (not pois or len(pois) < 2) and expect_many:
                more = await _overpass_search(session, query, city, limit, ssl_param)
                if more:
                    return more
            return pois

def haversine_km(a: tuple[float, float], b: tuple[float, float]) -> float:
    lat1, lon1 = map(math.radians, a)
    lat2, lon2 = map(math.radians, b)
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    r = 6371.0088
    h = math.sin(dlat/2)**2 + math.cos(lat1)*math.cos(lat2)*math.sin(dlon/2)**2
    return 2 * r * math.asin(math.sqrt(h))

def find_within_radius(df: pd.DataFrame, center: tuple[float,float], radius_km: float) -> pd.DataFrame:
    rows = []
    for _, row in df.iterrows():
        d = haversine_km(center, (row["lat"], row["lon"]))
        if d <= radius_km:
            rows.append({
                "screen_id": row.get("screen_id", ""),
                "name": row.get("name", ""),
                "city": row.get("city", ""),
                "format": row.get("format", ""),
                "owner": row.get("owner", ""),
                "lat": row["lat"],
                "lon": row["lon"],
                "distance_km": round(d, 3),
            })
    out = pd.DataFrame(rows)
    return out.sort_values("distance_km") if not out.empty else out

def spread_select(df: pd.DataFrame, n: int, *, random_start: bool = True, seed: int | None = None) -> pd.DataFrame:
    """–ñ–∞–¥–Ω—ã–π k-center (Gonzalez) c —Ä–∞–Ω–¥–æ–º–Ω—ã–º —Å—Ç–∞—Ä—Ç–æ–º –∏ —Å–ª—É—á–∞–π–Ω—ã–º–∏ —Ç–∞–π-–±—Ä–µ–π–∫–∞–º–∏."""
    import random as _random
    if df.empty or n <= 0:
        return df.iloc[0:0]
    n = min(n, len(df))

    if seed is not None:
        _random.seed(seed)

    coords = df[["lat", "lon"]].to_numpy()

    if random_start:
        start_idx = _random.randrange(len(df))
    else:
        lat_med = float(df["lat"].median())
        lon_med = float(df["lon"].median())
        start_idx = min(
            range(len(df)),
            key=lambda i: haversine_km((lat_med, lon_med), (coords[i][0], coords[i][1]))
        )

    chosen = [start_idx]
    dists = [
        haversine_km((coords[start_idx][0], coords[start_idx][1]), (coords[i][0], coords[i][1]))
        for i in range(len(df))
    ]

    while len(chosen) < n:
        maxd = max(dists)
        candidates = [i for i, d in enumerate(dists) if d == maxd]
        next_idx = _random.choice(candidates)
        chosen.append(next_idx)
        cx, cy = coords[next_idx]
        for i in range(len(df)):
            d = haversine_km((cx, cy), (coords[i][0], coords[i][1]))
            if d < dists[i]:
                dists[i] = d

    res = df.iloc[chosen].copy()
    res["min_dist_to_others_km"] = 0.0
    cc = res[["lat","lon"]].to_numpy()
    for i in range(len(res)):
        mind = min(
            haversine_km((cc[i][0], cc[i][1]), (cc[j][0], cc[j][1]))
            for j in range(len(res)) if j != i
        ) if len(res) > 1 else 0.0
        res.iat[i, res.columns.get_loc("min_dist_to_others_km")] = round(mind, 3)
    return res

def parse_kwargs(parts: list[str]) -> dict[str, str]:
    """–ü–∞—Ä—Å–∏–º —Ö–≤–æ—Å—Ç –∫–æ–º–∞–Ω–¥—ã –≤–∏–¥–∞ key=value (–∑–Ω–∞—á–µ–Ω–∏—è –º–æ–∂–Ω–æ –±—Ä–∞—Ç—å –≤ –∫–∞–≤—ã—á–∫–∏)."""
    out: dict[str,str] = {}
    for p in parts:
        if "=" in p:
            k, v = p.split("=", 1)
            out[k.strip()] = v.strip().strip('"').strip("'")
    return out

def parse_fields(arg: str) -> list[str]:
    allowed = {
        "screen_id","name","city","format","owner","lat","lon",
        "distance_km","min_dist_to_others_km"
    }
    cols = [c.strip() for c in arg.split(",")]
    return [c for c in cols if c in allowed]

def parse_list(val: str) -> list[str]:
    if not isinstance(val, str):
        return []
    for sep in ("|", ";"):
        val = val.replace(sep, ",")
    return [x.strip() for x in val.split(",") if x.strip()]

def _format_mask(series: pd.Series, token: str) -> pd.Series:
    col = series.astype(str).str.upper().str.strip()
    t = token.strip().upper()
    if t in {"CITY", "CITY_FORMAT", "CITYFORMAT", "CITYLIGHT", "–ì–ò–î", "–ì–ò–î–´"}:
        return col.str.startswith("CITY_FORMAT")
    if t in {"BILLBOARD", "BB"}:
        return col == "BILLBOARD"
    return col == t

def apply_filters(df: pd.DataFrame, kwargs: dict[str,str]) -> pd.DataFrame:
    out = df
    # FORMAT
    fmt_val = kwargs.get("format") or kwargs.get("formats") or kwargs.get("format_in")
    if fmt_val:
        fmt_list_raw = parse_list(fmt_val)
        fmt_list = [s.upper() for s in fmt_list_raw]
        mask = None
        col = out["format"].astype(str).str.upper()
        for f in fmt_list:
            if f.lower() in {"city", "city_format", "cityformat", "citylight", "–≥–∏–¥", "–≥–∏–¥—ã"}:
                m = col.str.startswith("CITY_FORMAT")
            else:
                m = (col == f)
            mask = m if mask is None else (mask | m)
        if mask is not None:
            out = out[mask]
    # OWNER
    own_val = kwargs.get("owner") or kwargs.get("owners") or kwargs.get("owner_in")
    if own_val:
        owners = parse_list(own_val)
        mask = None
        col = out["owner"].astype(str).str.lower()
        for o in owners:
            m = col.str.contains(o.strip().lower())
            mask = m if mask is None else (mask | m)
        if mask is not None:
            out = out[mask]
    return out

# –†–∞–∑–±–∏–≤–∫–∞ –¥–ª–∏–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏
async def send_lines(message: types.Message, lines: list[str], header: str | None = None, chunk: int = 60, parse_mode: str | None = None):
    if header:
        await message.answer(header, parse_mode=parse_mode)
    if not lines:
        return
    MAX_CHARS = 3900
    buf: list[str] = []
    buf_len = 0
    buf_cnt = 0
    for line in lines:
        s = str(line)
        if len(s) > MAX_CHARS:
            if buf:
                await message.answer("\n".join(buf), parse_mode=parse_mode)
                buf, buf_len, buf_cnt = [], 0, 0
            for i in range(0, len(s), MAX_CHARS):
                await message.answer(s[i:i+MAX_CHARS], parse_mode=parse_mode)
            continue
        if buf and (buf_len + 1 + len(s) > MAX_CHARS or buf_cnt >= chunk):
            await message.answer("\n".join(buf), parse_mode=parse_mode)
            buf, buf_len, buf_cnt = [], 0, 0
        buf.append(s)
        buf_len += (len(s) + 1)
        buf_cnt += 1
    if buf:
        await message.answer("\n".join(buf), parse_mode=parse_mode)

# ===== Geocoding / Places =====
import aiohttp, urllib.parse, asyncio

async def geocode_query(query: str, *, city: str | None = None, limit: int = 5, provider: str | None = None) -> list[dict]:
    """
    –í–µ—Ä–Ω—ë—Ç —Å–ø–∏—Å–æ–∫ dict: { 'name': str, 'lat': float, 'lon': float, 'provider': str, 'raw': any }
    provider: 'nominatim'|'google'|'yandex'|'2gis'|'auto'
    """
    prov = (provider or GEOCODER_PROVIDER or "nominatim").lower()
    if prov == "auto":
        prov = "nominatim"

    query_full = query.strip()
    if city and city.strip():
        # –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –¥–æ–±–∞–≤–∏–º –≥–æ—Ä–æ–¥, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç –≤ –∑–∞–ø—Ä–æ—Å–µ
        if city.lower() not in query_full.lower():
            query_full = f"{query_full}, {city}"

    if prov == "nominatim":
        return await _gc_nominatim(query_full, limit=limit)
    elif prov == "google":
        return await _gc_google(query_full, limit=limit)
    elif prov == "yandex":
        return await _gc_yandex(query_full, limit=limit)
    elif prov == "2gis":
        return await _gc_2gis(query_full, limit=limit)
    else:
        return await _gc_nominatim(query_full, limit=limit)


async def _gc_nominatim(q: str, *, limit: int = 5) -> list[dict]:
    """
    –ë–∞–∑–æ–≤—ã–π –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç. –í–∞–∂–Ω–æ: —É–≤–∞–∂–∞—Ç—å rate limit.
    """
    url = "https://nominatim.openstreetmap.org/search"
    params = {
        "q": q,
        "limit": max(1, min(int(limit or 5), 25)),
        "format": "jsonv2",
        "addressdetails": 1,
    }
    headers = {
        "User-Agent": "omnika-bot/1.0 (contact: admin@example.com)"
    }
    async with aiohttp.ClientSession(headers=headers, timeout=aiohttp.ClientTimeout(total=20)) as s:
        async with s.get(url, params=params) as r:
            r.raise_for_status()
            data = await r.json()
    out = []
    for it in data or []:
        try:
            out.append({
                "name": it.get("display_name") or q,
                "lat": float(it["lat"]),
                "lon": float(it["lon"]),
                "provider": "nominatim",
                "raw": it
            })
        except Exception:
            continue
    return out


# –ó–∞–≥–ª—É—à–∫–∏ –ø–æ–¥ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ (–µ—Å–ª–∏ –∑–∞—Ö–æ—á–µ—à—å ‚Äî –¥–æ–ø–∏–ª–∏—à—å –∫–ª—é—á–∏ –∏ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã)
async def _gc_google(q: str, *, limit: int = 5) -> list[dict]:
    if not GOOGLE_PLACES_KEY:
        return []
    # TODO: —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ (Places API / Text Search)
    return []

async def _gc_yandex(q: str, *, limit: int = 5) -> list[dict]:
    if not YANDEX_API_KEY:
        return []
    # TODO: —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ (Geocoder API)
    return []

async def _gc_2gis(q: str, *, limit: int = 5) -> list[dict]:
    if not D2GIS_API_KEY:
        return []
    # TODO: —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ (2GIS Search API)
    return []

# ====== SSL / HTTP helpers ======
def _ssl_ctx_certifi() -> ssl.SSLContext:
    if certifi is not None:
        ctx = ssl.create_default_context(cafile=certifi.where())
    else:
        ctx = ssl.create_default_context()
    ctx.check_hostname = True
    ctx.verify_mode = ssl.CERT_REQUIRED
    return ctx

def _make_ssl_param_for_aiohttp():
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      - False  -> –æ—Ç–∫–ª—é—á–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É (aiohttp –ø—Ä–∏–º–µ—Ç ssl=False)
      - ssl.SSLContext -> –∫–∞—Å—Ç–æ–º–Ω—ã–π CA (OBDSP_CA_BUNDLE) –∏–ª–∏ certifi
      - None  -> —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∫–æ—Ä–Ω–∏
    """
    if OBDSP_SSL_VERIFY in {"0", "false", "no", "off"} or OBDSP_SSL_NO_VERIFY:
        return False
    if OBDSP_CA_BUNDLE:
        return ssl.create_default_context(cafile=OBDSP_CA_BUNDLE)
    if certifi is not None:
        try:
            return ssl.create_default_context(cafile=certifi.where())
        except Exception:
            pass
    return None

def _auth_headers() -> dict:
    t = (OBDSP_TOKEN or "").strip()
    scheme = (OBDSP_AUTH_SCHEME or "Bearer").strip().lower()
    if not t:
        return {}
    if scheme == "apikey":
        return {"X-API-Key": t}
    if scheme == "basic":
        return {"Authorization": f"Basic {t}"}
    if scheme == "token":
        return {"Authorization": f"Token {t}"}
    return {"Authorization": f"Bearer {t}"}

def _owner_only(user_id: int) -> bool:
    # –µ—Å–ª–∏ TELEGRAM_OWNER_ID=0 ‚Äî —Ä–∞–∑—Ä–µ—à–∞–µ–º –≤—Å–µ–º (—É–¥–æ–±–Ω–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
    return TELEGRAM_OWNER_ID == 0 or user_id == TELEGRAM_OWNER_ID

# ====== API: –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å ======
def _build_server_query(filters: dict | None) -> dict:
    if not filters:
        return {}
    q: dict[str, Any] = {}
    city = (filters.get("city") or "").strip()
    if city:
        q["city"] = city
        q["cityName"] = city
        q["search"] = city
    fmts = filters.get("formats") or []
    if fmts:
        q["type"] = fmts
        q["format"] = fmts
        q["types"] = fmts
        q["formats"] = fmts
    owners = filters.get("owners") or []
    if owners:
        q["owner"] = owners
        q["displayOwnerName"] = owners
        q["search"] = (" ".join([q.get("search",""), *owners])).strip()
    for k, v in (filters.get("api_params") or {}).items():
        q[k] = v
    return {k: v for k, v in q.items() if v not in ("", None, [], {})}

async def _fetch_inventories(
    pages_limit: int | None = None,
    page_size: int = 500,
    total_limit: int | None = None,
    m: types.Message | None = None,
    filters: dict | None = None,
) -> list[dict]:
    base = (OBDSP_BASE or "https://proddsp.omniboard360.io").rstrip("/")
    root = f"{base}/api/v1.0/clients/inventories"
    headers = {**_auth_headers(), "Accept": "application/json"}
    timeout = aiohttp.ClientTimeout(total=180)
    ssl_param = _make_ssl_param_for_aiohttp()
    server_q = _build_server_query(filters)

    items: list[dict] = []
    page = 0
    pages_fetched = 0

    async with aiohttp.ClientSession(timeout=timeout) as session:
        while True:
            params: dict[str, Any] = {"page": page, "size": page_size}
            params.update(server_q)
            async with session.get(root, headers=headers, params=params, ssl=ssl_param) as resp:
                text = await resp.text()
                if resp.status != 200:
                    raise RuntimeError(f"API {resp.status}: {text[:300]}")
                try:
                    data = await resp.json()
                except Exception:
                    raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON: {text[:500]}")

                page_items = data.get("content") or []
                items.extend(page_items)

                pages_fetched += 1
                page += 1

                if total_limit is not None and len(items) >= total_limit:
                    items = items[:total_limit]
                    break
                if pages_limit is not None and pages_fetched >= pages_limit:
                    break
                if data.get("last") is True:
                    break
                if data.get("totalPages") is not None and page >= int(data["totalPages"]):
                    break
                if data.get("numberOfElements") == 0:
                    break

                if m and (pages_fetched % 5 == 0):
                    try:
                        await m.answer(f"‚Ä¶–∑–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {pages_fetched}, –≤—Å–µ–≥–æ –ø–æ–∑–∏—Ü–∏–π: {len(items)}")
                    except Exception:
                        pass
    return items

def _normalize_api_to_df(items: list[dict]) -> pd.DataFrame:
    if not items:
        return pd.DataFrame(columns=[
            "id","screen_id","name","format","placement","installation",
            "owner_id","owner","city","address","lat","lon",
            "width_mm","height_mm","width_px","height_px",
            "phys_width_px","phys_height_px",
            "sspProvider","sspTypes","minBid","ots","grp","meta_format",
            "image_url","image_preview"
        ])

    def g(obj, path, default=None):
        try:
            cur = obj
            for k in path:
                if cur is None:
                    return default
                if isinstance(k, int):
                    cur = (cur or [])[k] if isinstance(cur, list) and len(cur) > k else None
                else:
                    cur = (cur or {}).get(k)
            return default if cur is None else cur
        except Exception:
            return default

    rows = []
    for it in items:
        rows.append({
            "id": it.get("id"),
            "screen_id": it.get("gid"),
            "name": it.get("name"),
            "format": it.get("type"),
            "placement": it.get("placement"),
            "installation": it.get("installation"),
            "owner_id": g(it, ["displayOwner","id"]),
            "owner":    g(it, ["displayOwner","name"]),
            "city":    g(it, ["location","city"]),
            "address": g(it, ["location","address"]),
            "lat":     g(it, ["location","latitude"]),
            "lon":     g(it, ["location","longitude"]),
            "width_mm":  g(it, ["surfaceDimensionMM","width"]),
            "height_mm": g(it, ["surfaceDimensionMM","height"]),
            "width_px":  g(it, ["screenResolutionPx","width"]),
            "height_px": g(it, ["screenResolutionPx","height"]),
            "phys_width_px":  g(it, ["physicalResolutionPx","width"]),
            "phys_height_px": g(it, ["physicalResolutionPx","height"]),
            "sspProvider": it.get("sspProvider"),
            "sspTypes":    ",".join(it.get("sspTypes") or []),
            "minBid": g(it, ["minBidInfo","minBid"]),
            "ots":    g(it, ["minBidInfo","ots"]),
            "grp":    g(it, ["metadata","grp"]),
            "meta_format": g(it, ["metadata","format"]),
            "image_url":     g(it, ["images", 0, "url"]),
            "image_preview": g(it, ["images", 0, "preview"]),
        })
    df = pd.DataFrame(rows)

    # –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –∫ float
    for c in ("lat","lon"):
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
    # –Ω–∞ –≤—Å—è–∫–∏–π ‚Äî —É–±–µ—Ä—ë–º –º—É—Å–æ—Ä–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
    if {"lat","lon"}.issubset(df.columns):
        df = df.dropna(subset=["lat","lon"]).reset_index(drop=True)
    return df

# ====== API: —Ñ–æ—Ç–æ–æ—Ç—á—ë—Ç—ã ======
async def _fetch_impression_shots(
    campaign_id: int,
    per: int = 0,
    want_zip: bool = False,
    m: types.Message | None = None,
    dbg: bool = False,
):
    base = (OBDSP_BASE or "https://proddsp.omniboard360.io").rstrip("/")
    headers = {**_auth_headers(), "Accept": "application/json"}
    ssl_param = _make_ssl_param_for_aiohttp()
    timeout = aiohttp.ClientTimeout(total=180)

    async with aiohttp.ClientSession(timeout=timeout) as session:
        if want_zip:
            url = f"{base}/api/v1.0/campaigns/{campaign_id}/impression-shots/export"
            payload = {"shotCountPerInventoryCreative": per if per > 0 else 0}
            if dbg and m:
                try: await m.answer(f"POST {url} (export, per={per})")
                except: pass
            async with session.post(url, headers=headers, json=payload, ssl=ssl_param) as resp:
                if resp.status == 200:
                    body = await resp.read()
                    return [{"__binary__": True, "__body__": body}]
                elif resp.status not in (404, 405):
                    raise RuntimeError(f"API {resp.status}: {await resp.text()}")

        q = {"shotCountPerInventoryCreative": per} if per > 0 else {}
        url = f"{base}/api/v1.0/campaigns/{campaign_id}/impression-shots"
        if dbg and m:
            try: await m.answer(f"GET {url} {q}")
            except: pass
        async with session.get(url, headers=headers, params=q, ssl=ssl_param) as resp:
            txt = await resp.text()
            if resp.status == 200:
                try:
                    data = await resp.json()
                except Exception:
                    raise RuntimeError(f"–ù–µ JSON: {txt[:400]}")
                if isinstance(data, dict) and "content" in data:
                    return data.get("content") or []
                return data if isinstance(data, list) else []
            elif resp.status not in (404, 405):
                raise RuntimeError(f"API {resp.status}: {txt[:400]}")

        url = f"{base}/api/v1.0/clients/campaigns/{campaign_id}/impression-shots"
        if dbg and m:
            try: await m.answer(f"GET {url} {q}")
            except: pass
        async with session.get(url, headers=headers, params=q, ssl=ssl_param) as resp:
            txt = await resp.text()
            if resp.status == 200:
                try:
                    data = await resp.json()
                except Exception:
                    raise RuntimeError(f"–ù–µ JSON: {txt[:400]}")
                if isinstance(data, dict) and "content" in data:
                    return data.get("content") or []
                return data if isinstance(data, list) else []
            elif resp.status not in (404, 405):
                raise RuntimeError(f"API {resp.status}: {txt[:400]}")

        url = f"{base}/api/v1.0/impression-shots"
        params = {"campaignId": campaign_id}
        if per > 0:
            params["shotCountPerInventoryCreative"] = per
        if dbg and m:
            try: await m.answer(f"GET {url} {params}")
            except: pass
        async with session.get(url, headers=headers, params=params, ssl=ssl_param) as resp:
            txt = await resp.text()
            if resp.status == 200:
                try:
                    data = await resp.json()
                except Exception:
                    raise RuntimeError(f"–ù–µ JSON: {txt[:400]}")
                if isinstance(data, dict) and "content" in data:
                    return data.get("content") or []
                return data if isinstance(data, list) else []
            raise RuntimeError(f"API {resp.status}: {txt[:400]}")

def _normalize_shots(raw: list[dict]) -> pd.DataFrame:
    if not raw:
        return pd.DataFrame(columns=[
            "shot_id","campaign_id",
            "inventory_id","inventory_gid","inventory_name",
            "city","address","lat","lon",
            "creative_id","creative_name",
            "shot_time","image_url","preview_url"
        ])

    def g(o, path, default=None):
        try:
            cur = o
            for k in path:
                if cur is None:
                    return default
                cur = cur[k] if isinstance(k, int) else (cur or {}).get(k)
            return default if cur is None else cur
        except Exception:
            return default

    rows = []
    for it in raw:
        if it.get("__binary__"):
            # —ç—Ç–æ—Ç —Ç–∏–ø –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è ZIP ‚Äî –Ω–µ –ø–æ–ø–∞–¥–∞–µ—Ç –≤ –Ω–æ—Ä–º–∞–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
            continue
        inv   = it.get("inventory") or {}
        loc   = inv.get("location") or {}
        img   = it.get("image") or {}
        img_url     = img.get("url") or it.get("imageUrl") or it.get("url")
        preview_url = img.get("preview") or it.get("previewUrl") or it.get("thumbnailUrl")

        rows.append({
            "shot_id":       it.get("id"),
            "campaign_id":   g(it, ["campaign","id"]) or it.get("campaignId"),
            "inventory_id":  inv.get("id"),
            "inventory_gid": inv.get("gid"),
            "inventory_name":inv.get("name"),
            "city":          loc.get("city"),
            "address":       loc.get("address"),
            "lat":           loc.get("latitude"),
            "lon":           loc.get("longitude"),
            "creative_id":   g(it, ["creative","id"]) or it.get("creativeId"),
            "creative_name": g(it, ["creative","name"]),
            "shot_time":     it.get("shotTime") or it.get("time") or it.get("created"),
            "image_url":     img_url,
            "preview_url":   preview_url,
        })

    df = pd.DataFrame(rows)
    if "shot_time" in df.columns:
        with pd.option_context("mode.chained_assignment", None):
            try:
                df["shot_time"] = pd.to_datetime(df["shot_time"], errors="coerce", utc=True).dt.tz_convert(None)
            except Exception:
                pass
    return df

# ====== –ü—Ä–æ–≥–Ω–æ–∑ ======
def _parse_hours_windows(s: str | None) -> int | None:
    if not s:
        return None
    total = 0
    parts = [p.strip() for p in s.split(",") if p.strip()]
    for p in parts:
        if "-" in p:
            a, b = p.split("-", 1)
            try:
                a = int(a); b = int(b)
                if 0 <= a <= 23 and 0 <= b <= 23:
                    if b > a:
                        total += (b - a)
                    else:
                        total += (24 - a + b)
            except Exception:
                pass
    return total or None

def _fill_min_bid(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    src_col = None
    for cand in ("minBid", "min_bid", "min_bid_rub", "min_bid_rur"):
        if cand in out.columns:
            src_col = cand
            break
    if src_col:
        vals = pd.to_numeric(out[src_col], errors="coerce")
        median = float(vals.median()) if not vals.dropna().empty else None
        out["min_bid_used"] = vals.fillna(median if median else 0)
        out["min_bid_source"] = src_col
    else:
        out["min_bid_used"] = None
        out["min_bid_source"] = None
    return out

def _distribute_slots_evenly(n_items: int, total_slots: int) -> list[int]:
    if n_items <= 0 or total_slots <= 0:
        return [0] * max(0, n_items)
    base = total_slots // n_items
    extra = total_slots % n_items
    res = [base] * n_items
    for i in range(extra):
        res[i] += 1
    return res

# ====== BOT (aiogram 3.x) ======
BOT_TOKEN = os.getenv("BOT_TOKEN")
if not BOT_TOKEN:
    raise SystemExit("Set BOT_TOKEN env var first")

bot = Bot(BOT_TOKEN)
dp = Dispatcher()

geo_router = Router(name="geo")
router = Router()
nlu_router = Router(name="nlu")


# ---------- GEO router ----------
from aiogram import Router, F, types
from aiogram.filters import Command

geo_router = Router(name="geo")

# –•—Ä–∞–Ω–∏–ª–∏—â–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ø–∏—Å–∫–∞ POI (–µ—Å–ª–∏ —É —Ç–µ–±—è —É–∂–µ –µ—Å—Ç—å ‚Äî –æ—Å—Ç–∞–≤—å —Å–≤–æ—ë)
LAST_POI = []

@geo_router.message(Command("geo"))
async def cmd_geo(m: types.Message):
    """
    /geo <–∑–∞–ø—Ä–æ—Å> [city=...] [limit=...] [provider=nominatim|google|yandex|2gis]
    –ü—Ä–∏–º–µ—Ä—ã:
      /geo –¢–≤–æ–π –¥–æ–º city=–ú–æ—Å–∫–≤–∞ limit=5
      /geo –Ω–æ–≤–æ—Å—Ç—Ä–æ–π–∫–∏ –±–∏–∑–Ω–µ—Å-–∫–ª–∞—Å—Å–∞ city=–í–æ—Ä–æ–Ω–µ–∂
    """
    global LAST_POI
    text = (m.text or "").strip()
    parts = text.split()[1:]
    if not parts:
        await m.answer("–§–æ—Ä–º–∞—Ç: /geo <–∑–∞–ø—Ä–æ—Å> [city=...] [limit=5] [provider=nominatim]")
        return

    # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∏ key=value
    query_tokens, kv = [], {}
    for p in parts:
        if "=" in p:
            k, v = p.split("=", 1)
            kv[k.strip().lower()] = v.strip()
        else:
            query_tokens.append(p)
    query = " ".join(query_tokens).strip()
    if not query:
        await m.answer("–ù—É–∂–µ–Ω —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞. –ü—Ä–∏–º–µ—Ä: /geo –¢–≤–æ–π –¥–æ–º city=–ú–æ—Å–∫–≤–∞ limit=5")
        return

    city = kv.get("city")
    try:
        limit = int(kv.get("limit", "5") or 5)
    except Exception:
        limit = 5
    provider = (kv.get("provider") or "nominatim").lower()

    await m.answer(f"üîé –ò—â—É —Ç–æ—á–∫–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É ¬´{query}¬ª" + (f" –≤ –≥–æ—Ä–æ–¥–µ {city}" if city else "") + "‚Ä¶")

    pois = []
    try:
        pois = await geocode_query(query, city=city, limit=limit, provider=provider)
    except Exception as e:
        await m.answer(f"‚ö†Ô∏è –ì–µ–æ–∫–æ–¥–µ—Ä {provider} –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É: {e}. –ü—Ä–æ–±—É—é –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É‚Ä¶")

    # fallback –Ω–∞ OpenAI ‚Äî –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏/–æ—à–∏–±–∫–∞
    if not pois:
        try:
            await m.answer("üß† –ü—Ä–æ–±—É—é –Ω–∞–π—Ç–∏ —á–µ—Ä–µ–∑ OpenAI‚Ä¶")
            ai_pois = await find_poi_ai(query=query, city=city, limit=limit, country_hint="–†–æ—Å—Å–∏—è")
        except Exception:
            ai_pois = []
        if ai_pois:
            pois = [{
                "name": p.get("name", ""),
                "lat": float(p["lat"]) if p.get("lat") is not None else None,
                "lon": float(p["lon"]) if p.get("lon") is not None else None,
                "provider": p.get("provider", "openai"),
                "address": p.get("address", "")
            } for p in ai_pois if p.get("lat") is not None and p.get("lon") is not None]

    if not pois:
        await m.answer("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à—ë–ª. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É—Ç–æ—á–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å, —É–≤–µ–ª–∏—á–∏—Ç—å limit –∏–ª–∏ —Å–º–µ–Ω–∏—Ç—å provider.")
        return

    LAST_POI = pois

    # –°–æ–±–∏—Ä–∞–µ–º —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ–º –ª–∏–º–∏—Ç Telegram
    lines = []
    for i, p in enumerate(pois, 1):
        addr = (p.get("address") or "").strip()
        prov = p.get("provider", "")
        try:
            lat_s = f"{float(p['lat']):.6f}"
            lon_s = f"{float(p['lon']):.6f}"
        except Exception:
            lat_s = str(p.get("lat", ""))
            lon_s = str(p.get("lon", ""))
        line = f"{i}. {p.get('name','')}" + (f", {addr}" if addr else "") + f"\n   [{lat_s}, {lon_s}] ({prov})"
        lines.append(line)

    # –í —á–∞—Ç ‚Äî –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ + —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –ø–∞—á–∫–∏
    to_show = lines[:100]
    header = (
        f"üìç –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–æ—á–∫–∏: –≤—Å–µ–≥–æ {len(pois)}\n"
        f"(–ø–æ–∫–∞–∑–∞–Ω–æ {len(to_show)}; –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ ‚Äî –≤ CSV)\n\n"
        "–¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ: /near_geo 2  ‚Äî –ø–æ–¥–æ–±—Ä–∞—Ç—å —ç–∫—Ä–∞–Ω—ã —Ä—è–¥–æ–º"
    )
    await send_lines(m, to_show, header=header, chunk=40)

    # –û—Ç–ø—Ä–∞–≤–∏–º –ø–æ–ª–Ω—ã–π CSV —Å–æ –≤—Å–µ–º–∏ POI
    try:
        import io as _io, csv as _csv
        buf = _io.StringIO()
        w = _csv.writer(buf)
        w.writerow(["name", "address", "lat", "lon", "provider"])
        for p in pois:
            w.writerow([
                p.get("name",""),
                p.get("address",""),
                p.get("lat",""),
                p.get("lon",""),
                p.get("provider",""),
            ])
        csv_bytes = buf.getvalue().encode("utf-8-sig")
        await bot.send_document(
            m.chat.id,
            types.BufferedInputFile(csv_bytes, filename="geo_pois.csv"),
            caption=f"–í—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–æ—á–∫–∏: {len(pois)} (CSV)"
        )
    except Exception as e:
        await m.answer(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å CSV —Å —Ç–æ—á–∫–∞–º–∏: {e}")

# ---------- /near_geo (–≤ —Ç–æ–º –∂–µ geo_router) ----------
@geo_router.message(Command("near_geo"))
async def cmd_near_geo(m: types.Message):
    """
    /near_geo [R] [fields=screen_id] [dedup=1] [query=...] [city=...] [limit=...] [provider=...]
    –í–∞—Ä–∏–∞–Ω—Ç—ã:
      1) —Å–Ω–∞—á–∞–ª–∞ /geo ... ; –ø–æ—Ç–æ–º /near_geo 2
      2) —Å—Ä–∞–∑—É: /near_geo 2 query="–¢–≤–æ–π –¥–æ–º" city=–ú–æ—Å–∫–≤–∞ limit=5
    """
    import io as _io

    global SCREENS, LAST_RESULT, LAST_POI
    if SCREENS is None or SCREENS.empty:
        await m.answer("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å (CSV/XLSX –∏–ª–∏ /sync_api).")
        return

    text = (m.text or "").strip()
    tail = text.split()[1:]

    # –†–∞–¥–∏—É—Å (–µ—Å–ª–∏ –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω –±–µ–∑ '=')
    radius_km = USER_RADIUS.get(m.from_user.id, DEFAULT_RADIUS)
    start_i = 0
    if tail and "=" not in tail[0]:
        try:
            radius_km = float(tail[0].strip("[](){}"))
            start_i = 1
        except Exception:
            pass

    # key=value
    kv = {}
    for p in tail[start_i:]:
        if "=" in p:
            k, v = p.split("=", 1)
            kv[k.strip().lower()] = v.strip().strip('"').strip("'")

    fields_req = (kv.get("fields") or "").strip()
    dedup = str(kv.get("dedup", "1")).lower() in {"1","true","yes","on"}

    # –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ POI (–µ—Å–ª–∏ –¥–∞–ª–∏ query=...)
    if "query" in kv:
        q = kv.get("query") or ""
        city = kv.get("city")
        limit = int(kv.get("limit", "5") or 5)
        provider = kv.get("provider", "nominatim")
        await m.answer(f"üîé –ò—â—É —Ç–æ—á–∫–∏ ¬´{q}¬ª" + (f" –≤ {city}" if city else "") + "‚Ä¶")
        try:
            LAST_POI = await geocode_query(q, city=city, limit=limit, provider=provider)
        except Exception as e:
            await m.answer(f"üö´ –ì–µ–æ–∫–æ–¥–µ—Ä –æ—Ç–≤–µ—Ç–∏–ª –æ—à–∏–±–∫–æ–π: {e}")
            return

    pois = LAST_POI or []
    if not pois:
        await m.answer("–°–Ω–∞—á–∞–ª–∞ –Ω–∞–π–¥–∏—Ç–µ —Ç–æ—á–∫–∏: /geo <–∑–∞–ø—Ä–æ—Å> [city=...] ‚Äî –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ /near_geo R query=‚Ä¶")
        return

    await m.answer(f"üß≠ –ü–æ–¥–±–∏—Ä–∞—é —ç–∫—Ä–∞–Ω—ã –≤ —Ä–∞–¥–∏—É—Å–µ {radius_km} –∫–º –≤–æ–∫—Ä—É–≥ {len(pois)} —Ç–æ—á–µ–∫‚Ä¶")

    # –≠–∫—Ä–∞–Ω—ã –≤–æ–∫—Ä—É–≥ –≤—Å–µ—Ö POI
    frames = []
    for p in pois:
        df = find_within_radius(SCREENS, (p["lat"], p["lon"]), radius_km)
        if df is not None and not df.empty:
            df = df.copy()
            df["poi_name"] = p["name"]
            df["poi_lat"]  = p["lat"]
            df["poi_lon"]  = p["lon"]
            frames.append(df)

    if not frames:
        await m.answer("–í –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ä–∞–¥–∏—É—Å–∞—Ö –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —ç–∫—Ä–∞–Ω–æ–≤ –Ω–µ –Ω–∞—à–ª–æ—Å—å.")
        return

    res = pd.concat(frames, ignore_index=True)

    # –î–µ–¥—É–ø –ø–æ screen_id
    if dedup and "screen_id" in res.columns:
        res = res.drop_duplicates(subset=["screen_id"]).reset_index(drop=True)

    LAST_RESULT = res

    # –ß–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–π —Å–ø–∏—Å–æ–∫ (—É—Å–µ—á—ë–º)
    lines = []
    show = res.head(20)
    for _, r in show.iterrows():
        nm = r.get("name","") or r.get("screen_id","")
        fmt = r.get("format","") or ""
        own = r.get("owner","") or ""
        poi = r.get("poi_name","")
        dist = r.get("distance_km", "")
        lines.append(f"‚Ä¢ {r.get('screen_id','')} ‚Äî {nm} [{fmt}/{own}] ‚Äî {dist} –∫–º –æ—Ç ¬´{poi}¬ª")
    await send_lines(
        m,
        lines,
        header=f"–ù–∞–π–¥–µ–Ω–æ {len(res)} —ç–∫—Ä–∞–Ω–æ–≤ —Ä—è–¥–æ–º —Å {len(pois)} —Ç–æ—á–∫–∞–º–∏ (—Ä–∞–¥–∏—É—Å {radius_km} –∫–º)",
        chunk=60
    )

    # ====== –í—ã–¥–∞—á–∞ —Ñ–∞–π–ª–æ–≤ ======
    try:
        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å–∏–ª–∏ –ø–æ–ª—è ‚Äî –ø–æ–¥–≥–æ—Ç–æ–≤–∏–º –æ—Ç–¥–µ–ª—å–Ω–æ–µ "–≤–∏–¥" –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
        if fields_req:
            cols = [c.strip() for c in fields_req.split(",") if c.strip()]
            cols = [c for c in cols if c in res.columns]
            if not cols:
                await m.answer("–ü–æ–ª—è –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω—ã. –î–æ—Å—Ç—É–ø–Ω—ã–µ: " + ", ".join(res.columns))
                return
            view = res[cols].copy()

            # CSV (view)
            csv_bytes = view.to_csv(index=False).encode("utf-8-sig")
            await bot.send_document(
                m.chat.id,
                BufferedInputFile(csv_bytes, filename="near_geo_selection.csv"),
                caption=f"–≠–∫—Ä–∞–Ω—ã —Ä—è–¥–æ–º —Å POI (–ø–æ–ª—è: {', '.join(cols)}) ‚Äî {len(view)} —Å—Ç—Ä–æ–∫ (CSV)"
            )

            # XLSX (view)
            xbuf = _io.BytesIO()
            with pd.ExcelWriter(xbuf, engine="openpyxl") as w:
                view.to_excel(w, index=False, sheet_name="near_geo")
            xbuf.seek(0)
            await bot.send_document(
                m.chat.id,
                BufferedInputFile(xbuf.getvalue(), filename="near_geo_selection.xlsx"),
                caption=f"–≠–∫—Ä–∞–Ω—ã —Ä—è–¥–æ–º —Å POI (–ø–æ–ª—è: {', '.join(cols)}) ‚Äî {len(view)} —Å—Ç—Ä–æ–∫ (XLSX)"
            )

        # –ü–æ–ª–Ω—ã–π CSV
        csv_full = res.to_csv(index=False).encode("utf-8-sig")
        await bot.send_document(
            m.chat.id,
            BufferedInputFile(csv_full, filename="near_geo_full.csv"),
            caption=f"–ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ {len(res)} —ç–∫—Ä–∞–Ω–æ–≤ (CSV)"
        )

        # –ü–æ–ª–Ω—ã–π XLSX
        xbuf_full = _io.BytesIO()
        with pd.ExcelWriter(xbuf_full, engine="openpyxl") as w:
            res.to_excel(w, index=False, sheet_name="near_geo_full")
        xbuf_full.seek(0)
        await bot.send_document(
            m.chat.id,
            BufferedInputFile(xbuf_full.getvalue(), filename="near_geo_full.xlsx"),
            caption=f"–ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ {len(res)} —ç–∫—Ä–∞–Ω–æ–≤ (XLSX)"
        )

    except Exception as e:
        await m.answer(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ñ–∞–π–ª—ã: {e}")

dp.include_router(geo_router)


# ---------- –±–∞–∑–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã ----------
@router.message(Command("start"))
async def start_cmd(m: Message):
    status = f"–≠–∫—Ä–∞–Ω–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(SCREENS)}." if (SCREENS is not None and not SCREENS.empty) else "–≠–∫—Ä–∞–Ω–æ–≤ –µ—â—ë –Ω–µ—Ç ‚Äî –ø—Ä–∏—à–ª–∏—Ç–µ CSV/XLSX."
    await m.answer(
        "–ü—Ä–∏–≤–µ—Ç! üíñ –Ø –≥–æ—Ç–æ–≤–∞ –ø–æ–º–æ—á—å —Å –ø–æ–¥–±–æ—Ä–æ–º —ç–∫—Ä–∞–Ω–æ–≤.\n"
        f"{status}\n\n"
        "‚ñ∂Ô∏è –ù–∞–∂–º–∏ /help, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å –ø—Ä–∏–º–µ—Ä—ã –∫–æ–º–∞–Ω–¥.",
        reply_markup=make_main_menu()
    )

@router.message(Command("ping"))
async def ping_cmd(m: Message):
    await m.answer("pong")

# ---------- –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∫—ç—à–∞ ----------
@router.message(Command("cache_info"))
async def cache_info(m: Message):
    try:
        lines = [
            f"CACHE_DIR: {CACHE_DIR}",
            f"exists: {CACHE_DIR.exists()}",
            f"writable: {os.access(CACHE_DIR, os.W_OK)}",
            f"CACHE_CSV exists: {CACHE_CSV.exists()}",
            f"CACHE_META exists: {CACHE_META.exists()}",
            f"diag: {_cache_diag()}",
        ]
        await m.answer("\n".join(lines))
    except Exception as e:
        await m.answer(f"cache_info error: {e}")


# ---------- —Å—Ç–∞—Ç—É—Å ----------
@router.message(Command("status"))
async def cmd_status(m: types.Message):
    base = (OBDSP_BASE or "").strip()
    tok  = (OBDSP_TOKEN or "").strip()
    screens_count = len(SCREENS) if SCREENS is not None else 0
    text = [
        "üìä *OmniDSP Bot Status*",
        f"‚Ä¢ API Base: `{base or '‚Äî'}`",
        f"‚Ä¢ Token: {'‚úÖ' if tok else '‚ùå –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}",
        f"‚Ä¢ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —ç–∫—Ä–∞–Ω–æ–≤: *{screens_count}*",
    ]
    if screens_count and "city" in SCREENS.columns:
        try:
            sample_cities = ", ".join(SCREENS['city'].dropna().astype(str).unique()[:5])
            text.append(f"‚Ä¢ –ü—Ä–∏–º–µ—Ä –≥–æ—Ä–æ–¥–æ–≤: {sample_cities}")
        except Exception:
            pass
    await m.answer("\n".join(text), parse_mode="Markdown")

# ---------- diag / help ----------
@router.message(Command("diag_env"))
async def cmd_diag_env(m: types.Message):
    tok = (OBDSP_TOKEN or "").strip()
    base = (OBDSP_BASE or "").strip()
    sslv = (os.getenv("OBDSP_SSL_VERIFY","") or "").strip()
    shown = f"{tok[:6]}‚Ä¶{tok[-6:]}" if tok else "(empty)"
    await m.answer(
        "ENV:\n"
        f"BASE={base}\n"
        f"TOKEN_LEN={len(tok)} TOKEN={shown}\n"
        f"OBDSP_SSL_VERIFY={sslv}\n"
    )

@router.message(Command("diag_whoami_force"))
async def diag_whoami_force(m: types.Message):
    try:
        base = (OBDSP_BASE or "https://proddsp.omniboard360.io").rstrip("/")
        tok  = (OBDSP_TOKEN or "").strip().strip('"').strip("'")
        if not tok:
            await m.answer("OBDSP_TOKEN –ø—É—Å—Ç –≤–Ω—É—Ç—Ä–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞.")
            return
        url = f"{base}/api/v1.0/users/current"
        headers = {"Authorization": f"Bearer {tok}", "Accept": "application/json"}
        timeout = aiohttp.ClientTimeout(total=20)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(url, headers=headers, ssl=_make_ssl_param_for_aiohttp()) as resp:
                text = await resp.text()
                await m.answer(
                    f"GET {url}\n"
                    f"sent_header=Authorization: Bearer <{len(tok)} chars>\n"
                    f"status={resp.status}\n"
                    f"body={text[:900]}"
                )
    except Exception as e:
        await m.answer(f"–û—à–∏–±–∫–∞: {e}")

@router.message(Command("help"))
async def cmd_help(m: types.Message):
    await m.answer(HELP, reply_markup=make_main_menu())

@router.message(Command("diag_url"))
async def cmd_diag_url(m: types.Message):
    base = (OBDSP_BASE or "").strip().rstrip("/")
    root = f"{base}/api/v1.0/clients/inventories"
    await m.answer(f"GET {root}\n(–ø—Ä–∏–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã) {root}?page=0&size=1")

@router.message(Command("examples"))
async def cmd_examples(m: types.Message):
    text = (
        "üîç –ü—Ä–∏–º–µ—Ä—ã:\n"
        "‚Ä¢ /near 55.714349 37.553834 2\n"
        "‚Ä¢ /near 55.714349 37.553834 2 fields=screen_id\n"
        "‚Ä¢ /pick_city –ú–æ—Å–∫–≤–∞ 20 fields=screen_id\n"
        "‚Ä¢ /pick_city –ú–æ—Å–∫–≤–∞ 20 format=city fields=screen_id\n"
        "‚Ä¢ /pick_city –ú–æ—Å–∫–≤–∞ 20 format=billboard,supersite mix=billboard:70%,supersite:30% fields=screen_id\n"
        "‚Ä¢ /pick_at 55.75 37.62 25 15\n"
    )
    await m.answer(text, reply_markup=make_main_menu())

# ---------- C–∏–Ω–∫ –∏–∑ API ----------
@router.message(Command("sync_api"))
async def cmd_sync_api(m: types.Message):
    if not _owner_only(m.from_user.id):
        await m.answer("‚õîÔ∏è –¢–æ–ª—å–∫–æ –≤–ª–∞–¥–µ–ª–µ—Ü –±–æ—Ç–∞ –º–æ–∂–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å —ç—Ç—É –∫–æ–º–∞–Ω–¥—É.")
        return

    text = (m.text or "").strip()
    parts = text.split()[1:]

    def _get_opt(name, cast, default):
        for p in parts:
            if p.startswith(name + "="):
                val = p.split("=", 1)[1]
                try:
                    return cast(val)
                except Exception:
                    return default
        return default

    def _as_list(s):
        return [x.strip() for x in str(s).replace(";",",").replace("|",",").split(",") if x.strip()] if s else []

    pages_limit = _get_opt("pages", int, None)
    page_size   = _get_opt("size", int, 500)
    total_limit = _get_opt("limit", int, None)

    city     = _get_opt("city", str, "").strip()
    formats  = _as_list(_get_opt("formats", str, "") or _get_opt("format", str, ""))
    owners   = _as_list(_get_opt("owners", str, "")  or _get_opt("owner", str, ""))

    raw_api = {}
    for p in parts:
        if p.startswith("api.") and "=" in p:
            k, v = p.split("=", 1)
            raw_api[k[4:]] = v

    filters = {"city": city, "formats": formats, "owners": owners, "api_params": raw_api}

    pretty = []
    if city:    pretty.append(f"city={city}")
    if formats: pretty.append(f"formats={','.join(formats)}")
    if owners:  pretty.append(f"owners={','.join(owners)}")
    if raw_api: pretty.append("+" + "&".join(f"{k}={v}" for k, v in raw_api.items()))
    hint = (" (—Ñ–∏–ª—å—Ç—Ä—ã: " + ", ".join(pretty) + ")") if pretty else ""
    await m.answer("‚è≥ –¢—è–Ω—É –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å –∏–∑ –≤–Ω–µ—à–Ω–µ–≥–æ API‚Ä¶" + hint)

    try:
        items = await _fetch_inventories(
            pages_limit=pages_limit,
            page_size=page_size,
            total_limit=total_limit,
            m=m,
            filters=filters,
        )
    except Exception as e:
        logging.exception("sync_api failed")
        await m.answer(f"üö´ –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∏–Ω–∫–Ω—É—Ç—å: {e}")
        return

    if not items:
        await m.answer("API –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫.")
        return

    df = _normalize_api_to_df(items)
    if df.empty:
        await m.answer("–°–ø–∏—Å–æ–∫ –ø—Ä–∏—à—ë–ª, –Ω–æ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—É—Å—Ç–æ (–ø—Ä–æ–≤–µ—Ä—å –º–∞–ø–ø–∏–Ω–≥ –ø–æ–ª–µ–π).")
        return

    # –í –ø–∞–º—è—Ç—å + –∫—ç—à
    global SCREENS
    SCREENS = df
    try:
        if save_screens_cache(df):
            await m.answer(f"üíæ –ö—ç—à —Å–æ—Ö—Ä–∞–Ω—ë–Ω –Ω–∞ –¥–∏—Å–∫: {len(df)} —Å—Ç—Ä–æ–∫.")
        else:
            await m.answer("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à –Ω–∞ –¥–∏—Å–∫.")
    except Exception as e:
        await m.answer(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫—ç—à–∞: {e}")

    # –§–∞–π–ª—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
    try:
        csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
        await m.bot.send_document(
            m.chat.id,
            BufferedInputFile(csv_bytes, filename="inventories_sync.csv"),
            caption=f"–ò–Ω–≤–µ–Ω—Ç–∞—Ä—å –∏–∑ API: {len(df)} —Å—Ç—Ä–æ–∫ (CSV)"
        )
    except Exception as e:
        await m.answer(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å CSV: {e}")

    try:
        xlsx_buf = io.BytesIO()
        with pd.ExcelWriter(xlsx_buf, engine="openpyxl") as writer:
            df.to_excel(writer, index=False, sheet_name="inventories")
        xlsx_buf.seek(0)
        await m.bot.send_document(
            m.chat.id,
            BufferedInputFile(xlsx_buf.getvalue(), filename="inventories_sync.xlsx"),
            caption=f"–ò–Ω–≤–µ–Ω—Ç–∞—Ä—å –∏–∑ API: {len(df)} —Å—Ç—Ä–æ–∫ (XLSX)"
        )
    except Exception as e:
        await m.answer(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å XLSX: {e} (–ø—Ä–æ–≤–µ—Ä—å, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ openpyxl)")

    await m.answer(f"‚úÖ –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –æ–∫: {len(df)} —ç–∫—Ä–∞–Ω–æ–≤.")

# ---------- –§–æ—Ç–æ–æ—Ç—á—ë—Ç—ã ----------
@router.message(Command("shots"))
async def cmd_shots(m: types.Message):
    if not _owner_only(m.from_user.id):
        await m.answer("‚õîÔ∏è –¢–æ–ª—å–∫–æ –≤–ª–∞–¥–µ–ª–µ—Ü –±–æ—Ç–∞ –º–æ–∂–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å —ç—Ç—É –∫–æ–º–∞–Ω–¥—É.")
        return

    text = (m.text or "").strip()
    parts = text.split()[1:]

    def _get_opt(name, cast, default):
        for p in parts:
            if p.startswith(name + "="):
                v = p.split("=", 1)[1]
                try:
                    return cast(v)
                except:
                    return default
        return default

    def _get_str(name, default=""):
        for p in parts:
            if p.startswith(name + "="):
                return p.split("=", 1)[1]
        return default

    campaign_id = _get_opt("campaign", int, None)
    per         = _get_opt("per", int, 0)
    limit       = _get_opt("limit", int, None)
    want_zip    = str(_get_str("zip", "0")).lower() in {"1","true","yes","on"}
    fields_req  = _get_str("fields", "").strip()
    dbg         = str(_get_str("dbg", "0")).lower() in {"1","true","yes","on"}

    if not campaign_id:
        await m.answer("–§–æ—Ä–º–∞—Ç: /shots campaign=<ID> [per=0] [limit=100] [zip=1] [fields=...]")
        return

    await m.answer(f"‚è≥ –°–æ–±–∏—Ä–∞—é —Ñ–æ—Ç–æ–æ—Ç—á—ë—Ç –ø–æ –∫–∞–º–ø–∞–Ω–∏–∏ {campaign_id}‚Ä¶")

    try:
        shots = await _fetch_impression_shots(
            campaign_id, per=per, want_zip=want_zip, m=m, dbg=dbg
        )
    except Exception as e:
        await m.answer(f"üö´ –û—à–∏–±–∫–∞ API: {e}")
        return

    # ZIP –∫–µ–π—Å
    if shots and isinstance(shots, list) and isinstance(shots[0], dict) and shots[0].get("__binary__") and want_zip:
        body = shots[0]["__body__"]
        await m.bot.send_document(
            m.chat.id,
            BufferedInputFile(body, filename=f"shots_{campaign_id}.zip"),
            caption="ZIP —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ (—ç–∫—Å–ø–æ—Ä—Ç –æ—Ç —Å–µ—Ä–≤–µ—Ä–∞)"
        )
        return

    df = _normalize_shots(shots)
    if limit and not df.empty and len(df) > limit:
        df = df.head(limit)

    if df.empty:
        await m.answer("–§–æ—Ç–æ–æ—Ç—á—ë—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return

    if fields_req:
        cols = [c.strip() for c in fields_req.split(",") if c.strip()]
        cols = [c for c in cols if c in df.columns]
        if not cols:
            await m.answer("–ü–æ–ª—è –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω—ã. –î–æ—Å—Ç—É–ø–Ω—ã–µ: " + ", ".join(df.columns))
            return
        view = df[cols].copy()
        csv_bytes = view.to_csv(index=False).encode("utf-8-sig")
        await m.bot.send_document(
            m.chat.id,
            BufferedInputFile(csv_bytes, filename=f"shots_{campaign_id}.csv"),
            caption=f"–ö–∞–¥—Ä—ã –∫–∞–º–ø–∞–Ω–∏–∏ {campaign_id} (–ø–æ–ª—è: {', '.join(cols)})"
        )
    else:
        # –ü–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä
        try:
            csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
            await m.bot.send_document(
                m.chat.id,
                BufferedInputFile(csv_bytes, filename=f"shots_{campaign_id}.csv"),
                caption=f"–§–æ—Ç–æ–æ—Ç—á—ë—Ç –∫–∞–º–ø–∞–Ω–∏–∏ {campaign_id}: {len(df)} —Å—Ç—Ä–æ–∫ (CSV)"
            )
        except Exception as e:
            await m.answer(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å CSV: {e}")

        try:
            xbuf = io.BytesIO()
            with pd.ExcelWriter(xbuf, engine="openpyxl") as w:
                df.to_excel(w, index=False, sheet_name="shots")
            xbuf.seek(0)
            await m.bot.send_document(
                m.chat.id,
                BufferedInputFile(xbuf.getvalue(), filename=f"shots_{campaign_id}.xlsx"),
                caption=f"–§–æ—Ç–æ–æ—Ç—á—ë—Ç –∫–∞–º–ø–∞–Ω–∏–∏ {campaign_id}: {len(df)} —Å—Ç—Ä–æ–∫ (XLSX)"
            )
        except Exception as e:
            await m.answer(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å XLSX: {e} (–ø—Ä–æ–≤–µ—Ä—å openpyxl)")

    # –õ–æ–∫–∞–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ ZIP (–µ—Å–ª–∏ ask zip=1 –∏ —Å–µ—Ä–≤–µ—Ä –Ω–µ –¥–∞–ª ZIP)
    if want_zip:
        urls = [u for u in (df["image_url"].dropna().tolist() or []) if isinstance(u, str) and u.startswith("http")]
        if not urls:
            await m.answer("–ù–µ—Ç —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, zip –Ω–µ —Å–æ–±—Ä–∞–Ω.")
            return
        await m.answer(f"üì¶ –°–∫–∞—á–∏–≤–∞—é {len(urls)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π‚Ä¶")
        ssl_param = _make_ssl_param_for_aiohttp()
        timeout = aiohttp.ClientTimeout(total=300)

        zip_buf = io.BytesIO()
        import zipfile
        async with aiohttp.ClientSession(timeout=timeout) as session, zipfile.ZipFile(zip_buf, "w", zipfile.ZIP_DEFLATED) as zf:
            sem = asyncio.Semaphore(8)
            async def grab(i, url):
                async with sem:
                    try:
                        async with session.get(url, ssl=ssl_param) as r:
                            if r.status == 200:
                                content = await r.read()
                                zf.writestr(f"shot_{i:05d}.jpg", content)
                    except Exception:
                        pass
            await asyncio.gather(*[grab(i, u) for i, u in enumerate(urls, 1)])
        zip_buf.seek(0)
        await m.bot.send_document(
            m.chat.id,
            BufferedInputFile(zip_buf.getvalue(), filename=f"shots_{campaign_id}.zip"),
            caption="ZIP —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏"
        )

# ---------- Forecast ----------
@router.message(Command("forecast"))
async def cmd_forecast(m: types.Message):
    global LAST_RESULT
    if LAST_RESULT is None or LAST_RESULT.empty:
        await m.answer("–ù–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤—ã–±–æ—Ä–∫–∏. –°–Ω–∞—á–∞–ª–∞ –ø–æ–¥–±–µ—Ä–∏—Ç–µ —ç–∫—Ä–∞–Ω—ã (/pick_city, /pick_any, /pick_at, /near –∏–ª–∏ —á–µ—Ä–µ–∑ /ask).")
        return

    parts = (m.text or "").strip().split()[1:]
    kv: dict[str,str] = {}
    for p in parts:
        if "=" in p:
            k, v = p.split("=", 1)
            kv[k.strip().lower()] = v.strip()

    budget = None
    if "budget" in kv:
        try:
            v = kv["budget"].lower().replace(" ", "")
            if v.endswith("m"): budget = float(v[:-1]) * 1_000_000
            elif v.endswith("k"): budget = float(v[:-1]) * 1_000
            else: budget = float(v)
        except Exception:
            budget = None

    days = int(kv.get("days", 7)) if str(kv.get("days","")).isdigit() else 7
    hours_per_day = None
    if "hours_per_day" in kv:
        try: hours_per_day = int(kv["hours_per_day"])
        except Exception: hours_per_day = None
    hours = kv.get("hours", "")
    win_hours = _parse_hours_windows(hours) if hours else None
    if hours_per_day is None:
        hours_per_day = (win_hours if (win_hours is not None) else 8)

    base = LAST_RESULT.copy()
    base = _fill_min_bid(base)
    mb_valid = pd.to_numeric(base["min_bid_used"], errors="coerce").dropna()
    if mb_valid.empty:
        await m.answer("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ü–µ–Ω–∏—Ç—å —Å—Ç–∞–≤–∫—É: –Ω–∏ —É –æ–¥–Ω–æ–≥–æ —ç–∫—Ä–∞–Ω–∞ –Ω–µ—Ç minBid (–∏ –Ω–µ—á–µ–≥–æ –ø–æ–¥—Å—Ç–∞–≤–∏—Ç—å).")
        return
    avg_min = float(mb_valid.mean())

    n_screens = len(base)
    capacity  = n_screens * days * hours_per_day * MAX_PLAYS_PER_HOUR

    if budget is not None:
        total_slots = int(budget // avg_min)
        total_slots = min(total_slots, capacity)
    else:
        total_slots = capacity
        budget = total_slots * avg_min

    per_screen = _distribute_slots_evenly(n_screens, total_slots)

    base = base.reset_index(drop=True)
    base["planned_slots"] = per_screen
    base["planned_cost"]  = base["planned_slots"] * pd.to_numeric(base["min_bid_used"], errors="coerce").fillna(avg_min)

    total_cost  = float(base["planned_cost"].sum())
    total_slots = int(base["planned_slots"].sum())

    export_cols = []
    for c in ("screen_id","name","city","format","owner","lat","lon","minBid","min_bid_used","min_bid_source","planned_slots","planned_cost"):
        if c in base.columns:
            export_cols.append(c)
    plan_df = base[export_cols].copy()

    try:
        csv_bytes = plan_df.to_csv(index=False).encode("utf-8-sig")
        await m.bot.send_document(
            m.chat.id,
            BufferedInputFile(csv_bytes, filename=f"forecast_{LAST_SELECTION_NAME}.csv"),
            caption=f"–ü—Ä–æ–≥–Ω–æ–∑ (—Å—Ä–µ–¥–Ω. minBid‚âà{avg_min:,.0f}): {total_slots} –≤—ã—Ö–æ–¥–æ–≤, –±—é–¥–∂–µ—Ç‚âà{total_cost:,.0f} ‚ÇΩ"
        )
    except Exception as e:
        await m.answer(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å CSV: {e}")

    try:
        xbuf = io.BytesIO()
        with pd.ExcelWriter(xbuf, engine="openpyxl") as w:
            plan_df.to_excel(w, index=False, sheet_name="forecast")
        xbuf.seek(0)
        await m.bot.send_document(
            m.chat.id,
            BufferedInputFile(xbuf.getvalue(), filename=f"forecast_{LAST_SELECTION_NAME}.xlsx"),
            caption=f"–ü—Ä–æ–≥–Ω–æ–∑ (–ø–æ–¥—Ä–æ–±–Ω–æ): –¥–Ω–∏={days}, —á–∞—Å—ã/–¥–µ–Ω—å={hours_per_day}, max {MAX_PLAYS_PER_HOUR}/—á–∞—Å"
        )
    except Exception as e:
        await m.answer(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å XLSX: {e}")

# ---------- PLAN (–±—é–¥–∂–µ—Ç ‚Üí –ø–æ–¥–±–æ—Ä —ç–∫—Ä–∞–Ω–æ–≤ –∏ –ø–ª–∞–Ω –ø–æ–∫–∞–∑–æ–≤) ----------
def _as_list_any(sep_str: str | None) -> list[str]:
    if not sep_str:
        return []
    s = sep_str.replace(";", ",").replace("|", ",")
    return [x.strip() for x in s.split(",") if x.strip()]

def _priority_mask_by_formats(df: pd.DataFrame, tokens: list[str]) -> pd.DataFrame:
    """–û—Å—Ç–∞–≤–∏—Ç—å –≤ df —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏, —É –∫–æ—Ç–æ—Ä—ã—Ö format –ø–æ–ø–∞–¥–∞–µ—Ç –≤ tokens (—É—á—ë—Ç CITY –∞–ª–∏–∞—Å–æ–≤)."""
    if "format" not in df.columns or not tokens:
        return df.copy()
    col = df["format"].astype(str).str.upper()
    mask = None
    for tok in tokens:
        if tok.lower() in {"city","city_format","cityformat","citylight","–≥–∏–¥","–≥–∏–¥—ã"}:
            m = col.str.startswith("CITY_FORMAT")
        else:
            m = (col == tok.upper())
        mask = m if mask is None else (mask | m)
    return df[mask].copy()

def _prefer_formats(df: pd.DataFrame, n: int) -> pd.DataFrame:
    """–ï—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –Ω–µ –∑–∞–¥–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º: —Å–Ω–∞—á–∞–ª–∞ BILLBOARD, –ø–æ—Ç–æ–º SUPERSITE, –ø–æ—Ç–æ–º CITY_FORMAT*, –∑–∞—Ç–µ–º –æ—Å—Ç–∞–ª—å–Ω–æ–µ.
       –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É–ª –∏–∑ –Ω–µ –±–æ–ª–µ–µ n*3-4 —Å—Ç—Ä–æ–∫ (—á—Ç–æ–±—ã –±—ã–ª–æ –∏–∑ —á–µ–≥–æ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –≤—ã–±–∏—Ä–∞—Ç—å)."""
    if "format" not in df.columns or df.empty:
        return df
    wanted = []
    # 1) BILLBOARD
    bb = df[df["format"].astype(str).str.upper().eq("BILLBOARD")]
    wanted.append(bb)
    # 2) SUPERSITE
    ss = df[df["format"].astype(str).str.upper().eq("SUPERSITE")]
    wanted.append(ss)
    # 3) CITY_*
    cc = df[df["format"].astype(str).str.upper().str.startswith("CITY_FORMAT")]
    wanted.append(cc)
    # 4) –û—Å—Ç–∞–ª—å–Ω–æ–µ
    other = df[~df.index.isin(pd.concat(wanted, ignore_index=False).index)]
    wanted.append(other)
    # —Å–∫–ª–µ–∏–º, –Ω–æ —á—É—Ç—å –æ–≥—Ä–∞–Ω–∏—á–∏–º —Ä–∞–∑–º–µ—Ä, —á—Ç–æ–±—ã spread_select —Ä–∞–±–æ—Ç–∞–ª —à—É—Å—Ç—Ä–µ–µ
    pool = pd.concat(wanted, ignore_index=True)
    return pool.head(max(n * 5, n))  # –Ω–µ–±–æ–ª—å—à–æ–π –∑–∞–ø–∞—Å

@router.message(Command("plan"))
async def cmd_plan(m: types.Message):
    global SCREENS
    if SCREENS is None or SCREENS.empty:
        await m.answer("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å (CSV/XLSX) –∏–ª–∏ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ /sync_api.")
        return

    # ---- –ø–∞—Ä—Å–∏–Ω–≥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ----
    parts = (m.text or "").strip().split()[1:]
    kv: dict[str,str] = {}
    for p in parts:
        if "=" in p:
            k, v = p.split("=", 1)
            kv[k.strip().lower()] = v.strip()

    # –±—é–¥–∂–µ—Ç (–æ–±—è–∑.)
    budget_raw = kv.get("budget") or kv.get("b")
    if not budget_raw:
        await m.answer("–ù—É–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –±—é–¥–∂–µ—Ç: /plan budget=200000 [city=...] [format=...] [owner=...] [n=10] [days=10] [hours_per_day=8] [top=1]")
        return
    try:
        v = budget_raw.lower().replace(" ", "")
        if v.endswith("m"):
            budget_total = float(v[:-1]) * 1_000_000
        elif v.endswith("k"):
            budget_total = float(v[:-1]) * 1_000
        else:
            budget_total = float(v)
    except Exception:
        await m.answer("–ù–µ –ø–æ–Ω—è–ª –±—é–¥–∂–µ—Ç. –ü—Ä–∏–º–µ—Ä: budget=200000 –∏–ª–∏ budget=200k")
        return

    # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ
    city   = kv.get("city")
    n      = int(kv["n"]) if kv.get("n","").isdigit() else 10
    days   = int(kv["days"]) if kv.get("days","").isdigit() else 10
    # —á–∞—Å—ã: –ª–∏–±–æ hours_per_day=8, –ª–∏–±–æ windows hours=07-10,17-21
    hours_per_day = int(kv["hours_per_day"]) if kv.get("hours_per_day","").isdigit() else None
    if hours_per_day is None:
        win = _parse_hours_windows(kv.get("hours"))
        hours_per_day = win if (win is not None) else 8

    formats = _as_list_any(kv.get("format") or kv.get("formats"))
    owners  = _as_list_any(kv.get("owner")  or kv.get("owners"))
    want_top = str(kv.get("top","0")).lower() in {"1","true","yes","on"} or \
               str(kv.get("coverage","0")).lower() in {"1","true","yes","on"}

    # ---- —Ñ–æ—Ä–º–∏—Ä—É–µ–º –ø—É–ª ----
    pool = SCREENS.copy()
    # city
    if city and "city" in pool.columns:
        pool = pool[pool["city"].astype(str).str.strip().str.lower() == city.strip().lower()]
    if pool.empty:
        await m.answer("–ü–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –≥–æ—Ä–æ–¥—É –Ω–µ—Ç —ç–∫—Ä–∞–Ω–æ–≤ (—Å —É—á—ë—Ç–æ–º –≤–≤–æ–¥–Ω—ã—Ö).")
        return
    # filters (format/owner)
    if formats:
        pool = _priority_mask_by_formats(pool, formats)
    if owners:
        pool = apply_filters(pool, {"owner": ",".join(owners)})
    if pool.empty:
        await m.answer("–ü–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ñ–∏–ª—å—Ç—Ä–æ–≤ —ç–∫—Ä–∞–Ω–æ–≤ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å.")
        return

    # minBid –æ–±–æ–≥–∞—â–µ–Ω–∏–µ
    pool = _fill_min_bid(pool)

    # –µ—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –Ω–µ —É–∫–∞–∑–∞–Ω ‚Äî –æ—Ç–¥–∞—ë–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç BB‚ÜíSUPERSITE‚ÜíCITY‚Üí–æ—Å—Ç–∞–ª—å–Ω—ã–µ
    if not formats:
        pool = _prefer_formats(pool, n)

    # ---- –≤—ã–±–æ—Ä —ç–∫—Ä–∞–Ω–æ–≤: top –ø–æ OTS –∏–ª–∏ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ ----
    if want_top and "ots" in pool.columns:
        # –±–µ—Ä—ë–º —Ç–æ–ø –ø–æ OTS (–µ—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥–æ—Ä–æ–¥–æ–≤ ‚Äî –≤ —Ä–∞–º–∫–∞—Ö —Ç–µ–∫—É—â–µ–≥–æ city)
        # –µ—Å–ª–∏ OTS –ø—É—Å—Ç—ã ‚Äî fallback –∫ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–º—É
        try:
            ots_vals = pd.to_numeric(pool["ots"], errors="coerce")
            if ots_vals.dropna().empty:
                raise ValueError("empty ots")
            pool = pool.assign(_ots=ots_vals).sort_values("_ots", ascending=False)
            selected = pool.head(n).drop(columns=["_ots"])
        except Exception:
            selected = spread_select(pool.reset_index(drop=True), n, random_start=True, seed=None)
    else:
        selected = spread_select(pool.reset_index(drop=True), n, random_start=True, seed=None)

    if selected.empty:
        await m.answer("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–±—Ä–∞—Ç—å —ç–∫—Ä–∞–Ω—ã (—Å–ª–∏—à–∫–æ–º —Å—Ç—Ä–æ–≥–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è?).")
        return

    # ---- —Ä–∞—Å—á—ë—Ç –ø–ª–∞–Ω–æ–≤ ----
    # –±—é–¥–∂–µ—Ç/–¥–µ–Ω—å/—ç–∫—Ä–∞–Ω
    budget_per_day_per_screen = budget_total / max(n, 1) / max(days, 1)

    # —Ñ–ª–∞–≥ —Å—Ç–∞–≤–∫–∏
    mb = pd.to_numeric(selected["min_bid_used"], errors="coerce")
    # –ø–æ–¥—Å—Ç–∞–≤–∏–º –º–µ–¥–∏–∞–Ω—É, –µ—Å–ª–∏ —É –∫–æ–≥–æ-—Ç–æ NaN
    median_mb = float(mb.dropna().median()) if not mb.dropna().empty else 0.0
    mb = mb.fillna(median_mb)

    # –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ —Å–ª–æ—Ç—ã –≤ –¥–µ–Ω—å –ø–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–º—É –ª–∏–º–∏—Ç—É
    per_day_cap = hours_per_day * PLAN_MAX_PLAYS_PER_HOUR

    # —Ä–∞—Å—á—ë—Ç —Å–ª–æ—Ç–æ–≤/–¥–µ–Ω—å –∏ –∏—Ç–æ–≥–æ–≤
    slots_per_day = (budget_per_day_per_screen // mb).astype(int)
    slots_per_day = slots_per_day.clip(lower=0, upper=per_day_cap)
    total_slots = slots_per_day * days
    planned_cost = total_slots * mb

    out = selected.copy()
    out["budget_per_day"] = round(budget_per_day_per_screen, 2)
    out["min_bid_used"] = mb
    out["planned_slots_per_day"] = slots_per_day
    out["total_slots"] = total_slots
    out["planned_cost"] = planned_cost

    # ---- —ç–∫—Å–ø–æ—Ä—Ç ----
    try:
        csv_bytes = out.to_csv(index=False).encode("utf-8-sig")
        await m.bot.send_document(
            m.chat.id,
            BufferedInputFile(csv_bytes, filename="plan.csv"),
            caption=(
                f"–ü–ª–∞–Ω: –±—é–¥–∂–µ—Ç={budget_total:,.0f} ‚ÇΩ, n={n}, days={days}, "
                f"hours/day={hours_per_day}, cap={PLAN_MAX_PLAYS_PER_HOUR}/—á–∞—Å"
            ).replace(",", " ")
        )
    except Exception as e:
        await m.answer(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å CSV: {e}")

    try:
        xbuf = io.BytesIO()
        with pd.ExcelWriter(xbuf, engine="openpyxl") as w:
            out.to_excel(w, index=False, sheet_name="plan")
        xbuf.seek(0)
        await m.bot.send_document(
            m.chat.id,
            BufferedInputFile(xbuf.getvalue(), filename="plan.xlsx"),
            caption="–ü–ª–∞–Ω (XLSX)"
        )
    except Exception as e:
        await m.answer(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å XLSX: {e}")

# ---------- –†–∞–¥–∏—É—Å, Near ----------
@router.message(Command("radius"))
async def set_radius(m: types.Message):
    try:
        r = float((m.text or "").split()[1])
        if r <= 0 or r > 50:
            raise ValueError
        USER_RADIUS[m.from_user.id] = r
        await m.answer(f"–†–∞–¥–∏—É—Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {r:.2f} –∫–º")
    except Exception:
        await m.answer("–£–∫–∞–∂–∏ —Ä–∞–¥–∏—É—Å –≤ –∫–º: /radius 2")

@router.message(Command("near"))
async def cmd_near(m: types.Message):
    global LAST_RESULT, SCREENS
    if SCREENS is None or SCREENS.empty:
        await m.answer("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —ç–∫—Ä–∞–Ω–æ–≤ (CSV/XLSX) –∏–ª–∏ /sync_api.")
        return

    parts = (m.text or "").strip().split()
    if len(parts) < 3:
        await m.answer("–§–æ—Ä–º–∞—Ç: /near lat lon [radius_km] [fields=screen_id]")
        return

    try:
        lat = float(parts[1]); lon = float(parts[2])
        radius = USER_RADIUS.get(m.from_user.id, DEFAULT_RADIUS)
        tail_from = 3
        if len(parts) >= 4 and "=" not in parts[3]:
            radius = float(parts[3].strip("[](){}"))
            tail_from = 4
        kwargs = {}
        for p in parts[tail_from:]:
            if "=" in p:
                k, v = p.split("=", 1)
                kwargs[k.strip().lower()] = v.strip().strip('"').strip("'")
    except Exception:
        await m.answer("–ü—Ä–∏–º–µ—Ä: /near 55.714349 37.553834 2 fields=screen_id")
        return

    res = find_within_radius(SCREENS, (lat, lon), radius)
    if res is None or res.empty:
        await m.answer(f"–í —Ä–∞–¥–∏—É—Å–µ {radius} –∫–º –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        return

    LAST_RESULT = res
    if kwargs.get("fields", "").lower() == "screen_id":
        ids = [str(x) for x in res.get("screen_id", pd.Series([""]*len(res))).tolist()]
        if not ids:
            await m.answer(f"–ù–∞–π–¥–µ–Ω–æ {len(res)} —ç–∫—Ä., –Ω–æ –∫–æ–ª–æ–Ω–∫–∞ screen_id –ø—É—Å—Ç–∞—è.")
            return
        await send_lines(m, ids, header=f"–ù–∞–π–¥–µ–Ω–æ {len(ids)} screen_id:", chunk=60)
        return

    lines = []
    for _, r in res.iterrows():
        sid = r.get("screen_id", "")
        name = r.get("name", "")
        dist = r.get("distance_km", "")
        fmt  = r.get("format", "")
        own  = r.get("owner", "")
        lines.append(f"‚Ä¢ {sid} ‚Äî {name} ({dist} –∫–º) [{fmt} / {own}]")
    await send_lines(m, lines, header=f"–ù–∞–π–¥–µ–Ω–æ: {len(res)} —ç–∫—Ä. –≤ —Ä–∞–¥–∏—É—Å–µ {radius} –∫–º", chunk=60)

# ---------- pick_city ----------
@router.message(Command("pick_city"))
async def pick_city(m: types.Message):
    global LAST_RESULT, SCREENS
    if SCREENS is None or SCREENS.empty:
        await m.answer("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å (CSV/XLSX –∏–ª–∏ /sync_api).")
        return

    parts = (m.text or "").strip().split()
    if len(parts) < 3:
        await m.answer("–§–æ—Ä–º–∞—Ç: /pick_city –ì–æ—Ä–æ–¥ N [format=...] [owner=...] [fields=...] [shuffle=1] [fixed=1] [seed=42]")
        return

    pos, keyvals = [], []
    for p in parts[1:]:
        (keyvals if "=" in p else pos).append(p)

    try:
        n = int(pos[-1])
        city = " ".join(pos[:-1]) if len(pos) > 1 else ""
        kwargs = parse_kwargs(keyvals)
        shuffle_flag = str(kwargs.get("shuffle", "0")).lower() in {"1","true","yes","on"}
        fixed        = str(kwargs.get("fixed",   "0")).lower() in {"1","true","yes","on"}
        seed         = int(kwargs["seed"]) if str(kwargs.get("seed","")).isdigit() else None
    except Exception:
        await m.answer("–ü—Ä–∏–º–µ—Ä: /pick_city –ú–æ—Å–∫–≤–∞ 20 format=BILLBOARD fields=screen_id shuffle=1")
        return

    if "city" not in SCREENS.columns:
        await m.answer("–í –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç —Å—Ç–æ–ª–±—Ü–∞ city. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /near –∏–ª–∏ /sync_api —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π.")
        return

    subset = SCREENS[SCREENS["city"].astype(str).str.strip().str.lower() == city.strip().lower()]
    subset = apply_filters(subset, kwargs) if not subset.empty and kwargs else subset

    if subset.empty:
        await m.answer(f"–ù–µ –Ω–∞—à—ë–ª —ç–∫—Ä–∞–Ω–æ–≤ –≤ –≥–æ—Ä–æ–¥–µ: {city} (—Å —É—á—ë—Ç–æ–º —Ñ–∏–ª—å—Ç—Ä–æ–≤).")
        return

    if shuffle_flag:
        subset = subset.sample(frac=1, random_state=None).reset_index(drop=True)

    res = spread_select(subset.reset_index(drop=True), n, random_start=not fixed, seed=seed)
    LAST_RESULT = res

    fields = parse_fields(kwargs.get("fields","")) if "fields" in kwargs else []
    if fields:
        view = res[fields]
        if fields == ["screen_id"]:
            # –±–µ—Ä—ë–º —Å—Ç—Ä–æ–≥–æ Series; –ø—Ä–∏ –¥—É–±–ª–∏–∫–∞—Ç–∞—Ö –∫–æ–ª–æ–Ω–æ–∫ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é —Å–ª–∞–π—Å–æ–º
            ser = res["screen_id"] if "screen_id" in res.columns else pd.Series(dtype=str)
            if isinstance(ser, pd.DataFrame):
                ser = ser.iloc[:, 0]
            ids = [s for s in ser.astype(str).tolist() if s and s.lower() != "nan"]
            await send_lines(m, ids, header=f"–í—ã–±—Ä–∞–Ω–æ {len(ids)} screen_id –ø–æ –≥–æ—Ä–æ–¥—É ¬´{city}¬ª:")
        else:
            lines = [" | ".join(str(row[c]) for c in fields) for _, row in view.iterrows()]
            await send_lines(m, lines, header=f"–í—ã–±—Ä–∞–Ω–æ {len(view)} —ç–∫—Ä–∞–Ω–æ–≤ –ø–æ –≥–æ—Ä–æ–¥—É ¬´{city}¬ª (–ø–æ–ª—è: {', '.join(fields)}):")
    else:
        lines = []
        for _, r in res.iterrows():
            nm  = r.get("name","") or r.get("screen_id","")
            fmt = r.get("format","") or ""
            own = r.get("owner","") or ""
            md  = r.get("min_dist_to_others_km", None)
            tail = f"(–º–∏–Ω. –¥–æ —Å–æ—Å–µ–¥–∞ {md} –∫–º)" if md is not None else ""
            lines.append(f"‚Ä¢ {r.get('screen_id','')} ‚Äî {nm} [{r['lat']:.5f},{r['lon']:.5f}] [{fmt} / {own}] {tail}".strip())
        await send_lines(m, lines, header=f"–í—ã–±—Ä–∞–Ω–æ {len(res)} —ç–∫—Ä–∞–Ω–æ–≤ –ø–æ –≥–æ—Ä–æ–¥—É ¬´{city}¬ª (—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ):")

    await send_gid_if_any(m, res, filename="city_screen_ids.xlsx", caption=f"GID –ø–æ –≥–æ—Ä–æ–¥—É ¬´{city}¬ª (XLSX)")

# ---------- pick_at ----------
def parse_mix(val: str) -> list[tuple[str, str]]:
    if not isinstance(val, str) or not val.strip():
        return []
    s = val.replace("|", ",").replace(";", ",")
    items = []
    for part in s.split(","):
        part = part.strip()
        if not part or ":" not in part:
            continue
        token, v = part.split(":", 1)
        items.append((token.strip(), v.strip()))
    return items

def _allocate_counts(total_n: int, mix_items: list[tuple[str, str]]) -> list[tuple[str, int]]:
    fixed: list[tuple[str, int]] = []
    perc:  list[tuple[str, float]] = []
    for token, v in mix_items:
        if v.endswith("%"):
            try: perc.append((token, float(v[:-1])))
            except: pass
        else:
            try: fixed.append((token, int(v)))
            except: pass
    fixed_sum = sum(cnt for _, cnt in fixed)
    remaining = max(0, total_n - fixed_sum)
    out: list[tuple[str, int]] = fixed[:]
    if remaining > 0 and perc:
        p_total = sum(p for _, p in perc) or 1.0
        raw = [(tok, remaining * p / p_total) for tok, p in perc]
        base = [(tok, int(x)) for tok, x in raw]
        used = sum(cnt for _, cnt in base)
        rem  = remaining - used
        fracs = sorted(((x - int(x), tok) for tok, x in raw), reverse=True)
        extra: dict[str,int] = {}
        for i in range(rem):
            _, tok = fracs[i % len(fracs)]
            extra[tok] = extra.get(tok, 0) + 1
        for tok, cnt in base:
            out.append((tok, cnt + extra.get(tok, 0)))
    total = sum(cnt for _, cnt in out)
    if total > total_n:
        delta = total - total_n
        trimmed = []
        for tok, cnt in out:
            take = max(0, cnt - delta)
            delta -= (cnt - take)
            trimmed.append((tok, take))
            if delta <= 0:
                trimmed.extend(out[len(trimmed):])
                break
        out = trimmed
    return out

def _select_with_mix(df_city: pd.DataFrame, n: int, mix_arg: str | None,
                     *, random_start: bool = True, seed: int | None = None) -> pd.DataFrame:
    if not mix_arg:
        return spread_select(df_city.reset_index(drop=True), n, random_start=random_start, seed=seed)
    items = parse_mix(mix_arg)
    if not items:
        return spread_select(df_city.reset_index(drop=True), n, random_start=random_start, seed=seed)

    allowed_tokens = [tok for tok, _ in items]

    if "format" not in df_city.columns:
        base_pool = df_city.copy()
    else:
        mask_allowed = None
        col = df_city["format"]
        for tok in allowed_tokens:
            m = _format_mask(col, tok)
            mask_allowed = m if mask_allowed is None else (mask_allowed | m)
        base_pool = df_city[mask_allowed] if mask_allowed is not None else df_city.copy()

    if base_pool.empty:
        return spread_select(df_city.reset_index(drop=True), n, random_start=random_start, seed=seed)

    targets = _allocate_counts(n, items)
    selected_parts: list[pd.DataFrame] = []
    used_ids: set[str] = set()
    pool = base_pool.copy()

    for token, need in targets:
        if need <= 0 or pool.empty:
            continue
        mask = _format_mask(pool["format"], token) if "format" in pool.columns else pd.Series([True]*len(pool), index=pool.index)
        subset = pool[mask]
        if subset.empty:
            continue
        pick_n = min(need, len(subset))
        picked = spread_select(subset.reset_index(drop=True), pick_n, random_start=random_start, seed=seed)
        selected_parts.append(picked)

        if "screen_id" in pool.columns and "screen_id" in picked.columns:
            chosen_ids = picked["screen_id"].astype(str).tolist()
            used_ids.update(chosen_ids)
            pool = pool[~pool["screen_id"].astype(str).isin(used_ids)]
        else:
            coords = set((float(a), float(b)) for a, b in picked[["lat","lon"]].itertuples(index=False, name=None))
            pool = pool[~((pool["lat"].astype(float).round(7).isin([x for x, _ in coords])) &
                          (pool["lon"].astype(float).round(7).isin([y for _, y in coords])))]
        if pool.empty:
            break

    combined = pd.concat(selected_parts, ignore_index=True) if selected_parts else base_pool.iloc[0:0]
    remain = n - len(combined)
    if remain > 0 and not pool.empty:
        extra = spread_select(pool.reset_index(drop=True), min(remain, len(pool)), random_start=random_start, seed=seed)
        combined = pd.concat([combined, extra], ignore_index=True)
    return combined.head(n)

@router.message(Command("pick_at"))
async def pick_at(m: types.Message):
    global LAST_RESULT, SCREENS
    if SCREENS is None or SCREENS.empty:
        await m.answer("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —ç–∫—Ä–∞–Ω–æ–≤ (CSV/XLSX) –∏–ª–∏ /sync_api.")
        return

    parts = (m.text or "").strip().split()
    if len(parts) < 4:
        await m.answer("–§–æ—Ä–º–∞—Ç: /pick_at lat lon N [radius_km] [mix=...] [fixed=1] [seed=42]")
        return

    try:
        lat, lon = float(parts[1]), float(parts[2])
        n = int(parts[3])
        radius = float(parts[4]) if len(parts) >= 5 and "=" not in parts[4] else 20.0
        kwargs = parse_kwargs(parts[5:] if len(parts) > 5 else [])
    except Exception:
        await m.answer("–ü—Ä–∏–º–µ—Ä: /pick_at 55.75 37.62 30 15")
        return

    circle = find_within_radius(SCREENS, (lat, lon), radius)
    if circle.empty:
        await m.answer(f"–í —Ä–∞–¥–∏—É—Å–µ {radius} –∫–º –Ω–µ—Ç —ç–∫—Ä–∞–Ω–æ–≤.")
        return

    fixed = str(kwargs.get("fixed", "0")).lower() in {"1","true","yes","on"}
    seed = int(kwargs["seed"]) if kwargs.get("seed","").isdigit() else None
    mix_arg = kwargs.get("mix") or kwargs.get("mix_formats")

    res = _select_with_mix(circle.reset_index(drop=True), n, mix_arg, random_start=not fixed, seed=seed)
    LAST_RESULT = res

    lines = []
    for _, r in res.iterrows():
        nm = r.get("name","") or r.get("screen_id","")
        fmt = r.get("format",""); own = r.get("owner","")
        md  = r.get("min_dist_to_others_km", 0)
        lines.append(f"‚Ä¢ {r.get('screen_id','')} ‚Äî {nm} [{r['lat']:.5f},{r['lon']:.5f}] [{fmt} / {own}] (–º–∏–Ω. –¥–æ —Å–æ—Å–µ–¥–∞ {md} –∫–º)")
    await send_lines(m, lines, header=f"–í—ã–±—Ä–∞–Ω–æ {len(res)} —ç–∫—Ä–∞–Ω–æ–≤ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –≤ —Ä–∞–¥–∏—É—Å–µ {radius} –∫–º:")

    await send_gid_if_any(m, res, filename="picked_at_screen_ids.xlsx", caption="GID (XLSX)")

# ---------- Export last ----------
async def send_gid_xlsx(chat_id: int, ids: list[str], *, filename: str = "screen_ids.xlsx", caption: str = "GID —Å–ø–∏—Å–æ–∫ (XLSX)"):
    df = pd.DataFrame({"GID": [str(x) for x in ids]})
    bio = io.BytesIO()
    with pd.ExcelWriter(bio, engine="openpyxl") as writer:
        df.to_excel(writer, index=False)
    bio.seek(0)
    await bot.send_document(
        chat_id,
        BufferedInputFile(bio.read(), filename=filename),
        caption=caption,
    )

async def send_gid_if_any(message: types.Message, df: pd.DataFrame, *, filename: str, caption: str):
    if df is None or df.empty or "screen_id" not in df.columns:
        return
    ids = [s for s in (df["screen_id"].astype(str).tolist()) if str(s).strip() and str(s).lower() != "nan"]
    if ids:
        await send_gid_xlsx(message.chat.id, ids, filename=filename, caption=caption)

@router.message(Command("export_last"))
async def export_last(m: types.Message):
    global LAST_RESULT
    if LAST_RESULT is None or LAST_RESULT.empty:
        await m.answer("–ü–æ–∫–∞ –Ω–µ—á–µ–≥–æ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å. –°–Ω–∞—á–∞–ª–∞ —Å–¥–µ–ª–∞–π—Ç–µ –≤—ã–±–æ—Ä–∫—É (/near, /pick_city, /pick_at).")
        return
    csv_bytes = LAST_RESULT.to_csv(index=False).encode("utf-8-sig")
    await m.bot.send_document(
        m.chat.id,
        BufferedInputFile(csv_bytes, filename="selection.csv"),
        caption="–ü–æ—Å–ª–µ–¥–Ω—è—è –≤—ã–±–æ—Ä–∫–∞ (CSV)",
    )

# ---------- –ü—Ä–∏—ë–º CSV/XLSX ----------
@router.message(F.content_type.in_({"document"}))
async def on_file(m: types.Message):
    """–ü—Ä–∏–Ω–∏–º–∞–µ–º CSV/XLSX –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å."""
    try:
        tg_file = await m.bot.get_file(m.document.file_id)
        file_bytes = await m.bot.download_file(tg_file.file_path)
        data = file_bytes.read()

        if m.document.file_name.lower().endswith(".xlsx"):
            df = pd.read_excel(io.BytesIO(data))
        else:
            try:
                df = pd.read_csv(io.BytesIO(data), encoding="utf-8-sig")
            except Exception:
                df = pd.read_csv(io.BytesIO(data))

        rename_map = {
            "Screen_ID":"screen_id","ScreenId":"screen_id","id":"screen_id","ID":"screen_id",
            "Name":"name","–ù–∞–∑–≤–∞–Ω–∏–µ":"name",
            "Latitude":"lat","Lat":"lat","–®–∏—Ä–æ—Ç–∞":"lat",
            "Longitude":"lon","Lon":"lon","–î–æ–ª–≥–æ—Ç–∞":"lon",
            "City":"city","–ì–æ—Ä–æ–¥":"city",
            "Format":"format","–§–æ—Ä–º–∞—Ç":"format",
            "Owner":"owner","–í–ª–∞–¥–µ–ª–µ—Ü":"owner","–û–ø–µ—Ä–∞—Ç–æ—Ä":"owner"
        }
        df = df.rename(columns=rename_map)

        if not {"lat","lon"}.issubset(df.columns):
            await m.answer("–ù—É–∂–Ω—ã –∫–æ–ª–æ–Ω–∫–∏ –º–∏–Ω–∏–º—É–º: lat, lon. (–û–ø—Ü.: screen_id, name, city, format, owner)")
            return

        df["lat"] = pd.to_numeric(df["lat"], errors="coerce")
        df["lon"] = pd.to_numeric(df["lon"], errors="coerce")
        df = df.dropna(subset=["lat","lon"])

        for col in ["screen_id","name","city","format","owner"]:
            if col not in df.columns:
                df[col] = ""

        global SCREENS
        SCREENS = df[["screen_id","name","lat","lon","city","format","owner"]].reset_index(drop=True)

        # —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à
        try:
            save_screens_cache(SCREENS)
        except Exception:
            pass

        await m.answer(
            f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —ç–∫—Ä–∞–Ω–æ–≤: {len(SCREENS)}.\n"
            "–¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ: –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –≥–µ–æ–ª–æ–∫–∞—Ü–∏—é üìç, /near lat lon [R], /pick_city –ì–æ—Ä–æ–¥ N, /pick_at lat lon N [R]."
        )
    except Exception as e:
        await m.answer(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª: {e}")

# ---------- Fallback ----------
@router.message(F.text)
async def fallback_text(m: types.Message):
    t = (m.text or "").strip()
    if t.startswith("/"):
        await m.answer("–Ø –≤–∞—Å –ø–æ–Ω—è–ª–∞, –Ω–æ —Ç–∞–∫–æ–π –∫–æ–º–∞–Ω–¥—ã –Ω–µ—Ç. –ù–∞–∂–º–∏—Ç–µ /help –¥–ª—è —Å–ø–∏—Å–∫–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π.", reply_markup=make_main_menu())
    else:
        await m.answer(
            "–ß—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å, –ø—Ä–∏—à–ª–∏—Ç–µ —Ñ–∞–π–ª CSV/XLSX —Å —ç–∫—Ä–∞–Ω–∞–º–∏, –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ /near, /pick_city, /pick_at.\n"
            "–ù–∞–∂–º–∏—Ç–µ /help, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å –ø—Ä–∏–º–µ—Ä—ã.",
            reply_markup=make_main_menu()
        )

# ---------- –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —Ä–æ—É—Ç–µ—Ä–∞ –∏ –∑–∞–ø—É—Å–∫ ----------
dp.include_router(router)

# ---------- NLU-–ø–æ–¥—Å–∫–∞–∑–∫–∏ –ø–æ —Å–≤–æ–±–æ–¥–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É (safe, –±–µ–∑ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ —Å –∫–æ–º–∞–Ω–¥–∞–º–∏) ----------
import re
from aiogram import Router, F, types
from aiogram.utils.text_decorations import html_decoration as hd

# –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç (–Ω–µ –∫–æ–º–∞–Ω–¥—ã, –Ω–µ –æ—Ç –±–æ—Ç–æ–≤)
nlu_router.message.filter(F.text, ~F.text.regexp(r"^/"), ~F.via_bot)

# ===== helpers =====

def _parse_money(s: str) -> float | None:
    """
    –ò—â–µ—Ç —Å—É–º–º—É –¥–µ–Ω–µ–≥ –≤ —Ç–µ–∫—Å—Ç–µ.
    –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: '–±—é–¥–∂–µ—Ç ...' -> –∏–Ω–∞—á–µ –ø–µ—Ä–≤–æ–µ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ —á–∏—Å–ª–æ.
    –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å—É—Ñ—Ñ–∏–∫—Å–æ–≤: –∫/K (—Ç—ã—Å), –º/M (–º–ª–Ω).
    –ü—Ä–∏–º–µ—Ä—ã: '250000', '250 000', '200–∫', '1.5–º', '–±—é–¥–∂–µ—Ç 250k'
    """
    if not s:
        return None
    t = s.lower()

    # 1) –°–ø–µ—Ä–≤–∞ –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é —Å –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º "–±—é–¥–∂–µ—Ç"
    m = re.search(
        r"(?:–±—é–¥–∂–µ—Ç|budget)\s*[:=]?\s*"
        r"(\d{1,3}(?:[ \u00A0]?\d{3})+|\d+(?:[.,]\d+)?)\s*([–∫k–ºm])?\b",
        t,
        flags=re.IGNORECASE
    )
    if not m:
        # 2) –ò–Ω–∞—á–µ ‚Äî –ø–µ—Ä–≤–æ–µ "—Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–µ" —á–∏—Å–ª–æ —Å –≤–æ–∑–º–æ–∂–Ω—ã–º —Å—É—Ñ—Ñ–∏–∫—Å–æ–º
        m = re.search(
            r"\b(\d{1,3}(?:[ \u00A0]?\d{3})+|\d+(?:[.,]\d+)?)\s*([–∫k–ºm])?\b",
            t,
            flags=re.IGNORECASE
        )
    if not m:
        return None

    num = m.group(1)
    suf = (m.group(2) or "").lower()

    # —É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã-—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —Ç—ã—Å—è—á –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∑–∞–ø—è—Ç—É—é –∫ —Ç–æ—á–∫–µ
    num = num.replace(" ", "").replace("\u00A0", "").replace(",", ".")
    try:
        val = float(num)
    except ValueError:
        return None

    if suf == "–º" or suf == "m":
        val *= 1_000_000
    elif suf == "–∫" or suf == "k":
        val *= 1_000

    return val

def _parse_int(s: str) -> int | None:
    m = re.search(r"\b(\d{1,6})\b", s or "")
    return int(m.group(1)) if m else None

def _normalize_city_token(raw: str) -> str:
    """–ü–µ—Ä–µ–≤–æ–¥–∏–º '–≤ –º–æ—Å–∫–≤–µ', '—Å–ø–±', '–ø–∏—Ç–µ—Ä–µ' –∏ —Ç.–ø. –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º—É –≤–∏–¥—É –¥–ª—è –∫–æ–º–∞–Ω–¥—ã."""
    t = (raw or "").strip(" .,!?:;\"'()").lower()
    t = re.sub(r"^(?:–≥–æ—Ä–æ–¥|–≥\.)\s+", "", t)
    specials = {
        "–º—Å–∫": "–ú–æ—Å–∫–≤–∞", "–º–æ—Å–∫–≤–∞": "–ú–æ—Å–∫–≤–∞", "–≤ –º–æ—Å–∫–≤–µ": "–ú–æ—Å–∫–≤–∞", "–ø–æ –º–æ—Å–∫–≤–µ": "–ú–æ—Å–∫–≤–∞", "–∏–∑ –º–æ—Å–∫–≤—ã": "–ú–æ—Å–∫–≤–∞", "–º–æ—Å–∫–≤–µ": "–ú–æ—Å–∫–≤–∞",
        "—Å–ø–±": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥", "–ø–∏—Ç–µ—Ä": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥", "–ø–∏—Ç–µ—Ä–µ": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥",
        "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥", "—Å–∞–Ω–∫—Ç –ø–µ—Ç–µ—Ä–±—É—Ä–≥": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥",
        "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥–µ": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥", "—Å–∞–Ω–∫—Ç –ø–µ—Ç–µ—Ä–±—É—Ä–≥–µ": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥",
        "–ø–µ—Ç–µ—Ä–±—É—Ä–≥": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥", "–≤ —Å–ø–±": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥", "–≤ –ø–∏—Ç–µ—Ä–µ": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥",
        "–∫–∞–∑–∞–Ω—å": "–ö–∞–∑–∞–Ω—å", "–≤ –∫–∞–∑–∞–Ω–∏": "–ö–∞–∑–∞–Ω—å", "–∫–∞–∑–∞–Ω–∏": "–ö–∞–∑–∞–Ω—å",
        "–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫", "–≤ –Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–µ": "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫", "–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–µ": "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫",
        "–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥", "–≤ –µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥–µ": "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥", "–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥–µ": "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥", 
        "–Ω–∏–∂–Ω–∏–π –Ω–æ–≤–≥–æ—Ä–æ–¥": "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥", "–≤ –Ω–∏–∂–Ω–µ–º –Ω–æ–≤–≥–æ—Ä–æ–¥–µ": "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥", "–Ω–∏–∂–Ω–µ–º –Ω–æ–≤–≥–æ—Ä–æ–¥–µ": "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥",
        "—Ç–≤–µ—Ä—å": "–¢–≤–µ—Ä—å", "–≤ —Ç–≤–µ—Ä–∏": "–¢–≤–µ—Ä—å", "—Ç–≤–µ—Ä–∏": "–¢–≤–µ—Ä—å",
        "—Å–∞–º–∞—Ä–∞": "–°–∞–º–∞—Ä–∞", "–≤ —Å–∞–º–∞—Ä–µ": "–°–∞–º–∞—Ä–∞", "—Å–∞–º–∞—Ä–µ": "–°–∞–º–∞—Ä–∞",
        "—Ä–æ—Å—Ç–æ–≤-–Ω–∞-–¥–æ–Ω—É": "–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É", "–≤ —Ä–æ—Å—Ç–æ–≤–µ-–Ω–∞-–¥–æ–Ω—É": "–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É", "—Ä–æ—Å—Ç–æ–≤–µ-–Ω–∞-–¥–æ–Ω—É": "–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É",
        "–≤–æ—Ä–æ–Ω–µ–∂": "–í–æ—Ä–æ–Ω–µ–∂", "–≤ –≤–æ—Ä–æ–Ω–µ–∂–µ": "–í–æ—Ä–æ–Ω–µ–∂", "–≤–æ—Ä–æ–Ω–µ–∂–µ": "–í–æ—Ä–æ–Ω–µ–∂",
        "–ø–µ—Ä–º—å": "–ü–µ—Ä–º—å", "–≤ –ø–µ—Ä–º–∏": "–ü–µ—Ä–º—å", "–ø–µ—Ä–º–∏": "–ü–µ—Ä–º—å",
        "—É—Ñ–∞": "–£—Ñ–∞", "–≤ —É—Ñ–µ": "–£—Ñ–∞", "—É—Ñ–µ": "–£—Ñ–∞",
    }
    if t in specials:
        return specials[t]
    # –≥—Ä—É–±–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Å—Ç–Ω–æ–≥–æ –ø–∞–¥–µ–∂–∞: –ú–æ—Å–∫–≤–µ -> –ú–æ—Å–∫–≤–∞, –¢–≤–µ—Ä–∏ -> –¢–≤–µ—Ä–∏ (–æ—Å—Ç–∞–≤–∏–º –∫–∞–∫ –µ—Å—Ç—å)
    if t.endswith("–µ") and len(t) >= 4:
        t = t[:-1] + "–∞"
    t = re.sub(r"\s{2,}", " ", t).strip()
    # –¢–∞–π—Ç–ª-–∫–µ–π—Å (–¥–ª—è ¬´–ù–∏–∂–Ω–∏–π –Ω–æ–≤–≥–æ—Ä–æ–¥¬ª –æ—Å—Ç–∞–≤–∏–º –∫–∞–∫ –µ—Å—Ç—å ‚Äî —ç—Ç–æ —É–ø—Ä–æ—â—ë–Ω–∫–∞)
    return t.capitalize() if t else ""

def _extract_city(text: str) -> str | None:
    """–î–æ—Å—Ç–∞—ë–º –≥–æ—Ä–æ–¥ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ë–ï–ó –ø—Ä–µ–¥–ª–æ–≥–∞, –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π."""
    # –ø–æ—Å–ª–µ –ø—Ä–µ–¥–ª–æ–≥–æ–≤
    m = re.search(r"(?:^|\s)(?:–≤|–ø–æ|–∏–∑)\s+([–ê-–ØA-Z–Å][\w\- ]{1,40})", text or "", flags=re.IGNORECASE)
    if m:
        cand = re.split(r"[,.!?:;0-9]", m.group(1).strip())[0]
        norm = _normalize_city_token(cand)
        return norm or None
    # —è–≤–Ω—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è
    low = (text or "").lower()
    for key in ("–º–æ—Å–∫–≤–∞", "–º—Å–∫", "—Å–ø–±", "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥", "—Å–∞–Ω–∫—Ç –ø–µ—Ç–µ—Ä–±—É—Ä–≥", "–ø–∏—Ç–µ—Ä"):
        if key in low:
            return _normalize_city_token(key)
    return None

def _extract_latlon(text: str):
    m = re.search(r"(-?\d{1,2}\.\d+)[, ]+(-?\d{1,3}\.\d+)", text or "")
    if m:
        try:
            return float(m.group(1)), float(m.group(2))
        except Exception:
            return None
    return None

def _has_any(text: str, words: list[str]) -> bool:
    t = (text or "").lower()
    return any(w in t for w in words)

def _extract_formats(text: str) -> list[str]:
    """
    –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–Ω—è—Ç—å —Ñ–æ—Ä–º–∞—Ç—ã –∏–∑ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–∫–µ–Ω—ã –≤ —Ç–æ–º –≤–∏–¥–µ, –∫–∞–∫ –∏—Ö –∂–¥—É—Ç —Ñ–∏–ª—å—Ç—Ä—ã (–≤–µ—Ä—Ö–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä),
    —á—Ç–æ–±—ã apply_filters –º–æ–≥ —Å—Ä–∞–≤–Ω–∏—Ç—å –ø–æ —Ç–æ—á–Ω–æ–º—É —Ä–∞–≤–µ–Ω—Å—Ç–≤—É.
    """
    t = (text or "").lower()
    fmts = []

    # –±–∏–ª–±–æ—Ä–¥—ã
    if any(w in t for w in ("billboard", "–±–∏–ª–±–æ—Ä", "–±–∏–ª–ª–±–æ—Ä", "–±–∏–ª–±–æ—Ä–¥", "–±–∏–ª–±–æ—Ä–¥—ã", "–±–∏–ª–ª–±–æ—Ä–¥", "–±–∏–ª–ª–±–æ—Ä–¥—ã", "bb", "dbb", "–±–±")):
        fmts.append("BILLBOARD")

    # —Å—É–ø–µ—Ä—Å–∞–π—Ç—ã
    if any(w in t for w in ("supersite", "—Å—É–ø–µ—Ä—Å–∞–π—Ç", "—Å—É–ø–µ—Ä—Å–∞–∏ÃÜ—Ç", "—Å—É–ø–µ—Ä—Å–∞–π—Ç—ã", "ss", "dss")):
        fmts.append("SUPERSITE")

    # —Å–∏—Ç–∏–±–æ—Ä–¥—ã 
    if any(w in t for w in ("cb", "—Å–∏—Ç–∏–∫", "—Å–∏—Ç–∏–±–æ—Ä–¥", "—Å–∏—Ç–∏–±–æ—Ä–¥—ã", "cityboard", "city board", "dcb", "—Å–∏—Ç–∏ –±–æ—Ä–¥", "—Å–∏—Ç–∏ –±–æ—Ä–¥—ã", "—Å–∏—Ç–∏-–±–æ—Ä–¥", "—Å–∏—Ç–∏-–±–æ—Ä–¥—ã")):
        fmts.append("CITY_BOARD")

    # —Å–∏—Ç–∏—Ñ–æ—Ä–º–∞—Ç—ã 
    if any(w in t for w in ("cf", "—Å–∏—Ç–∏—Ñ–æ—Ä–º–∞—Ç", "—Å–∏—Ç–∏—Ñ–æ—Ä–º–∞—Ç—ã", "—Å–∏—Ç–∏ —Ñ–æ—Ä–º–∞—Ç—ã", "—Å–∏—Ç–∏ —Ñ–æ—Ä–º–∞—Ç", "—Å–∏—Ç–∏-—Ñ–æ—Ä–º–∞—Ç", "dcf", "—Å–∏—Ç–∏-—Ñ–æ—Ä–º–∞—Ç—ã")):
        fmts.append("CITY_FORMAT")    

    # —ç–∫—Ä–∞–Ω—ã –Ω–∞ –º—Ü–∫ 
    if any(w in t for w in ("–º—Ü–∫", "—ç–∫—Ä–∞–Ω—ã –Ω–∞ –º—Ü–∫")):
        fmts.append("CITY_FORMAT_RC")    

    # —ç–∫—Ä–∞–Ω—ã –≤ –º–µ—Ç—Ä–æ 
    if any(w in t for w in ("—ç–∫—Ä–∞–Ω—ã –≤ –º–µ—Ç—Ä–æ", "–º–µ—Ç—Ä–æ")):
        fmts.append("CITY_FORMAT_WD")

    # —ç–∫—Ä–∞–Ω—ã –Ω–∞ –≤–æ–∫–∑–∞–ª–∞—Ö 
    if any(w in t for w in ("—ç–∫—Ä–∞–Ω—ã –Ω–∞ –≤–æ–∫–∑–∞–ª–µ", "—ç–∫—Ä–∞–Ω—ã –Ω–∞ –≤–æ–∫–∑–∞–ª–∞—Ö", "–≤–æ–∫–∑–∞–ª", "–≤–æ–∫–∑–∞–ª—ã")):
        fmts.append("CITY_FORMAT_RD")

    # –º–µ–¥–∏a—Ñ–∞—Å–∞–¥—ã / —Ñ–∞—Å–∞–¥—ã
    if any(w in t for w in ("–º–µ–¥–∏–∞—Ñ–∞—Å–∞–¥", "–º–µ–¥–∏–∞—Ñ–∞—Å–∞–¥—ã", "—Ñ–∞—Å–∞–¥", "—Ñ–∞—Å–∞–¥—ã", "mediafacade", "media facade", "—Ñ–∞—Å–∞–¥–æ–≤")):
        fmts.append("MEDIAFACADE")
    
    # —ç–∫—Ä–∞–Ω—ã –≤ –ø–æ–º–µ—â–µ–Ω–∏—è—Ö 
    if any(w in t for w in ("–∏–Ω–¥–æ—Ä", "–∏–Ω–¥–æ—Ä–Ω—ã–µ —ç–∫—Ä–∞–Ω—ã", "—ç–∫—Ä–∞–Ω—ã –≤ –ø–æ–º–µ—â–µ–Ω–∏—è—Ö", "–≤–Ω—É—Ç—Ä–∏ –ø–æ–º–µ—â–µ–Ω–∏–π", "indoor", "–≤ –¢–¶", "–≤ —Ç—Ü", "—Ç–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä", "—Ç–æ—Ä–≥–æ–≤—ã–µ —Ü–µ–Ω—Ç—Ä—ã")):
        fmts.append("OTHER")

    # —ç–∫—Ä–∞–Ω—ã –≤ –∞—ç—Ä–æ–ø–æ—Ä—Ç–∞—Ö 
    if any(w in t for w in ("–∞—ç—Ä–æ–ø–æ—Ä—Ç—ã", "—ç–∫—Ä–∞–Ω –≤ –∞—ç—Ä–æ–ø–æ—Ä—Ç—É", "—ç–∫—Ä–∞–Ω –≤ –∞—ç—Ä–æ–ø–æ—Ä—Ç–∞—Ö", "airport", "airports", "—ç–∫—Ä–∞–Ω—ã –≤ –∞—ç—Ä–æ–ø–æ—Ä—Ç—É", "—ç–∫—Ä–∞–Ω—ã –≤ –∞—ç—Ä–æ–ø–æ—Ä—Ç–∞—Ö")):
        fmts.append("SKY_DIGITAL")

    # —ç–∫—Ä–∞–Ω—ã –≤ –ø–≤–∑ 
    if any(w in t for w in ("–ø–≤–∑", "—ç–∫—Ä–∞–Ω –≤ –ø–≤–∑", "—ç–∫—Ä–∞–Ω—ã –≤ –ø–≤–∑", "—ç–∫—Ä–∞–Ω—ã –≤ –ø—É–Ω–∫—Ç–∞—Ö –≤—ã–¥–∞—á–∏", "—ç–∫—Ä–∞–Ω –≤ –ø—É–Ω–∫—Ç–µ –≤—ã–¥–∞—á–∏", "pickup point", "pickup points", "—ç–∫—Ä–∞–Ω –≤ –ø—É–Ω–∫—Ç–µ –≤—ã–¥–∞—á–∏ –∑–∞–∫–∞–∑–æ–≤", "—ç–∫—Ä–∞–Ω—ã –≤ –ø—É–Ω–∫—Ç–∞—Ö –≤—ã–¥–∞—á–∏ –∑–∞–∫–∞–∑–æ–≤", "–ø–≤–∑ wildberries", "–ø–≤–∑ –≤–±", "—ç–∫—Ä–∞–Ω –≤ –ø–≤–∑ wildberries", "—ç–∫—Ä–∞–Ω—ã –≤ –ø–≤–∑ wildberries")):
        fmts.append("PVZ_SCREEN")


    # —É–±–µ—Ä—ë–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω–∏–º –ø–æ—Ä—è–¥–æ–∫ –ø–µ—Ä–≤–æ–≥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è
    seen = set()
    out = []
    for f in fmts:
        if f not in seen:
            out.append(f); seen.add(f)
    return out

def _extract_owners(text: str) -> list[str]:
    t = (text or "")
    # –ª–æ–≤–∏–º owner=..., –∞ —Ç–∞–∫–∂–µ ¬´–≤–ª–∞–¥–µ–ª–µ—Ü(a/u) <—Å–ª–æ–≤–∞>¬ª –∏ ¬´–æ–ø–µ—Ä–∞—Ç–æ—Ä <—Å–ª–æ–≤–∞>¬ª
    m = re.search(r"(?:owner|–≤–ª–∞–¥–µ–ª–µ—Ü|–≤–ª–∞–¥–µ–ª—å—Ü[–∞—É]|–æ–ø–µ—Ä–∞—Ç–æ—Ä)\s*[:=]?\s*([A-Za-z–ê-–Ø–∞-—è0-9_\-\s,;|]+)", t, flags=re.IGNORECASE)
    if not m:
        return []
    vals = re.split(r"[;,\|]\s*|\s+", m.group(1).strip())
    vals = [v for v in vals if v and not v.isdigit()]
    # —Å—Ä–µ–∂–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ö–≤–æ—Å—Ç—ã –ø–æ—Å–ª–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
    cleaned = []
    for v in vals:
        if v.lower() in {"format", "city", "days", "n", "budget", "hours", "hours_per_day"}:
            break
        cleaned.append(v)
    return cleaned

def suggest_command_from_text(text: str) -> tuple[str | None, str]:
    t = (text or "").strip()
    low = t.lower()

    # ---------- /plan ‚Äî –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥ –±—é–¥–∂–µ—Ç ----------
    if _has_any(low, ["–ø–ª–∞–Ω", "—Å–ø–ª–∞–Ω–∏—Ä—É–π", "–Ω–∞ –±—é–¥–∂–µ—Ç", "–ø–æ–¥ –±—é–¥–∂–µ—Ç", "–∫–∞–º–ø–∞–Ω", "—Ä–∞—Å–ø—Ä–µ–¥", "–ø–æ–∫–∞–∑—ã"]):
        budget = _parse_money(low) or 200_000
        n = _parse_int(low) or 10
        m_days = re.search(r"(\d+)\s*–¥–Ω", low)
        days = int(m_days.group(1)) if m_days else 10
        city_raw = _extract_city(t)
        city = _normalize_city_token(city_raw) if city_raw else "–ú–æ—Å–∫–≤–∞"
        fmts = _extract_formats(low)
        owners = _extract_owners(t)
        top = " top=1" if _has_any(low, ["–æ—Ö–≤–∞—Ç–Ω", "—Å–∞–º—ã–µ –æ—Ö–≤–∞—Ç–Ω—ã–µ", "–º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –æ—Ö–≤–∞—Ç", "coverage"]) else ""
        fmt_part = f" format={','.join(sorted(set(fmts)).upper() for fmts in [])}"  # placeholder (see below)
        # ‚Üë –º–∞–ª–µ–Ω—å–∫–∞—è —Ö–∏—Ç—Ä–æ—Å—Ç—å –Ω–∏–∂–µ: –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å–æ–±–µ—Ä—ë–º formats
        if fmts:
            fmt_norm = ",".join(s.upper() for s in sorted(set(fmts)))
            fmt_part = f" format={fmt_norm}"
        else:
            fmt_part = ""

        own_part = f" owner={','.join(owners)}" if owners else ""
        cmd = f"/plan budget={int(budget)} city={city} n={n} days={days}{fmt_part}{own_part}{top}"
        return cmd, "–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–º–ø–∞–Ω–∏–∏ –ø–æ–¥ –±—é–¥–∂–µ—Ç"

    # ---------- /pick_city ‚Äî —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –ø–æ –≥–æ—Ä–æ–¥—É ----------
    if _has_any(low, ["–ø–æ–¥–±–µ—Ä–∏", "–≤—ã–±–µ—Ä–∏", "–Ω—É–∂–Ω–æ", "—Ö–æ—á—É"]) and _has_any(low, ["–≤ ", "–ø–æ ", "–∏–∑ "]):
        city_raw = _extract_city(t)
        if city_raw:
            city = _normalize_city_token(city_raw)
            n = _parse_int(low) or 20
            # —Ñ–æ—Ä–º–∞—Ç—ã ‚Äî —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ —è–≤–Ω–æ —É–ø–æ–º—è–Ω—É—Ç—ã
            fmts = _extract_formats(low)
            fmt_part = f" format={','.join(s.upper() for s in sorted(set(fmts)))}" if fmts else ""
            # –≤–ª–∞–¥–µ–ª—å—Ü—ã ‚Äî –∏–∑ —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ ¬´–≤–ª–∞–¥–µ–ª–µ—Ü/–≤–ª–∞–¥–µ–ª—å—Ü–∞/–æ–ø–µ—Ä–∞—Ç–æ—Ä/owner¬ª
            owners = _extract_owners(t)
            own_part = f" owner={','.join(owners)}" if owners else ""
            return f"/pick_city {city} {n}{fmt_part}{own_part}", "–†–∞–≤–Ω–æ–º–µ—Ä–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –ø–æ –≥–æ—Ä–æ–¥—É"

    # ---------- /near ‚Äî —ç–∫—Ä–∞–Ω—ã —Ä—è–¥–æ–º / –≤ —Ä–∞–¥–∏—É—Å–µ ----------
    latlon = _extract_latlon(t)
    if latlon or _has_any(low, ["—Ä—è–¥–æ–º", "–æ–∫–æ–ª–æ", "–≤ —Ä–∞–¥–∏—É—Å–µ", "–≤–æ–∫—Ä—É–≥", "near", "–ø–æ–±–ª–∏–∑–æ—Å—Ç–∏"]):
        if latlon:
            return f"/near {latlon[0]:.6f} {latlon[1]:.6f} 2", "–≠–∫—Ä–∞–Ω—ã –≤ —Ä–∞–¥–∏—É—Å–µ —Ç–æ—á–∫–∏ (–ø—Ä–∏–º–µ—Ä: 2 –∫–º)"
        else:
            return "üìç –ü—Ä–∏—à–ª–∏—Ç–µ –≥–µ–æ–ª–æ–∫–∞—Ü–∏—é –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ: /near <lat> <lon> 2", "–≠–∫—Ä–∞–Ω—ã –≤–æ–∫—Ä—É–≥ –≤–∞—à–µ–π —Ç–æ—á–∫–∏"

    # ---------- /forecast ‚Äî –æ—Ü–µ–Ω–∫–∞ –ø–æ–∫–∞–∑–æ–≤ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤—ã–±–æ—Ä–∫–∏ ----------
    if _has_any(low, ["—Å–∫–æ–ª—å–∫–æ –ø–æ–∫–∞–∑", "–ø—Ä–æ–≥–Ω–æ–∑", "forecast", "—Ö–≤–∞—Ç–∏—Ç –ª–∏", "–æ—Ü–µ–Ω–∫–∞ –ø–æ–∫–∞–∑–æ–≤"]):
        budget = _parse_money(low)
        if budget:
            return f"/forecast budget={int(budget)} days=7 hours_per_day=8", "–û—Ü–µ–Ω–∫–∞ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤—ã–±–æ—Ä–∫–µ"
        else:
            return "/forecast days=7 hours_per_day=8", "–û—Ü–µ–Ω–∫–∞ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤—ã–±–æ—Ä–∫–µ"

    # ---------- /sync_api ‚Äî –ø–æ–¥—Ç—è–Ω—É—Ç—å –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å –∏–∑ API ----------
    if _has_any(low, ["–æ–±–Ω–æ–≤–∏ —Å–ø–∏—Å–æ–∫", "–ø–æ–¥—Ç—è–Ω–∏ –∏–∑ –∞–ø–∏", "—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–π", "–æ–±–Ω–æ–≤–∏ —ç–∫—Ä–∞–Ω—ã", "sync api"]):
        fmts = _extract_formats(low)
        city_raw = _extract_city(t)
        city = _normalize_city_token(city_raw) if city_raw else None
        parts = []
        if city: parts.append(f"city={city}")
        if fmts: parts.append(f"formats={','.join(s.upper() for s in sorted(set(fmts)))}")
        base = "/sync_api " + " ".join(parts) if parts else "/sync_api size=500 pages=3"
        return base.strip(), "–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∏–Ω–≤–µ–Ω—Ç–∞—Ä—è –∏–∑ API"

    # ---------- /shots ‚Äî —Ñ–æ—Ç–æ–æ—Ç—á—ë—Ç –ø–æ –∫–∞–º–ø–∞–Ω–∏–∏ ----------
    if _has_any(low, ["—Ñ–æ—Ç–æ–æ—Ç—á–µ—Ç", "—Ñ–æ—Ç–æ –æ—Ç—á—ë—Ç", "–∫–∞–¥—Ä—ã –∫–∞–º–ø–∞–Ω–∏–∏", "impression", "shots"]):
        camp = _parse_int(low) or 0
        if camp > 0:
            return f"/shots campaign={camp} per=0 limit=100", "–§–æ—Ç–æ–æ—Ç—á—ë—Ç –ø–æ –∫–∞–º–ø–∞–Ω–∏–∏"
        else:
            return "/shots campaign=<ID> per=0 limit=100", "–§–æ—Ç–æ–æ—Ç—á—ë—Ç: —É–∫–∞–∂–∏—Ç–µ campaign ID"

    # ---------- /export_last ‚Äî —ç–∫—Å–ø–æ—Ä—Ç ----------
    if _has_any(low, ["–≤—ã–≥—Ä—É–∑–∏", "—ç–∫—Å–ø–æ—Ä—Ç", "csv", "xlsx", "—Ç–∞–±–ª–∏—Ü–∞"]):
        return "/export_last", "–≠–∫—Å–ø–æ—Ä—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤—ã–±–æ—Ä–∫–∏"

    # ---------- /radius ‚Äî –∏–∑–º–µ–Ω–∏—Ç—å —Ä–∞–¥–∏—É—Å ----------
    if _has_any(low, ["—Ä–∞–¥–∏—É—Å", "–ø–æ—Å—Ç–∞–≤—å —Ä–∞–¥–∏—É—Å", "–∏–∑–º–µ–Ω–∏—Ç—å —Ä–∞–¥–∏—É—Å"]):
        r = _parse_int(low) or 2
        return f"/radius {r}", "–ó–∞–¥–∞—Ç—å —Ä–∞–¥–∏—É—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–∫–º)"

    # ---------- /status /help ----------
    if _has_any(low, ["—Å—Ç–∞—Ç—É—Å", "—á—Ç–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ", "—Å–∫–æ–ª—å–∫–æ —ç–∫—Ä–∞–Ω–æ–≤"]):
        return "/status", "–°—Ç–∞—Ç—É—Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
    if _has_any(low, ["help", "–ø–æ–º–æ—â", "—á—Ç–æ —É–º–µ–µ—à—å", "–∫–æ–º–∞–Ω–¥—ã"]):
        return "/help", "–°–ø—Ä–∞–≤–∫–∞ –ø–æ –∫–æ–º–∞–Ω–¥–∞–º"

    # –ù–∏—á–µ–≥–æ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–ª–∏ ‚Äî –º—è–≥–∫–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫ /help –∏ @enterspring
    return None, "–ü–æ—Ö–æ–∂–µ, –≥–æ—Ç–æ–≤–æ–π –∫–æ–º–∞–Ω–¥—ã –¥–ª—è —ç—Ç–æ–≥–æ –Ω–µ—Ç. –ù–∞–ø–∏—à–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, @enterspring ‚Äî –æ–Ω–∞ –ø–æ–º–æ–∂–µ—Ç –¥–æ–±–∞–≤–∏—Ç—å –Ω—É–∂–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é."

# ===== —Ö—ç–Ω–¥–ª–µ—Ä =====

@nlu_router.message()
async def natural_language_assistant(m: types.Message):
    text = (m.text or "").strip()
    cmd, hint = suggest_command_from_text(text)

    # –≥–ª—É—à–∏–º —Å—Ç—Ä–∞–Ω–Ω—ã–µ –Ω–µ–≤–∏–¥–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã, —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞–ª–∏ HTML
    def _clean(s: str) -> str:
        return (s or "").replace("\u200b", "").replace("\ufeff", "").strip()

    hint = _clean(hint)
    cmd  = _clean(cmd) if cmd else None

    # –°–æ–±–∏—Ä–∞–µ–º –æ—Ç–≤–µ—Ç –¢–û–õ–¨–ö–û —á–µ—Ä–µ–∑ hd.*, –±–µ–∑ ¬´—Ä—É—á–Ω—ã—Ö¬ª <b>/<i>/<code>
    header = "–ü–æ—Ö–æ–∂–µ, —Å—Ä–∞–±–æ—Ç–∞–µ—Ç —ç—Ç–æ:"
    parts = [hd.quote(header), ""]  # –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ = –ø–µ—Ä–µ–Ω–æ—Å

    if cmd:
        # –ï—Å–ª–∏ —ç—Ç–æ –∫–æ–º–∞–Ω–¥–∞ ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤ <code>, –∏–Ω–∞—á–µ –ø—Ä–æ—Å—Ç–æ –∫–∞–∫ —Ç–µ–∫—Å—Ç
        line = hd.bold("–°–æ–≤–µ—Ç—É—é –∫–æ–º–∞–Ω–¥—É") + " üëâ " + (hd.code(cmd) if cmd.startswith("/") else hd.quote(cmd))
        parts.append(line)
        if hint:
            parts += ["", hd.italic(hint)]
        body = "\n".join(parts)
    else:
        # –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–µ–π –∫–æ–º–∞–Ω–¥—ã ‚Äî –º—è–≥–∫–æ —à–ª—ë–º –∫ /help –∏ @enterspring
        tail = "–ê –ø–æ–∫–∞ –º–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã: /help"
        body = hd.quote(hint) + "\n\n" + hd.quote(tail)

    await m.answer(body, parse_mode="HTML", disable_web_page_preview=True)

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ NLU-—Ä–æ—É—Ç–µ—Ä–∞ –î–û–õ–ñ–ù–û –±—ã—Ç—å –≤—ã—à–µ, —á–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π:
dp.include_router(nlu_router)

async def main():
    try:
        load_screens_cache()
    except Exception as e:
        logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫—ç—à –Ω–∞ —Å—Ç–∞—Ä—Ç–µ: {e}")

    await bot.set_my_commands([
        BotCommand(command="start", description="–ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –±–æ—Ç –∂–∏–≤"),
        BotCommand(command="ping", description="–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–≤–µ—Ç–∞"),
        BotCommand(command="cache_info", description="–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∫—ç—à–∞"),
        BotCommand(command="status", description="–°—Ç–∞—Ç—É—Å –±–æ—Ç–∞ –∏ –∫—ç—à–∞"),
        BotCommand(command="sync_api", description="–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∏–Ω–≤–µ–Ω—Ç–∞—Ä—è –∏–∑ API"),
        BotCommand(command="shots", description="–§–æ—Ç–æ–æ—Ç—á—ë—Ç—ã –∫–∞–º–ø–∞–Ω–∏–∏"),
        BotCommand(command="forecast", description="–ü—Ä–æ–≥–Ω–æ–∑ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤—ã–±–æ—Ä–∫–µ"),
        BotCommand(command="near", description="–≠–∫—Ä–∞–Ω—ã –≤–æ–∑–ª–µ —Ç–æ—á–∫–∏"),
        BotCommand(command="pick_city", description="–†–∞–≤–Ω–æ–º–µ—Ä–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –ø–æ –≥–æ—Ä–æ–¥—É"),
        BotCommand(command="pick_at", description="–†–∞–≤–Ω–æ–º–µ—Ä–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –≤ –∫—Ä—É–≥–µ"),
        BotCommand(command="export_last", description="–≠–∫—Å–ø–æ—Ä—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤—ã–±–æ—Ä–∫–∏"),
        BotCommand(command="help", description="–°–ø—Ä–∞–≤–∫–∞"),
        BotCommand(command="plan", description="–ü–ª–∞–Ω –ø–æ–∫–∞–∑–∞: –±—é–¥–∂–µ—Ç ‚Üí —ç–∫—Ä–∞–Ω—ã ‚Üí —Å–ª–æ—Ç—ã"),
    ])

    await bot.delete_webhook(drop_pending_updates=True)
    me = await bot.get_me()
    logging.info(f"‚úÖ –ë–æ—Ç @{me.username} –∑–∞–ø—É—â–µ–Ω –∏ –∂–¥—ë—Ç —Å–æ–æ–±—â–µ–Ω–∏–π‚Ä¶")
    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())