# ====== imports ======
import os, io, math, asyncio, logging, time, json, ssl
from pathlib import Path
from datetime import datetime
import random
import pandas as pd
import aiohttp

try:
    import certifi  # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
except Exception:
    certifi = None

# aiogram 3.x
from aiogram import Bot, Dispatcher, F, types, Router
from aiogram.types import Message, BufferedInputFile
from aiogram.filters import Command

# ====== logging ======
logging.basicConfig(level=logging.INFO)

# ====== SSL helpers ======
def _ssl_ctx_certifi() -> ssl.SSLContext:
    """–°–æ–∑–¥–∞—ë—Ç –±–µ–∑–æ–ø–∞—Å–Ω—ã–π SSL-–∫–æ–Ω—Ç–µ–∫—Å—Ç —Å CA –∏–∑ certifi, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω."""
    if certifi is not None:
        ctx = ssl.create_default_context(cafile=certifi.where())
    else:
        ctx = ssl.create_default_context()
    ctx.check_hostname = True
    ctx.verify_mode = ssl.CERT_REQUIRED
    return ctx

def _make_ssl_param_for_aiohttp():
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è aiohttp ssl=...
    –ï—Å–ª–∏ OBDSP_SSL_NO_VERIFY=1, –æ—Ç–∫–ª—é—á–∞–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫—É —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞.
    """
    no_verify = os.getenv("OBDSP_SSL_NO_VERIFY", "0").strip().lower() in {"1", "true", "yes", "on"}
    if no_verify:
        return False
    return _ssl_ctx_certifi()

# ====== ENV CONFIG ======
OBDSP_BASE = os.getenv("OBDSP_BASE", "https://obdsp.projects.eraga.net").strip()
OBDSP_TOKEN = os.getenv("OBDSP_TOKEN", "").strip()
OBDSP_AUTH_SCHEME = os.getenv("OBDSP_AUTH_SCHEME", "Bearer").strip()
OBDSP_CLIENT_ID = os.getenv("OBDSP_CLIENT_ID", "").strip()

try:
    TELEGRAM_OWNER_ID = int(os.getenv("TELEGRAM_OWNER_ID", "0"))
except Exception:
    TELEGRAM_OWNER_ID = 0

OBDSP_CA_BUNDLE = os.getenv("OBDSP_CA_BUNDLE", "").strip()
OBDSP_SSL_VERIFY = (os.getenv("OBDSP_SSL_VERIFY", "1") or "1").strip().lower()  # "1"/"0"/"true"/"false"
OBDSP_SSL_NO_VERIFY = os.getenv("OBDSP_SSL_NO_VERIFY", "0").strip().lower() in {"1", "true", "yes", "on"}

# ====== PATHS & STATE ======
BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR / "data"
DATA_DIR.mkdir(parents=True, exist_ok=True)

# –ï–î–ò–ù–´–ô –∫—ç—à-–∫–∞—Ç–∞–ª–æ–≥ (–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å env-–ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π)
CACHE_DIR = Path(os.getenv("SCREENS_CACHE_DIR", "/tmp/omnika_cache"))
CACHE_DIR.mkdir(parents=True, exist_ok=True)

CACHE_CSV  = CACHE_DIR / "screens_cache.csv"
CACHE_META = CACHE_DIR / "screens_cache.meta.json"

SCREENS: pd.DataFrame | None = None
LAST_RESULT: pd.DataFrame | None = None
LAST_SELECTION_NAME = "last"
MAX_PLAYS_PER_HOUR = 6
LAST_SYNC_TS: float | None = None

def _cache_diag() -> str:
    """–°—Ç—Ä–æ–∫–∞-–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–ª—è –ª–æ–≥–æ–≤/–æ—Ç–≤–µ—Ç–∞: –≥–¥–µ –ø–∏—à–µ–º –∏ –µ—Å—Ç—å –ª–∏ –ø—Ä–∞–≤–∞."""
    try:
        can_write_dir = os.access(CACHE_DIR, os.W_OK)
        parent = CACHE_DIR.parent
        return (
            f"BASE_DIR={BASE_DIR} | CACHE_DIR={CACHE_DIR} "
            f"| exists={CACHE_DIR.exists()} | writable={can_write_dir} "
            f"| parent_writable={os.access(parent, os.W_OK)}"
        )
    except Exception as e:
        return f"diag_error={e}"

def save_screens_cache(df: pd.DataFrame) -> bool:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫—ç—à –Ω–∞ –¥–∏—Å–∫ (CSV + meta)."""
    global LAST_SYNC_TS
    try:
        if df is None or df.empty:
            logging.warning("save_screens_cache: –ø—É—Å—Ç–æ–π df ‚Äî —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –Ω–µ—á–µ–≥–æ")
            return False

        # —Ç–µ—Å—Ç –∑–∞–ø–∏—Å–∏
        try:
            (CACHE_DIR / ".write_test").write_text("ok", encoding="utf-8")
        except Exception as e:
            logging.error(f"write_test failed: {e} | {_cache_diag()}")
            return False

        df.to_csv(CACHE_CSV, index=False, encoding="utf-8-sig")

        LAST_SYNC_TS = time.time()
        meta = {"ts": LAST_SYNC_TS, "rows": int(len(df))}
        CACHE_META.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")

        logging.info(f"üíæ –ö—ç—à —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {len(df)} —Å—Ç—Ä–æ–∫ ‚Üí {CACHE_CSV} | {_cache_diag()}")
        return True
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫—ç—à–∞: {e} | {_cache_diag()}", exc_info=True)
        return False

def load_screens_cache() -> bool:
    """–ü—ã—Ç–∞–µ—Ç—Å—è –ø–æ–¥–Ω—è—Ç—å –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å –∏–∑ CSV. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True/False."""
    global SCREENS, LAST_SYNC_TS
    try:
        if not CACHE_CSV.exists():
            logging.info(f"–ö—ç—à CSV –Ω–µ –Ω–∞–π–¥–µ–Ω: {CACHE_CSV} | {_cache_diag()}")
            return False

        df = pd.read_csv(CACHE_CSV)
        if df is None or df.empty:
            logging.warning(f"–ö—ç—à CSV –ø—É—Å—Ç–æ–π: {CACHE_CSV}")
            return False

        SCREENS = df

        if CACHE_META.exists():
            meta = json.loads(CACHE_META.read_text(encoding="utf-8"))
            LAST_SYNC_TS = float(meta.get("ts")) if "ts" in meta else None
        else:
            LAST_SYNC_TS = None

        logging.info(f"Loaded screens cache: {len(SCREENS)} rows, ts={LAST_SYNC_TS} | {_cache_diag()}")
        return True
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫—ç—à–∞: {e} | {_cache_diag()}", exc_info=True)
        return False

# ====== BOT (aiogram 3.x) ======
BOT_TOKEN = os.getenv("BOT_TOKEN")
if not BOT_TOKEN:
    raise SystemExit("Set BOT_TOKEN env var first")

bot = Bot(BOT_TOKEN)
dp = Dispatcher()
router = Router()

# --- —Ç–µ—Å—Ç–æ–≤—ã–µ –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ö—ç–Ω–¥–ª–µ—Ä—ã ---

@router.message(Command("start"))
async def start_cmd(m: types.Message):
    await m.answer("–Ø –Ω–∞ –º–µ—Å—Ç–µ. –ü–æ–ø—Ä–æ–±—É–π /cache_info")

@router.message(Command("ping"))
async def ping_cmd(m: types.Message):
    await m.answer("pong")

# –¢–≤–æ–π –æ—Å–Ω–æ–≤–Ω–æ–π —Ö—ç–Ω–¥–ª–µ—Ä –∫–æ–º–∞–Ω–¥—ã
@router.message(Command("cache_info"))
async def cache_info(m: types.Message):
    try:
        lines = [
            f"CACHE_DIR: {CACHE_DIR}",
            f"exists: {CACHE_DIR.exists()}",
            f"writable: {os.access(CACHE_DIR, os.W_OK)}",
            f"CACHE_CSV exists: {CACHE_CSV.exists()}",
            f"CACHE_META exists: {CACHE_META.exists()}",
            f"diag: {_cache_diag()}",
        ]
        await m.answer("\n".join(lines))
    except Exception as e:
        await m.answer(f"cache_info error: {e}")

# –î—É–±–ª–∏—Ä—É–µ–º –º–∞—Ç—á–∏–Ω–≥ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π (–≤ –≥—Ä—É–ø–ø–∞—Ö/—Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –±–æ—Ç–∞)
@router.message(F.text.func(lambda t: isinstance(t, str) and t.strip().startswith(("/cache_info", "/cache_info@"))))
async def cache_info_fallback(m: types.Message):
    await cache_info(m)

# –õ–æ–≤–∏–º –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∞–ø–¥–µ–π—Ç—ã –∏ –ª–æ–≥–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –ø–æ–Ω–∏–º–∞—Ç—å, —á—Ç–æ –∏–º–µ–Ω–Ω–æ ¬´–Ω–µ handled¬ª
@router.message()
async def _log_unhandled_messages(m: types.Message):
    logging.info(f"Unhandled message caught by router: "
                 f"type={m.content_type}, chat={m.chat.type}, text={getattr(m, 'text', None)!r}")

@router.chat_member()   # —Å–µ—Ä–≤–∏—Å–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è (–¥–æ–±–∞–≤–∏–ª–∏/–∫–∏–∫–Ω—É–ª–∏ –±–æ—Ç–∞ –∏ —Ç.–ø.)
async def _log_chat_member(ev: types.ChatMemberUpdated):
    logging.info(f"ChatMember update: chat={ev.chat.id}, old={ev.old_chat_member.status}, new={ev.new_chat_member.status}")

async def main():
    try:
        load_screens_cache()
    except Exception as e:
        logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫—ç—à –Ω–∞ —Å—Ç–∞—Ä—Ç–µ: {e}")

    # –ü–æ–¥—Å–∫–∞–∑–∫–∏ –∫–æ–º–∞–Ω–¥
    from aiogram.types import BotCommand
    await bot.set_my_commands([
        BotCommand(command="start", description="–ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –±–æ—Ç –∂–∏–≤"),
        BotCommand(command="ping", description="–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–≤–µ—Ç–∞"),
        BotCommand(command="cache_info", description="–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∫—ç—à–∞"),
    ])

    await dp.start_polling(bot)

@router.message(Command("cache_info"))
async def cache_info(m: Message):
    try:
        lines = [
            f"CACHE_DIR: {CACHE_DIR}",
            f"exists: {CACHE_DIR.exists()}",
            f"writable: {os.access(CACHE_DIR, os.W_OK)}",
            f"CACHE_CSV exists: {CACHE_CSV.exists()}",
            f"CACHE_META exists: {CACHE_META.exists()}",
            f"diag: {_cache_diag()}",
        ]
        await m.answer("\n".join(lines))
    except Exception as e:
        await m.answer(f"cache_info error: {e}")

# —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –≤—Å–µ —Ö—ç–Ω–¥–ª–µ—Ä—ã —á–µ—Ä–µ–∑ router
dp.include_router(router)

async def main():
    # –µ—Å–ª–∏ –Ω—É–∂–Ω–æ ‚Äî –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –ø–æ–¥–Ω–∏–º–µ–º –∫—ç—à
    try:
        load_screens_cache()
    except Exception as e:
        logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫—ç—à –Ω–∞ —Å—Ç–∞—Ä—Ç–µ: {e}")

    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())

def _cache_diag() -> str:
    """–°—Ç—Ä–æ–∫–∞-–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–ª—è –ª–æ–≥–æ–≤/–æ—Ç–≤–µ—Ç–∞: –≥–¥–µ –ø–∏—à–µ–º –∏ –µ—Å—Ç—å –ª–∏ –ø—Ä–∞–≤–∞."""
    try:
        can_write_dir = os.access(CACHE_DIR, os.W_OK)
        parent = CACHE_DIR.parent
        return (
            f"BASE_DIR={BASE_DIR} | CACHE_DIR={CACHE_DIR} "
            f"| exists={CACHE_DIR.exists()} | writable={can_write_dir} "
            f"| parent_writable={os.access(parent, os.W_OK)}"
        )
    except Exception as e:
        return f"diag_error={e}"

def save_screens_cache(df: pd.DataFrame) -> bool:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫—ç—à –Ω–∞ –¥–∏—Å–∫ (CSV + meta). –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True/False. –°—É–ø–µ—Ä-–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞."""
    global LAST_SYNC_TS
    try:
        if df is None or df.empty:
            logging.warning("save_screens_cache: –ø—É—Å—Ç–æ–π df ‚Äî —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –Ω–µ—á–µ–≥–æ")
            return False

        # –ø—Ä–æ–±—É–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –∑–∞–ø–∏—Å—å ¬´–º–∞—è—á–∫–∞¬ª, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å –ø—Ä–∞–≤–∞
        try:
            (CACHE_DIR / ".write_test").write_text("ok", encoding="utf-8")
        except Exception as e:
            logging.error(f"write_test failed: {e} | {_cache_diag()}")
            return False

        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º CSV
        df.to_csv(CACHE_CSV, index=False, encoding="utf-8-sig")

        # –º–µ—Ç–∞
        LAST_SYNC_TS = time.time()
        meta = {"ts": LAST_SYNC_TS, "rows": int(len(df))}
        CACHE_META.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")

        logging.info(f"üíæ –ö—ç—à —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {len(df)} —Å—Ç—Ä–æ–∫ ‚Üí {CACHE_CSV} | {_cache_diag()}")
        return True
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫—ç—à–∞: {e} | {_cache_diag()}", exc_info=True)
        return False

def load_screens_cache() -> bool:
    """–ü—ã—Ç–∞–µ—Ç—Å—è –ø–æ–¥–Ω—è—Ç—å –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å –∏–∑ CSV. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True/False."""
    global SCREENS, LAST_SYNC_TS
    try:
        if not CACHE_CSV.exists():
            logging.info(f"–ö—ç—à CSV –Ω–µ –Ω–∞–π–¥–µ–Ω: {CACHE_CSV} | {_cache_diag()}")
            return False

        df = pd.read_csv(CACHE_CSV)
        if df is None or df.empty:
            logging.warning(f"–ö—ç—à CSV –ø—É—Å—Ç–æ–π: {CACHE_CSV}")
            return False

        SCREENS = df

        if CACHE_META.exists():
            meta = json.loads(CACHE_META.read_text(encoding="utf-8"))
            LAST_SYNC_TS = float(meta.get("ts")) if "ts" in meta else None
        else:
            LAST_SYNC_TS = None

        logging.info(f"Loaded screens cache: {len(SCREENS)} rows, ts={LAST_SYNC_TS} | {_cache_diag()}")
        return True
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫—ç—à–∞: {e} | {_cache_diag()}", exc_info=True)
        return False

from aiogram.filters import Command
from aiogram import types

@dp.message(Command("cache_info"))
async def cache_info(m: types.Message):
    try:
        lines = [
            f"CACHE_DIR: {CACHE_DIR}",
            f"exists: {CACHE_DIR.exists()}",
            f"writable: {os.access(CACHE_DIR, os.W_OK)}",
            f"CACHE_CSV exists: {CACHE_CSV.exists()}",
            f"CACHE_META exists: {CACHE_META.exists()}",
        ]
        await m.answer("\n".join(lines))
    except Exception as e:
        await m.answer(f"cache_info error: {e}")

def _ssl_ctx_certifi() -> ssl.SSLContext:
    """–°–æ–∑–¥–∞—ë—Ç –±–µ–∑–æ–ø–∞—Å–Ω—ã–π SSL-–∫–æ–Ω—Ç–µ–∫—Å—Ç —Å CA –∏–∑ certifi, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω."""
    if certifi is not None:
        ctx = ssl.create_default_context(cafile=certifi.where())
    else:
        ctx = ssl.create_default_context()
    ctx.check_hostname = True
    ctx.verify_mode = ssl.CERT_REQUIRED
    return ctx

def _make_ssl_param_for_aiohttp():
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è aiohttp ssl=...
    –ï—Å–ª–∏ –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è OBDSP_SSL_NO_VERIFY=1, –æ—Ç–∫–ª—é—á–∞–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫—É —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞.
    """
    no_verify = os.getenv("OBDSP_SSL_NO_VERIFY", "0").strip().lower() in {"1", "true", "yes", "on"}
    if no_verify:
        return False
    return _ssl_ctx_certifi()

# ==== ENV CONFIG (—á–∏—Ç–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ) ====
OBDSP_BASE = os.getenv("OBDSP_BASE", "https://obdsp.projects.eraga.net").strip()
OBDSP_TOKEN = os.getenv("OBDSP_TOKEN", "").strip()
OBDSP_AUTH_SCHEME = os.getenv("OBDSP_AUTH_SCHEME", "Bearer").strip()  # "Bearer" | "Token" | "ApiKey" | "Basic"
OBDSP_CLIENT_ID = os.getenv("OBDSP_CLIENT_ID", "").strip()

try:
    TELEGRAM_OWNER_ID = int(os.getenv("TELEGRAM_OWNER_ID", "0"))
except:
    TELEGRAM_OWNER_ID = 0

OBDSP_CA_BUNDLE = os.getenv("OBDSP_CA_BUNDLE", "").strip()
OBDSP_SSL_VERIFY = (os.getenv("OBDSP_SSL_VERIFY", "1") or "1").strip().lower()  # "1"/"0"/"true"/"false"
OBDSP_SSL_NO_VERIFY = os.getenv("OBDSP_SSL_NO_VERIFY", "0").strip().lower() in {"1", "true", "yes", "on"}

# ==== GLOBAL PATHS & STATE ====
DATA_DIR = os.path.join(os.path.dirname(__file__), "data")
os.makedirs(DATA_DIR, exist_ok=True)

SCREENS_PATH = os.path.join(DATA_DIR, "screens.csv")
SCREENS = None
LAST_RESULT = None
LAST_SELECTION_NAME = "last"      # –∏–º—è –¥–ª—è —Ñ–∞–π–ª–æ–≤ (–º–æ–∂–µ—à—å –º–µ–Ω—è—Ç—å)
MAX_PLAYS_PER_HOUR = 6            # —Å–∫–æ–ª—å–∫–æ –≤—ã—Ö–æ–¥–æ–≤/—á–∞—Å –Ω–∞ —ç–∫—Ä–∞–Ω (—Ç–≤–æ—è –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∞)

# ‚Äî –Ω–æ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –∫—ç—à–∞ ‚Äî
from pathlib import Path
import time, json

LAST_SYNC_TS: float | None = None
CACHE_PARQUET = Path(DATA_DIR) / "screens_cache.parquet"
CACHE_CSV     = Path(DATA_DIR) / "screens_cache.csv"
CACHE_META    = Path(DATA_DIR) / "screens_cache.meta.json"


# ====== –ù–ê–°–¢–†–û–ô–ö–ê –ë–û–¢–ê ======
BOT_TOKEN = os.getenv("BOT_TOKEN")
if not BOT_TOKEN:
    raise SystemExit("Set BOT_TOKEN env var first")
bot = Bot(BOT_TOKEN)
dp = Dispatcher()
dp.message.register(cache_info_handler, Command("cache_info"))

from aiogram.filters import Command
from aiogram import types

from aiogram import types
from aiogram.filters import Command

async def cache_info_handler(m: types.Message):
    try:
        lines = [
            f"CACHE_DIR: {CACHE_DIR}",
            f"exists: {CACHE_DIR.exists()}",
            f"writable: {os.access(CACHE_DIR, os.W_OK)}",
            f"CACHE_CSV exists: {CACHE_CSV.exists()}",
            f"CACHE_META exists: {CACHE_META.exists()}",
        ]
        await m.answer("\n".join(lines))
    except Exception as e:
        await m.answer(f"cache_info error: {e}")

HELP = (
    "üëã –ü—Ä–∏–≤–µ—Ç. –Ø –ø–æ–¥–±–∏—Ä–∞—é —Ä–µ–∫–ª–∞–º–Ω—ã–µ —ç–∫—Ä–∞–Ω—ã.\n\n"
    "üìÑ –°–Ω–∞—á–∞–ª–∞ –ø—Ä–∏—à–ª–∏—Ç–µ —Ñ–∞–π–ª CSV/XLSX —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ –º–∏–Ω–∏–º—É–º: lat, lon.\n"
    "   –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è: screen_id, name, city, format, owner.\n\n"
    "üîé –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:\n"
    "‚Ä¢ /status ‚Äî —á—Ç–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏ —Å–∫–æ–ª—å–∫–æ —ç–∫—Ä–∞–Ω–æ–≤\n"
    "‚Ä¢ /radius 2 ‚Äî –∑–∞–¥–∞—Ç—å —Ä–∞–¥–∏—É—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–∫–º)\n"
    "‚Ä¢ /near <lat> <lon> [R] [filters] [fields=...] ‚Äî —ç–∫—Ä–∞–Ω—ã –≤ —Ä–∞–¥–∏—É—Å–µ\n"
    "   –ü—Ä–∏–º–µ—Ä—ã:\n"
    "   /near 55.714349 37.553834 2\n"
    "   /near 55.714349 37.553834 2 fields=screen_id\n"
    "   /near 55.714349 37.553834 2 format=city\n"
    "   /near 55.714349 37.553834 2 format=billboard,supersite\n\n"
    "‚Ä¢ /pick_city <–ì–æ—Ä–æ–¥> <N> [filters] [mix=...] [fields=...] ‚Äî —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –ø–æ –≥–æ—Ä–æ–¥—É\n"
    "   –ü—Ä–∏–º–µ—Ä—ã:\n"
    "   /pick_city –ú–æ—Å–∫–≤–∞ 20\n"
    "   /pick_city –ú–æ—Å–∫–≤–∞ 20 fields=screen_id\n"
    "   /pick_city –ú–æ—Å–∫–≤–∞ 20 format=city fields=screen_id\n"
    "   /pick_city –ú–æ—Å–∫–≤–∞ 20 format=billboard,supersite mix=billboard:70%,supersite:30% fields=screen_id\n\n"
    "‚Ä¢ /shots campaign=<ID> [per=0] [limit=100] [zip=1] [fields=...] ‚Äî —Ñ–æ—Ç–æ–æ—Ç—á—ë—Ç—ã –ø–æ –∫–∞–º–ø–∞–Ω–∏–∏.\n"
    "   per ‚Äî –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤ –Ω–∞ (—ç–∫—Ä–∞–Ω√ó–∫—Ä–µ–∞—Ç–∏–≤); zip=1 ‚Äî –ø—Ä–∏–ª–æ–∂–∏—Ç—å ZIP —Å —Ñ–æ—Ç–æ.\n\n"
    "   –û–ø—Ü–∏–∏ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏: shuffle=1 | fixed=1 | seed=42\n\n"
    "‚Ä¢ /pick_at <lat> <lon> <N> [R] ‚Äî —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –≤ –∫—Ä—É–≥–µ\n"
    "   –ü—Ä–∏–º–µ—Ä: /pick_at 55.75 37.62 25 15\n\n"
    "‚Ä¢ /export_last ‚Äî –≤—ã–≥—Ä—É–∑–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω—é—é –≤—ã–±–æ—Ä–∫—É (CSV)\n"
    "‚Ä¢ –û—Ç–ø—Ä–∞–≤—å—Ç–µ –≥–µ–æ–ª–æ–∫–∞—Ü–∏—é üìç ‚Äî –Ω–∞–π–¥—É —ç–∫—Ä–∞–Ω—ã –≤–æ–∫—Ä—É–≥ —Ç–æ—á–∫–∏ —Å —Ä–∞–¥–∏—É—Å–æ–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n\n"
    "üî§ –§–∏–ª—å—Ç—Ä—ã:\n"
    "   format=city ‚Äî –≤—Å–µ CITY_FORMAT_* (–∞–ª–∏–∞—Å ¬´–≥–∏–¥—ã¬ª)\n"
    "   format=A,B | A;B | A|B ‚Äî –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–æ—Ä–º–∞—Ç–æ–≤\n"
    "   owner=russ | owner=russ,gallery ‚Äî –ø–æ –≤–ª–∞–¥–µ–ª—å—Ü—É (–ø–æ–¥—Å—Ç—Ä–æ–∫–∞, –Ω–µ—á—É–≤—Å—Ç–≤. –∫ —Ä–µ–≥–∏—Å—Ç—Ä—É)\n"
    "   fields=screen_id | screen_id,format ‚Äî –∫–∞–∫–∏–µ –ø–æ–ª—è –≤—ã–≤–æ–¥–∏—Ç—å\n\n"
    "üß© –ü—Ä–æ–ø–æ—Ä—Ü–∏–∏ (–∫–≤–æ—Ç—ã) —Ñ–æ—Ä–º–∞—Ç–æ–≤ –≤ /pick_city:\n"
    "   mix=BILLBOARD:60%,CITY:40%  –∏–ª–∏  mix=CITY_FORMAT_RC:5,CITY_FORMAT_WD:15\n"
)

from aiogram.types import ReplyKeyboardMarkup, KeyboardButton

def make_main_menu() -> ReplyKeyboardMarkup:
    return ReplyKeyboardMarkup(
        keyboard=[
            [KeyboardButton(text="/help"), KeyboardButton(text="/status")],
            [KeyboardButton(text="/export_last"), KeyboardButton(text="/radius 2")],
            [KeyboardButton(text="üìç –û—Ç–ø—Ä–∞–≤–∏—Ç—å –≥–µ–æ–ª–æ–∫–∞—Ü–∏—é")],
        ],
        resize_keyboard=True,
        input_field_placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: /near <lat> <lon> 2  –∏–ª–∏  –ø—Ä–∏—à–ª–∏—Ç–µ —Ñ–∞–π–ª CSV/XLSX"
    )

# ====== –£–¢–ò–õ–ò–¢–´ ======


def _parse_hours_windows(s: str | None) -> int | None:
    """
    –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä –æ–∫–æ–Ω —á–∞—Å–æ–≤ –≤–∏–¥–∞ '07-10,17-21' -> —Å—É–º–º–∞—Ä–Ω–æ —á–∞—Å–æ–≤ –≤ –¥–µ–Ω—å.
    –ï—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω ‚Äî –≤–µ—Ä–Ω—ë—Ç None.
    """
    if not s:
        return None
    total = 0
    parts = [p.strip() for p in s.split(",") if p.strip()]
    for p in parts:
        if "-" in p:
            a, b = p.split("-", 1)
            try:
                a = int(a); b = int(b)
                if 0 <= a <= 23 and 0 <= b <= 23:
                    if b > a:
                        total += (b - a)
                    else:
                        # –ø–æ–¥–¥–µ—Ä–∂–∏–º ¬´—á–µ—Ä–µ–∑ –ø–æ–ª–Ω–æ—á—å¬ª, –Ω–∞–ø—Ä–∏–º–µ—Ä 22-02
                        total += (24 - a + b)
            except Exception:
                pass
    return total or None

def _fill_min_bid(df: pd.DataFrame) -> pd.DataFrame:
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç –∫–æ–ª–æ–Ω–∫—É min_bid_used –∏ –µ—ë –∏—Å—Ç–æ—á–Ω–∏–∫.
    –õ–æ–≥–∏–∫–∞:
      1) –±–µ—Ä—ë–º df['minBid'] –µ—Å–ª–∏ –µ—Å—Ç—å;
      2) –∏–Ω–∞—á–µ –ø—Ä–æ–±—É–µ–º df['min_bid'] / df['min_bid_rub'] / df['min_bid_rur'];
      3) –∏–Ω–∞—á–µ –ø–æ–¥—Å—Ç–∞–≤–ª—è–µ–º –º–µ–¥–∏–∞–Ω—É –ø–æ –∏–º–µ—é—â–∏–º—Å—è –∑–Ω–∞—á–µ–Ω–∏—è–º, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å.
    """
    out = df.copy()
    src_col = None
    for cand in ("minBid", "min_bid", "min_bid_rub", "min_bid_rur"):
        if cand in out.columns:
            src_col = cand
            break

    if src_col:
        vals = pd.to_numeric(out[src_col], errors="coerce")
        median = float(vals.median()) if not vals.dropna().empty else None
        out["min_bid_used"] = vals.fillna(median if median else 0)
        out["min_bid_source"] = src_col
    else:
        # –Ω–µ—Ç –Ω–∏ –æ–¥–Ω–æ–π –ø–æ–¥—Ö–æ–¥—è—â–µ–π –∫–æ–ª–æ–Ω–∫–∏ ‚Äî –≤–µ—Ä–Ω—ë–º –∫–∞–∫ –µ—Å—Ç—å, –¥–∞–ª—å—à–µ –∫–æ–¥ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ —É–ø–∞–¥—ë—Ç —Å —Å–æ–æ–±—â–µ–Ω–∏–µ–º
        out["min_bid_used"] = None
        out["min_bid_source"] = None
    return out

def _distribute_slots_evenly(n_items: int, total_slots: int) -> list[int]:
    """
    –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç total_slots –ø–æ n_items, —Ä–∞–∑–Ω–∏—Ü—É +/-1 —Ä–∞–∑–¥–∞—ë—Ç –ø–µ—Ä–≤—ã–º.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–ª–∏–Ω–æ–π n_items.
    """
    if n_items <= 0 or total_slots <= 0:
        return [0] * max(0, n_items)
    base = total_slots // n_items
    extra = total_slots % n_items
    res = [base] * n_items
    for i in range(extra):
        res[i] += 1
    return res

def haversine_km(a: tuple[float, float], b: tuple[float, float]) -> float:
    lat1, lon1 = map(math.radians, a)
    lat2, lon2 = map(math.radians, b)
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    r = 6371.0088
    h = math.sin(dlat/2)**2 + math.cos(lat1)*math.cos(lat2)*math.sin(dlon/2)**2
    return 2 * r * math.asin(math.sqrt(h))

def find_within_radius(df: pd.DataFrame, center: tuple[float,float], radius_km: float) -> pd.DataFrame:
    rows = []
    for _, row in df.iterrows():
        d = haversine_km(center, (row["lat"], row["lon"]))
        if d <= radius_km:
            rows.append({
                "screen_id": row.get("screen_id", ""),
                "name": row.get("name", ""),
                "city": row.get("city", ""),
                "format": row.get("format", ""),
                "owner": row.get("owner", ""),
                "lat": row["lat"],
                "lon": row["lon"],
                "distance_km": round(d, 3),
            })
    out = pd.DataFrame(rows)
    return out.sort_values("distance_km") if not out.empty else out

def spread_select(df: pd.DataFrame, n: int, *, random_start: bool = True, seed: int | None = None) -> pd.DataFrame:
    """–ñ–∞–¥–Ω—ã–π k-center (Gonzalez) c —Ä–∞–Ω–¥–æ–º–Ω—ã–º —Å—Ç–∞—Ä—Ç–æ–º –∏ —Å–ª—É—á–∞–π–Ω—ã–º–∏ —Ç–∞–π-–±—Ä–µ–π–∫–∞–º–∏."""
    import random as _random
    if df.empty or n <= 0:
        return df.iloc[0:0]
    n = min(n, len(df))

    if seed is not None:
        _random.seed(seed)

    coords = df[["lat", "lon"]].to_numpy()

    # —Å—Ç–∞—Ä—Ç: —Å–ª—É—á–∞–π–Ω—ã–π (–∏–ª–∏ –æ—Ç –º–µ–¥–∏–∞–Ω—ã, –µ—Å–ª–∏ random_start=False)
    if random_start:
        start_idx = _random.randrange(len(df))
    else:
        lat_med = float(df["lat"].median())
        lon_med = float(df["lon"].median())
        start_idx = min(
            range(len(df)),
            key=lambda i: haversine_km((lat_med, lon_med), (coords[i][0], coords[i][1]))
        )

    chosen = [start_idx]
    dists = [
        haversine_km((coords[start_idx][0], coords[start_idx][1]), (coords[i][0], coords[i][1]))
        for i in range(len(df))
    ]

    while len(chosen) < n:
        maxd = max(dists)
        candidates = [i for i, d in enumerate(dists) if d == maxd]
        next_idx = _random.choice(candidates)  # —Å–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä —Å—Ä–µ–¥–∏ —Å–∞–º—ã—Ö ¬´–¥–∞–ª—å–Ω–∏—Ö¬ª
        chosen.append(next_idx)
        cx, cy = coords[next_idx]
        for i in range(len(df)):
            d = haversine_km((cx, cy), (coords[i][0], coords[i][1]))
            if d < dists[i]:
                dists[i] = d

    res = df.iloc[chosen].copy()
    # –∏–Ω—Ñ–æ-–ø–æ–ª–µ: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–∏—Å—Ç–∞–Ω—Ü–∏—è –¥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ
    res["min_dist_to_others_km"] = 0.0
    cc = res[["lat","lon"]].to_numpy()
    for i in range(len(res)):
        mind = min(
            haversine_km((cc[i][0], cc[i][1]), (cc[j][0], cc[j][1]))
            for j in range(len(res)) if j != i
        ) if len(res) > 1 else 0.0
        res.iat[i, res.columns.get_loc("min_dist_to_others_km")] = round(mind, 3)
    return res

def parse_kwargs(parts: list[str]) -> dict[str, str]:
    """–ü–∞—Ä—Å–∏–º —Ö–≤–æ—Å—Ç –∫–æ–º–∞–Ω–¥—ã –≤–∏–¥–∞ key=value (–∑–Ω–∞—á–µ–Ω–∏—è –º–æ–∂–Ω–æ –±—Ä–∞—Ç—å –≤ –∫–∞–≤—ã—á–∫–∏)."""
    out: dict[str,str] = {}
    for p in parts:
        if "=" in p:
            k, v = p.split("=", 1)
            out[k.strip()] = v.strip().strip('"').strip("'")
    return out

def _split_list(val: str) -> list[str]:
    if not val:
        return []
    return [x.strip() for x in val.replace(";", ",").split(",") if x.strip()]

def parse_fields(arg: str) -> list[str]:
    allowed = {
        "screen_id","name","city","format","owner","lat","lon",
        "distance_km","min_dist_to_others_km"
    }
    cols = [c.strip() for c in arg.split(",")]
    return [c for c in cols if c in allowed]

def parse_list(val: str) -> list[str]:
    if not isinstance(val, str):
        return []
    # –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏: –∑–∞–ø—è—Ç–∞—è, —Ç–æ—á–∫–∞ —Å –∑–∞–ø—è—Ç–æ–π, –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–∞—è —á–µ—Ä—Ç–∞
    for sep in ("|", ";"):
        val = val.replace(sep, ",")
    return [x.strip() for x in val.split(",") if x.strip()]

def apply_filters(df: pd.DataFrame, kwargs: dict[str,str]) -> pd.DataFrame:
    """
    –ü–æ–¥–¥–µ—Ä–∂–∫–∞:
      - format=...  (–æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ: comma/; / |)
        —Å–ø–µ—Ü-–∞–ª–∏–∞—Å: city / –≥–∏–¥—ã ‚Üí –≤—Å–µ, —á—Ç–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å CITY_FORMAT
      - owner=...   (–æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ: comma/; / |), –ø–æ–¥—Å—Ç—Ä–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ (case-insensitive)
    """
    out = df



    # -------- FORMAT --------
    fmt_val = kwargs.get("format") or kwargs.get("formats") or kwargs.get("format_in")
    if fmt_val:
        fmt_list_raw = parse_list(fmt_val)
        fmt_list = [s.upper() for s in fmt_list_raw]
        mask = None
        col = out["format"].astype(str).str.upper()

        for f in fmt_list:
            if f.lower() in {"city", "city_format", "cityformat", "citylight", "–≥–∏–¥", "–≥–∏–¥—ã"}:
                m = col.str.startswith("CITY_FORMAT")
            else:
                m = (col == f)
            mask = m if mask is None else (mask | m)

        if mask is not None:
            out = out[mask]

    # -------- OWNER --------
    own_val = kwargs.get("owner") or kwargs.get("owners") or kwargs.get("owner_in")
    if own_val:
        owners = parse_list(own_val)
        mask = None
        col = out["owner"].astype(str).str.lower()
        for o in owners:
            m = col.str.contains(o.strip().lower())
            mask = m if mask is None else (mask | m)
        if mask is not None:
            out = out[mask]

    return out

async def send_lines(message: types.Message, lines: list[str], header: str | None = None, chunk: int = 60):
    if header:
        await message.answer(header)
    for i in range(0, len(lines), chunk):
        await message.answer("\n".join(lines[i:i+chunk]))

def _format_mask(series: pd.Series, token: str) -> pd.Series:
    """
    –ë—É–ª–µ–≤–∞ –º–∞—Å–∫–∞ –ø–æ —Ñ–æ—Ä–º–∞—Ç—É:
      - 'city', '–≥–∏–¥', ... ‚Üí –≤—Å—ë, —á—Ç–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å CITY_FORMAT
      - 'billboard', 'bb'  ‚Üí BILLBOARD
      - –∏–Ω–∞—á–µ ‚Äî —Ç–æ—á–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ –≤–µ—Ä—Ö–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    –ü—Ä–æ–±–µ–ª—ã –∏ —Ä–µ–≥–∏—Å—Ç—Ä –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è.
    """
    col = series.astype(str).str.upper().str.strip()
    t = token.strip().upper()
    if t in {"CITY", "CITY_FORMAT", "CITYFORMAT", "CITYLIGHT", "–ì–ò–î", "–ì–ò–î–´"}:
        return col.str.startswith("CITY_FORMAT")
    if t in {"BILLBOARD", "BB"}:
        return col == "BILLBOARD"
    return col == t

def save_screens_cache(df: pd.DataFrame):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫—ç—à –Ω–∞ –¥–∏—Å–∫ –≤ data/screens_cache.*"""
    global LAST_SYNC_TS

    try:
        if df is None or df.empty:
            return False

        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º parquet –∏ csv
        df.to_parquet(CACHE_PARQUET, index=False)
        df.to_csv(CACHE_CSV, index=False, encoding="utf-8-sig")

        LAST_SYNC_TS = time.time()
        meta = {"ts": LAST_SYNC_TS, "rows": len(df)}
        CACHE_META.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")

        print(f"üíæ –ö—ç—à —Å–æ—Ö—Ä–∞–Ω—ë–Ω –Ω–∞ –¥–∏—Å–∫: {len(df)} —Å—Ç—Ä–æ–∫.")
        return True
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫—ç—à–∞: {e}")
        return False


def load_screens_cache() -> bool:
    """–ü—ã—Ç–∞–µ—Ç—Å—è –ø–æ–¥–Ω—è—Ç—å –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å –∏–∑ –∫—ç—à–∞. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True/False ‚Äî —É–¥–∞–ª–æ—Å—å –ª–∏."""
    global SCREENS, LAST_SYNC_TS

    df: pd.DataFrame | None = None

    # –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º parquet ‚Äî –±—ã—Å—Ç—Ä–µ–µ
    if CACHE_PARQUET.exists():
        try:
            df = pd.read_parquet(CACHE_PARQUET)
        except Exception:
            df = None

    # –µ—Å–ª–∏ parquet –Ω–µ —É–¥–∞–ª–æ—Å—å ‚Äî –ø—Ä–æ–±—É–µ–º csv
    if df is None and CACHE_CSV.exists():
        try:
            df = pd.read_csv(CACHE_CSV)
        except Exception:
            df = None

    if df is None or df.empty:
        return False

    SCREENS = df

    # —á–∏—Ç–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–≤—Ä–µ–º—è/–∫–æ–ª-–≤–æ), –µ—Å–ª–∏ –µ—Å—Ç—å
    try:
        meta = json.loads(CACHE_META.read_text(encoding="utf-8"))
        LAST_SYNC_TS = float(meta.get("ts")) if "ts" in meta else None
    except Exception:
        LAST_SYNC_TS = None

    return True

if load_screens_cache():
    logging.info(f"Loaded screens cache: {len(SCREENS)} rows, ts={LAST_SYNC_TS}")
else:
    logging.info("No local screens cache found.")

def parse_mix(val: str) -> list[tuple[str, str]]:
    """
    –†–∞–∑–±–æ—Ä —Å—Ç—Ä–æ–∫–∏ mix=... –Ω–∞ –ø–∞—Ä—ã (token, value_str).
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏: ',', ';', '|'
    –ü—Ä–∏–º–µ—Ä—ã:
      "BILLBOARD:90%,CITY:10%"
      "CITY_FORMAT_RC:5,CITY_FORMAT_WD:15"
    """
    if not isinstance(val, str) or not val.strip():
        return []
    s = val.replace("|", ",").replace(";", ",")
    items = []
    for part in s.split(","):
        part = part.strip()
        if not part or ":" not in part:
            continue
        token, v = part.split(":", 1)
        items.append((token.strip(), v.strip()))
    return items

def _allocate_counts(total_n: int, mix_items: list[tuple[str, str]]) -> list[tuple[str, int]]:
    """
    –í—Ö–æ–¥: [('BILLBOARD','90%'), ('CITY','10%')] –∏–ª–∏ [('BILLBOARD','18'), ('CITY','2')]
    –í—ã—Ö–æ–¥: [('BILLBOARD', 18), ('CITY', 2)]
    –ü—Ä–∞–≤–∏–ª–∞:
      - –µ—Å–ª–∏ –µ—Å—Ç—å —Å—É—Ñ—Ñ–∏–∫—Å '%', —Å—á–∏—Ç–∞–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã (—Å –æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ–º –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –æ—Å—Ç–∞—Ç–∫–∞)
      - —á–∏—Å–ª–∞ –±–µ–∑ % —Ç—Ä–∞–∫—Ç—É—é—Ç—Å—è –∫–∞–∫ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —à—Ç—É–∫–∏
      - –¥–æ–ø—É—Å–∫–∞–µ—Ç—Å—è —Å–º–µ—à–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º: —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ + –ø—Ä–æ—Ü–µ–Ω—Ç—ã –Ω–∞ –æ—Å—Ç–∞—Ç–æ–∫
    """
    fixed: list[tuple[str, int]] = []
    perc:  list[tuple[str, float]] = []

    for token, v in mix_items:
        if v.endswith("%"):
            try:
                perc.append((token, float(v[:-1])))
            except:
                pass
        else:
            try:
                fixed.append((token, int(v)))
            except:
                pass

    # —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —á–∞—Å—Ç—å
    fixed_sum = sum(cnt for _, cnt in fixed)
    remaining = max(0, total_n - fixed_sum)

    # –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è —á–∞—Å—Ç—å
    out: list[tuple[str, int]] = fixed[:]
    if remaining > 0 and perc:
        p_total = sum(p for _, p in perc)
        if p_total <= 0:
            # –µ—Å–ª–∏ –ø—Ä–æ—Ü–µ–Ω—Ç—ã –∑–∞–¥–∞–Ω—ã, –Ω–æ —Å—É–º–º–∞ –Ω—É–ª–µ–≤–∞—è/–Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è ‚Äî –ø—Ä–æ—Å—Ç–æ –æ—Ç–¥–∞—ë–º –≤—Å—ë –ø–µ—Ä–≤–æ–º—É
            out.append((perc[0][0], remaining))
        else:
            # –±–∞–∑–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ + —Ä–∞–∑–¥–∞—á–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤ –ø–æ —É–±—ã–≤–∞–Ω–∏—é –¥—Ä–æ–±–Ω–æ–π —á–∞—Å—Ç–∏
            raw = [(tok, remaining * p / p_total) for tok, p in perc]
            base = [(tok, int(x)) for tok, x in raw]
            used = sum(cnt for _, cnt in base)
            rem  = remaining - used
            fracs = sorted(((x - int(x), tok) for tok, x in raw), reverse=True)
            extra = {}
            for i in range(rem):
                _, tok = fracs[i % len(fracs)]
                extra[tok] = extra.get(tok, 0) + 1
            # —Å–æ–±—Ä–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            for tok, cnt in base:
                out.append((tok, cnt + extra.get(tok, 0)))

    # –µ—Å–ª–∏ –ø—Ä–æ—Ü–µ–Ω—Ç—ã –µ—Å—Ç—å, –∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–µ—Ç –∏ remaining==0 (n –ø–µ—Ä–µ–∫—Ä—ã—Ç–æ —Ñ–∏–∫—Å–∞–º–∏) ‚Äî –ø—Ä–æ—Å—Ç–æ out —É–∂–µ –≥–æ—Ç–æ–≤
    # —É–±–µ–¥–∏–º—Å—è, —á—Ç–æ —Å—É–º–º–∞—Ä–Ω–æ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ–º total_n (–Ω–∞ –≤—Å—è–∫–∏–π)
    total = sum(cnt for _, cnt in out)
    if total > total_n:
        # –æ—Ç—Ä–µ–∂–µ–º –ª–∏—à–Ω–µ–µ —Å –∫–æ–Ω—Ü–∞
        delta = total - total_n
        trimmed = []
        for tok, cnt in out:
            take = max(0, cnt - delta)
            delta -= (cnt - take)
            trimmed.append((tok, take))
            if delta <= 0:
                trimmed.extend(out[len(trimmed):])
                break
        out = trimmed
    return out


def _select_with_mix(df_city: pd.DataFrame, n: int, mix_arg: str | None,
                     *, random_start: bool = True, seed: int | None = None) -> pd.DataFrame:
    """
    –î–µ–ª–∏—Ç –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –Ω–∞ –ø–æ–¥–Ω–∞–±–æ—Ä—ã –ø–æ —Ñ–æ—Ä–º–∞—Ç–∞–º —Å–æ–≥–ª–∞—Å–Ω–æ mix, —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –≤—ã–±–∏—Ä–∞–µ—Ç –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ,
    –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏ –¥–æ–±–∏—Ä–∞–µ—Ç –æ—Å—Ç–∞—Ç–æ–∫ –¢–û–õ–¨–ö–û –∏–∑ —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ mix.
    """
    # –±–µ–∑ mix ‚Üí –æ–±—ã—á–Ω—ã–π —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä
    if not mix_arg:
        return spread_select(df_city.reset_index(drop=True), n, random_start=random_start, seed=seed)

    items = parse_mix(mix_arg)
    if not items:
        return spread_select(df_city.reset_index(drop=True), n, random_start=random_start, seed=seed)

    # —Å–ø–∏—Å–æ–∫ —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –∏–∑ mix (–∫–∞–∫ —Ç–æ–∫–µ–Ω—ã)
    allowed_tokens = [tok for tok, _ in items]

    # —Å—É–∑–∏–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø—É–ª —Å—Ä–∞–∑—É —Ç–æ–ª—å–∫–æ –∫ —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∞–º
    if "format" not in df_city.columns:
        # –Ω–∞ –≤—Å—è–∫–∏–π ‚Äî –µ—Å–ª–∏ –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ format, –ø–∞–¥–∞–µ–º –≤ –æ–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
        base_pool = df_city.copy()
    else:
        mask_allowed = None
        col = df_city["format"]
        for tok in allowed_tokens:
            m = _format_mask(col, tok)
            mask_allowed = m if mask_allowed is None else (mask_allowed | m)
        base_pool = df_city[mask_allowed] if mask_allowed is not None else df_city.copy()

    if base_pool.empty:
        # –Ω–∏—á–µ–≥–æ –∏–∑ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ ‚Äî –≤–µ—Ä–Ω—ë–º –æ–±—ã—á–Ω—ã–π —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–π –∏–∑ –≤—Å–µ–≥–æ (—á—Ç–æ–±—ã –Ω–µ –ø—É—Å—Ç–æ)
        return spread_select(df_city.reset_index(drop=True), n, random_start=random_start, seed=seed)

    targets = _allocate_counts(n, items)  # [('BILLBOARD', 18), ('CITY', 2)]
    selected_parts: list[pd.DataFrame] = []
    used_ids: set[str] = set()

    pool = base_pool.copy()

    # –≤—ã–±–∏—Ä–∞–µ–º –ø–æ –∫–≤–æ—Ç–∞–º
    for token, need in targets:
        if need <= 0 or pool.empty:
            continue
        mask = _format_mask(pool["format"], token) if "format" in pool.columns else pd.Series([True]*len(pool), index=pool.index)
        subset = pool[mask]
        if subset.empty:
            continue
        pick_n = min(need, len(subset))
        picked = spread_select(subset.reset_index(drop=True), pick_n, random_start=random_start, seed=seed)
        selected_parts.append(picked)

        # –∏—Å–∫–ª—é—á–∏–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –∏–∑ –ø—É–ª–∞
        if "screen_id" in pool.columns and "screen_id" in picked.columns:
            chosen_ids = picked["screen_id"].astype(str).tolist()
            used_ids.update(chosen_ids)
            pool = pool[~pool["screen_id"].astype(str).isin(used_ids)]
        else:
            # fallback –ø–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º
            coords = set((float(a), float(b)) for a, b in picked[["lat","lon"]].itertuples(index=False, name=None))
            pool = pool[~((pool["lat"].astype(float).round(7).isin([x for x, _ in coords])) &
                          (pool["lon"].astype(float).round(7).isin([y for _, y in coords])))]
        if pool.empty:
            break

    combined = pd.concat(selected_parts, ignore_index=True) if selected_parts else base_pool.iloc[0:0]

    # –¥–æ–±–∏—Ä–∞–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–µ–µ –¢–û–õ–¨–ö–û –∏–∑ base_pool (—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã)
    remain = n - len(combined)
    if remain > 0 and not pool.empty:
        extra = spread_select(pool.reset_index(drop=True), min(remain, len(pool)), random_start=random_start, seed=seed)
        combined = pd.concat([combined, extra], ignore_index=True)

    return combined.head(n)
async def send_gid_xlsx(chat_id: int, ids: list[str], *, filename: str = "screen_ids.xlsx", caption: str = "GID —Å–ø–∏—Å–æ–∫ (XLSX)"):
    """–û—Ç–ø—Ä–∞–≤–∫–∞ XLSX —Å –æ–¥–Ω–∏–º —Å—Ç–æ–ª–±—Ü–æ–º GID –∏–∑ —Å–ø–∏—Å–∫–∞ screen_id."""
    df = pd.DataFrame({"GID": [str(x) for x in ids]})
    bio = io.BytesIO()
    with pd.ExcelWriter(bio, engine="openpyxl") as writer:
        df.to_excel(writer, index=False)
    bio.seek(0)
    await bot.send_document(
        chat_id,
        BufferedInputFile(bio.read(), filename=filename),
        caption=caption,
    )

async def send_gid_if_any(message: types.Message, df: pd.DataFrame, *, filename: str, caption: str):
    """–ï—Å–ª–∏ –≤ df –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∞ screen_id –∏ —Ç–∞–º –Ω–µ –ø—É—Å—Ç–æ ‚Äî –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º XLSX —Å –∫–æ–ª–æ–Ω–∫–æ–π GID."""
    if df is None or df.empty or "screen_id" not in df.columns:
        return
    ids = [s for s in (df["screen_id"].astype(str).tolist()) if str(s).strip() and str(s).lower() != "nan"]
    if ids:
        await send_gid_xlsx(message.chat.id, ids, filename=filename, caption=caption)

# --- ACCESS CONTROL HELPERS ---

def _owner_only(user_id: int) -> bool:
    return TELEGRAM_OWNER_ID == 0 or user_id == TELEGRAM_OWNER_ID

def _auth_headers_all_variants(token: str) -> list[tuple[str, dict]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ (label, headers). label ‚Äî —á—Ç–æ–±—ã –∫—Ä–∞—Å–∏–≤–æ –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å, —á—Ç–æ –º—ã –ø—Ä–æ–±–æ–≤–∞–ª–∏.
    """
    t = (token or "").strip()
    if not t:
        return [("no-auth", {})]
    return [
        ("Bearer",      {"Authorization": f"Bearer {t}"}),
        ("Token",       {"Authorization": f"Token {t}"}),
        ("X-API-Key",   {"X-API-Key": t}),
        ("x-api-key",   {"x-api-key": t}),
        ("X-Auth-Token",{"X-Auth-Token": t}),
        ("Auth-Token",  {"Auth-Token": t}),
        ("authToken",   {"authToken": t}),
        ("Cookie",      {"Cookie": f"authToken={t}"}),
    ]

from urllib.parse import urljoin
import json
import aiohttp

from urllib.parse import urljoin
import json, aiohttp, logging

# --- API FETCH (–ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–≤–µ–Ω—Ç–∞—Ä—è —Å —Å–µ—Ä–≤–µ—Ä–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π) ---
import aiohttp

def _build_server_query(filters: dict | None) -> dict:
    """
    –ì–æ—Ç–æ–≤–∏–º –Ω–∞–±–æ—Ä query-–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Å–µ—Ä–≤–µ—Ä–∞.
    –ú—ã –∑–∞–∫–ª–∞–¥—ã–≤–∞–µ–º —Å—Ä–∞–∑—É –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∏–º–µ–Ω (type/format, owner/displayOwnerName, city/cityName/search),
    –ø–æ—Ç–æ–º—É —á—Ç–æ —Å–µ—Ä–≤–µ—Ä —Å–ø–æ–∫–æ–π–Ω–æ –ø—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –Ω–µ–∑–Ω–∞–∫–æ–º—ã–µ, –∞ –∑–Ω–∞–∫–æ–º—ã–µ ‚Äî –ø—Ä–∏–º–µ–Ω–∏—Ç.
    """
    if not filters:
        return {}

    q: dict = {}

    # –≥–æ—Ä–æ–¥ (–ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é/–ø–æ–∏—Å–∫–æ–º)
    city = (filters.get("city") or "").strip()
    if city:
        q["city"] = city          # –≤–æ–∑–º–æ–∂–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
        q["cityName"] = city      # –≤–æ–∑–º–æ–∂–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
        q["search"] = city        # —á–∞—Å—Ç–æ –µ—Å—Ç—å –æ–±—â–∏–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∏–ª—å—Ç—Ä

    # —Ñ–æ—Ä–º–∞—Ç—ã (–Ω–µ—Å–∫–æ–ª—å–∫–æ —á–µ—Ä–µ–∑ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)
    fmts = filters.get("formats") or []
    if fmts:
        q["type"] = fmts          # canonical (—É Omniboard field 'type')
        q["format"] = fmts        # –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–µ –∏–º—è
        q["types"] = fmts
        q["formats"] = fmts

    # –ø–æ–¥—Ä—è–¥—á–∏–∫–∏ (–ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é)
    owners = filters.get("owners") or []
    if owners:
        q["owner"] = owners                 # –≤–æ–∑–º–æ–∂–Ω–æ–µ –∏–º—è
        q["displayOwnerName"] = owners      # –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–µ –∏–º—è
        # –¥–æ–±–∞–≤–∏–º –≤ –æ–±—â–∏–π search, —á—Ç–æ–±—ã —É–≤–µ–ª–∏—á–∏—Ç—å —à–∞–Ω—Å —Å–µ—Ä–≤–µ—Ä–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        q["search"] = (" ".join([q.get("search",""), *owners])).strip()

    # –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ ¬´—Å—ã—Ä–æ–≤—ã–µ¬ª –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ /sync_api –≤–∏–¥–∞ api.cityId=7
    for k, v in (filters.get("api_params") or {}).items():
        q[k] = v

    # –≤—ã–∫–∏–Ω–µ–º –ø—É—Å—Ç—ã–µ
    return {k: v for k, v in q.items() if v not in ("", None, [], {})}

async def _fetch_inventories(
    pages_limit: int | None = None,
    page_size: int = 500,
    total_limit: int | None = None,
    m: types.Message | None = None,
    filters: dict | None = None,          # <--- –ù–û–í–û–ï
) -> list[dict]:
    """
    /api/v1.0/clients/inventories ‚Äî —Ç—è–Ω–µ–º –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ.
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
      - pages_limit: –º–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü (None = –≤—Å–µ)
      - page_size:   —Ä–∞–∑–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã
      - total_limit: –æ–±—â–∏–π –ª–∏–º–∏—Ç —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (None = –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)
      - m:           Telegram message –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
      - filters:     { city: str, formats: [..], owners: [..], api_params: {rawKey: rawVal} }
    """
    base = (OBDSP_BASE or "https://proddsp.omniboard360.io").rstrip("/")
    root = f"{base}/api/v1.0/clients/inventories"

    headers = {
        "Authorization": f"Bearer {OBDSP_TOKEN}",
        "Accept": "application/json",
    }

    timeout = aiohttp.ClientTimeout(total=180)
    ssl_param = _make_ssl_param_for_aiohttp()

    # –ø–æ–¥–≥–æ—Ç–æ–≤–∏–º —Å–µ—Ä–≤–µ—Ä–Ω—ã–µ query-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–æ–Ω–∏ –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª—è—Ç—å—Å—è –∫ page/size)
    server_q = _build_server_query(filters)

    items: list[dict] = []
    page = 0
    pages_fetched = 0

    async with aiohttp.ClientSession(timeout=timeout) as session:
        while True:
            params = {"page": page, "size": page_size}
            # –¥–æ–±–∞–≤–ª—è–µ–º —Å–µ—Ä–≤–µ—Ä–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã
            for k, v in server_q.items():
                params[k] = v

            async with session.get(root, headers=headers, params=params, ssl=ssl_param) as resp:
                text = await resp.text()
                if resp.status != 200:
                    raise RuntimeError(f"API {resp.status}: {text[:300]}")

                try:
                    data = await resp.json()
                except Exception:
                    raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON: {text[:500]}")

                page_items = data.get("content") or []
                items.extend(page_items)

                pages_fetched += 1
                page += 1

                # –ª–∏–º–∏—Ç –ø–æ –æ–±—â–µ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É
                if total_limit is not None and len(items) >= total_limit:
                    items = items[:total_limit]
                    break

                # –ª–∏–º–∏—Ç –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
                if pages_limit is not None and pages_fetched >= pages_limit:
                    break

                # –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ–∫–æ–Ω—á–∞–Ω–∏—è
                if data.get("last") is True:
                    break
                if data.get("totalPages") is not None and page >= int(data["totalPages"]):
                    break
                if data.get("numberOfElements") == 0:
                    break

                # –ø—Ä–æ–≥—Ä–µ—Å—Å
                if m and (pages_fetched % 5 == 0):
                    try:
                        await m.answer(f"‚Ä¶–∑–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {pages_fetched}, –≤—Å–µ–≥–æ –ø–æ–∑–∏—Ü–∏–π: {len(items)}")
                    except Exception:
                        pass

    return items


@dp.message(Command("forecast"))
async def cmd_forecast(m: types.Message):
    """
    /forecast [budget=...] [days=7] [hours_per_day=8] [hours=07-10,17-21]
    –†–∞–±–æ—Ç–∞–µ—Ç –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤—ã–±–æ—Ä–∫–µ (LAST_RESULT).
    """
    global LAST_RESULT
    if LAST_RESULT is None or LAST_RESULT.empty:
        await m.answer("–ù–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤—ã–±–æ—Ä–∫–∏. –°–Ω–∞—á–∞–ª–∞ –ø–æ–¥–±–µ—Ä–∏—Ç–µ —ç–∫—Ä–∞–Ω—ã (/pick_city, /pick_any, /pick_at, /near –∏–ª–∏ —á–µ—Ä–µ–∑ /ask).")
        return

    parts = (m.text or "").strip().split()[1:]
    kv = {}
    for p in parts:
        if "=" in p:
            k, v = p.split("=", 1)
            kv[k.strip().lower()] = v.strip()

    budget = None
    if "budget" in kv:
        try:
            # –ø–æ–¥–¥–µ—Ä–∂–∏–º —Å—É—Ñ—Ñ–∏–∫—Å—ã m/k
            v = kv["budget"].lower().replace(" ", "")
            if v.endswith("m"): budget = float(v[:-1]) * 1_000_000
            elif v.endswith("k"): budget = float(v[:-1]) * 1_000
            else: budget = float(v)
        except Exception:
            budget = None

    days = int(kv.get("days", 7)) if str(kv.get("days","")).isdigit() else 7

    hours_per_day = None
    if "hours_per_day" in kv:
        try:
            hours_per_day = int(kv["hours_per_day"])
        except Exception:
            hours_per_day = None

    hours = kv.get("hours", "")
    win_hours = _parse_hours_windows(hours) if hours else None
    if hours_per_day is None:
        hours_per_day = (win_hours if (win_hours is not None) else 8)

    # –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ minBid
    base = LAST_RESULT.copy()
    base = _fill_min_bid(base)

    # —Å—Ä–µ–¥–Ω—è—è –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—Ç–∞–≤–∫–∞
    mb_valid = pd.to_numeric(base["min_bid_used"], errors="coerce").dropna()
    if mb_valid.empty:
        await m.answer("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ü–µ–Ω–∏—Ç—å —Å—Ç–∞–≤–∫—É: –Ω–∏ —É –æ–¥–Ω–æ–≥–æ —ç–∫—Ä–∞–Ω–∞ –Ω–µ—Ç minBid (–∏ –Ω–µ—á–µ–≥–æ –ø–æ–¥—Å—Ç–∞–≤–∏—Ç—å).")
        return
    avg_min = float(mb_valid.mean())

    n_screens = len(base)
    capacity  = n_screens * days * hours_per_day * MAX_PLAYS_PER_HOUR  # –º–∞–∫—Å–∏–º—É–º –≤—ã—Ö–æ–¥–æ–≤

    if budget is not None:
        # –ø–æ –±—é–¥–∂–µ—Ç—É ‚Äî —Å—á–∏—Ç–∞–µ–º –≤—ã—Ö–æ–¥—ã –æ—Ç —Å—Ä–µ–¥–Ω–µ–π —Å—Ç–∞–≤–∫–∏
        total_slots = int(budget // avg_min)
        total_slots = min(total_slots, capacity)
    else:
        # –±–µ–∑ –±—é–¥–∂–µ—Ç–∞ ‚Äî –º–∞–∫—Å–∏–º—É–º —á–∞—Å—Ç–æ—Ç—ã
        total_slots = capacity
        budget = total_slots * avg_min

    # —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ —ç–∫—Ä–∞–Ω–∞–º
    per_screen = _distribute_slots_evenly(n_screens, total_slots)

    # –¥–æ–±–∞–≤–∏–º –ø–ª–∞–Ω –≤ —Ç–∞–±–ª–∏—Ü—É
    base = base.reset_index(drop=True)
    base["planned_slots"] = per_screen
    # —Å—á–∏—Ç–∞–µ–º —Å—Ç–æ–∏–º–æ—Å—Ç—å —Ç–æ—á–Ω–µ–µ ‚Äî —É–º–Ω–æ–∂–∞—è –Ω–∞ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π min_bid_used
    base["planned_cost"]  = base["planned_slots"] * pd.to_numeric(base["min_bid_used"], errors="coerce").fillna(avg_min)

    # —Å–≤–æ–¥–∫–∞
    total_cost  = float(base["planned_cost"].sum())
    total_slots = int(base["planned_slots"].sum())

    # –∞–∫–∫—É—Ä–∞—Ç–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç
    export_cols = []
    for c in ("screen_id","name","city","format","owner","lat","lon","minBid","min_bid_used","min_bid_source","planned_slots","planned_cost"):
        if c in base.columns:
            export_cols.append(c)
    plan_df = base[export_cols].copy()

    # CSV + XLSX
    try:
        csv_bytes = plan_df.to_csv(index=False).encode("utf-8-sig")
        await bot.send_document(
            m.chat.id,
            BufferedInputFile(csv_bytes, filename=f"forecast_{LAST_SELECTION_NAME}.csv"),
            caption=f"–ü—Ä–æ–≥–Ω–æ–∑ (—Å—Ä–µ–¥–Ω. minBid‚âà{avg_min:,.0f}): {total_slots} –≤—ã—Ö–æ–¥–æ–≤, –±—é–¥–∂–µ—Ç‚âà{total_cost:,.0f} ‚ÇΩ"
        )
    except Exception as e:
        await m.answer(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å CSV: {e}")

    try:
        import io as _io
        xbuf = _io.BytesIO()
        with pd.ExcelWriter(xbuf, engine="openpyxl") as w:
            plan_df.to_excel(w, index=False, sheet_name="forecast")
        xbuf.seek(0)
        await bot.send_document(
            m.chat.id,
            BufferedInputFile(xbuf.getvalue(), filename=f"forecast_{LAST_SELECTION_NAME}.xlsx"),
            caption=f"–ü—Ä–æ–≥–Ω–æ–∑ (–ø–æ–¥—Ä–æ–±–Ω–æ): –¥–Ω–∏={days}, —á–∞—Å—ã/–¥–µ–Ω—å={hours_per_day}, max {MAX_PLAYS_PER_HOUR}/—á–∞—Å"
        )
    except Exception as e:
        await m.answer(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å XLSX: {e}")

# --- helpers: –∑–∞–ø—É—Å–∫ –ª–æ–≥–∏–∫–∏ pick_city –±–µ–∑ –ø–æ–¥–º–µ–Ω—ã Message ---

import re

async def _run_pick_city(
    m: types.Message,
    city: str,
    n: int,
    formats: list[str] | None = None,
    owners: list[str] | None = None,
    fields: list[str] | None = None,
    shuffle: bool = True,
    fixed: bool = False,
    seed: int | None = None,
):
    global SCREENS, LAST_RESULT

    if SCREENS is None or SCREENS.empty:
        await m.answer("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å: /sync_api –∏–ª–∏ –ø—Ä–∏—à–ª–∏—Ç–µ CSV/XLSX.")
        return

    df = SCREENS
    sub = df[df["city"].astype(str).str.strip().str.lower() == city.strip().lower()]

    if formats:
        formats_u = {f.strip().upper() for f in formats if f.strip()}
        if "format" in sub.columns:
            sub = sub[sub["format"].astype(str).str.upper().isin(formats_u)]

    if owners:
        if "owner" in sub.columns:
            pat = "|".join(re.escape(o) for o in owners if o.strip())
            sub = sub[sub["owner"].astype(str).str.contains(pat, case=False, na=False)]

    if sub.empty:
        await m.answer(f"–ù–µ –Ω–∞—à—ë–ª —ç–∫—Ä–∞–Ω–æ–≤ –≤ –≥–æ—Ä–æ–¥–µ ¬´{city}¬ª —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏.")
        return

    if shuffle:
        sub = sub.sample(frac=1, random_state=None).reset_index(drop=True)

    # —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ (—Ç–≤–æ—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è —Ñ—É–Ω–∫—Ü–∏—è)
    res = spread_select(
        sub.reset_index(drop=True),
        n,
        random_start=not fixed,
        seed=seed
    )
    LAST_RESULT = res

    # –µ—Å–ª–∏ –ø–æ–ø—Ä–æ—Å–∏–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø–æ–ª—è ‚Äî –æ—Ç–¥–∞–¥–∏–º –∫–æ–º–ø–∞–∫—Ç–Ω–æ
    if fields:
        ok_fields = [c for c in fields if c in res.columns]
        if not ok_fields:
            await m.answer("–ü–æ–ª—è –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω—ã. –î–æ—Å—Ç—É–ø–Ω—ã–µ: " + ", ".join(res.columns))
            return
        view = res[ok_fields]
        if ok_fields == ["screen_id"]:
            ids = [str(x) for x in view["screen_id"].tolist()]
            await send_lines(m, ids, header=f"–í—ã–±—Ä–∞–Ω–æ {len(ids)} screen_id –ø–æ –≥–æ—Ä–æ–¥—É ¬´{city}¬ª:")
        else:
            lines = [" | ".join(str(r[c]) for c in ok_fields) for _, r in view.iterrows()]
            await send_lines(m, lines, header=f"–í—ã–±—Ä–∞–Ω–æ {len(view)} —ç–∫—Ä–∞–Ω–æ–≤ –ø–æ –≥–æ—Ä–æ–¥—É ¬´{city}¬ª (–ø–æ–ª—è: {', '.join(ok_fields)}):")

        # –ø—Ä–∏–ª–æ–∂–∏–º XLSX —Å GID, –µ—Å–ª–∏ –µ—Å—Ç—å
        await send_gid_if_any(m, res, filename="city_screen_ids.xlsx",
                              caption=f"GID –ø–æ –≥–æ—Ä–æ–¥—É ¬´{city}¬ª (XLSX)")
        return

    # –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –≤—ã–≤–æ–¥
    lines = []
    for _, r in res.iterrows():
        nm = r.get("name","") or r.get("screen_id","")
        fmt = r.get("format",""); own = r.get("owner","")
        lat = r.get("lat"); lon = r.get("lon")
        lines.append(f"‚Ä¢ {r.get('screen_id','')} ‚Äî {nm} [{lat:.5f},{lon:.5f}] [{fmt} / {own}]")
    await send_lines(m, lines, header=f"–í—ã–±—Ä–∞–Ω–æ {len(res)} —ç–∫—Ä–∞–Ω–æ–≤ –ø–æ –≥–æ—Ä–æ–¥—É ¬´{city}¬ª (—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ):")

    await send_gid_if_any(m, res, filename="city_screen_ids.xlsx",
                          caption=f"GID –ø–æ –≥–æ—Ä–æ–¥—É ¬´{city}¬ª (XLSX)")


# ====== –£–¢–ò–õ–ò–¢–´ ======
def haversine_km(a: tuple[float, float], b: tuple[float, float]) -> float:
    lat1, lon1 = map(math.radians, a)
    lat2, lon2 = map(math.radians, b)
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    r = 6371.0088
    h = math.sin(dlat/2)**2 + math.cos(lat1)*math.cos(lat2)*math.sin(dlon/2)**2
    return 2 * r * math.asin(math.sqrt(h))

def find_within_radius(df: pd.DataFrame, center: tuple[float,float], radius_km: float) -> pd.DataFrame:
    rows = []
    for _, row in df.iterrows():
        d = haversine_km(center, (row["lat"], row["lon"]))
        if d <= radius_km:
            rows.append({
                "screen_id": row.get("screen_id", ""),
                "name": row.get("name", ""),
                "city": row.get("city", ""),
                "format": row.get("format", ""),
                "owner": row.get("owner", ""),
                "lat": row["lat"],
                "lon": row["lon"],
                "distance_km": round(d, 3),
            })
    out = pd.DataFrame(rows)
    return out.sort_values("distance_km") if not out.empty else out

def spread_select(df: pd.DataFrame, n: int, *, random_start: bool = True, seed: int | None = None) -> pd.DataFrame:
    """–ñ–∞–¥–Ω—ã–π k-center (Gonzalez) c —Ä–∞–Ω–¥–æ–º–Ω—ã–º —Å—Ç–∞—Ä—Ç–æ–º –∏ —Å–ª—É—á–∞–π–Ω—ã–º–∏ —Ç–∞–π-–±—Ä–µ–π–∫–∞–º–∏."""
    import random as _random
    if df.empty or n <= 0:
        return df.iloc[0:0]
    n = min(n, len(df))

    if seed is not None:
        _random.seed(seed)

    coords = df[["lat", "lon"]].to_numpy()

    # —Å—Ç–∞—Ä—Ç: —Å–ª—É—á–∞–π–Ω—ã–π (–∏–ª–∏ –æ—Ç –º–µ–¥–∏–∞–Ω—ã, –µ—Å–ª–∏ random_start=False)
    if random_start:
        start_idx = _random.randrange(len(df))
    else:
        lat_med = float(df["lat"].median())
        lon_med = float(df["lon"].median())
        start_idx = min(
            range(len(df)),
            key=lambda i: haversine_km((lat_med, lon_med), (coords[i][0], coords[i][1]))
        )

    chosen = [start_idx]
    dists = [
        haversine_km((coords[start_idx][0], coords[start_idx][1]), (coords[i][0], coords[i][1]))
        for i in range(len(df))
    ]

    while len(chosen) < n:
        maxd = max(dists)
        candidates = [i for i, d in enumerate(dists) if d == maxd]
        next_idx = _random.choice(candidates)  # —Å–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä —Å—Ä–µ–¥–∏ —Å–∞–º—ã—Ö ¬´–¥–∞–ª—å–Ω–∏—Ö¬ª
        chosen.append(next_idx)
        cx, cy = coords[next_idx]
        for i in range(len(df)):
            d = haversine_km((cx, cy), (coords[i][0], coords[i][1]))
            if d < dists[i]:
                dists[i] = d

    res = df.iloc[chosen].copy()
    # –∏–Ω—Ñ–æ-–ø–æ–ª–µ: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–∏—Å—Ç–∞–Ω—Ü–∏—è –¥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ
    res["min_dist_to_others_km"] = 0.0
    cc = res[["lat","lon"]].to_numpy()
    for i in range(len(res)):
        mind = min(
            haversine_km((cc[i][0], cc[i][1]), (cc[j][0], cc[j][1]))
            for j in range(len(res)) if j != i
        ) if len(res) > 1 else 0.0
        res.iat[i, res.columns.get_loc("min_dist_to_others_km")] = round(mind, 3)
    return res

def parse_kwargs(parts: list[str]) -> dict[str, str]:
    """–ü–∞—Ä—Å–∏–º —Ö–≤–æ—Å—Ç –∫–æ–º–∞–Ω–¥—ã –≤–∏–¥–∞ key=value (–∑–Ω–∞—á–µ–Ω–∏—è –º–æ–∂–Ω–æ –±—Ä–∞—Ç—å –≤ –∫–∞–≤—ã—á–∫–∏)."""
    out: dict[str,str] = {}
    for p in parts:
        if "=" in p:
            k, v = p.split("=", 1)
            out[k.strip()] = v.strip().strip('"').strip("'")
    return out

def _split_list(val: str) -> list[str]:
    if not val:
        return []
    return [x.strip() for x in val.replace(";", ",").split(",") if x.strip()]

def parse_fields(arg: str) -> list[str]:
    allowed = {
        "screen_id","name","city","format","owner","lat","lon",
        "distance_km","min_dist_to_others_km"
    }
    cols = [c.strip() for c in arg.split(",")]
    return [c for c in cols if c in allowed]

def parse_list(val: str) -> list[str]:
    if not isinstance(val, str):
        return []
    # –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏: –∑–∞–ø—è—Ç–∞—è, —Ç–æ—á–∫–∞ —Å –∑–∞–ø—è—Ç–æ–π, –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–∞—è —á–µ—Ä—Ç–∞
    for sep in ("|", ";"):
        val = val.replace(sep, ",")
    return [x.strip() for x in val.split(",") if x.strip()]

def apply_filters(df: pd.DataFrame, kwargs: dict[str,str]) -> pd.DataFrame:
    """
    –ü–æ–¥–¥–µ—Ä–∂–∫–∞:
      - format=...  (–æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ: comma/; / |)
        —Å–ø–µ—Ü-–∞–ª–∏–∞—Å: city / –≥–∏–¥—ã ‚Üí –≤—Å–µ, —á—Ç–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å CITY_FORMAT
      - owner=...   (–æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ: comma/; / |), –ø–æ–¥—Å—Ç—Ä–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ (case-insensitive)
    """
    out = df

    # -------- FORMAT --------
    fmt_val = kwargs.get("format") or kwargs.get("formats") or kwargs.get("format_in")
    if fmt_val:
        fmt_list_raw = parse_list(fmt_val)
        fmt_list = [s.upper() for s in fmt_list_raw]
        mask = None
        col = out["format"].astype(str).str.upper()

        for f in fmt_list:
            if f.lower() in {"city", "city_format", "cityformat", "citylight", "–≥–∏–¥", "–≥–∏–¥—ã"}:
                m = col.str.startswith("CITY_FORMAT")
            else:
                m = (col == f)
            mask = m if mask is None else (mask | m)

        if mask is not None:
            out = out[mask]

    # -------- OWNER --------
    own_val = kwargs.get("owner") or kwargs.get("owners") or kwargs.get("owner_in")
    if own_val:
        owners = parse_list(own_val)
        mask = None
        col = out["owner"].astype(str).str.lower()
        for o in owners:
            m = col.str.contains(o.strip().lower())
            mask = m if mask is None else (mask | m)
        if mask is not None:
            out = out[mask]

    return out

import io
from aiogram.types import BufferedInputFile

# –†–∞–∑–±–∏–≤–∫–∞ –¥–ª–∏–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏ (—á—Ç–æ–±—ã –¢–µ–ª–µ–≥—Ä–∞–º –≤—Å—ë —É–º–µ—Å—Ç–∏–ª)
async def send_lines(message, lines, header: str | None = None, chunk: int = 60, parse_mode: str | None = None):
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ –ø–∞—á–∫–∞–º–∏.
    - chunk: –º–∞–∫—Å. –∫–æ–ª-–≤–æ —Å—Ç—Ä–æ–∫ –≤ –æ–¥–Ω–æ–º —Å–æ–æ–±—â–µ–Ω–∏–∏ (–¥–æ–ø. –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ)
    - —Ç–∞–∫–∂–µ —Ä–µ–∂–µ—Ç –ø–æ –ª–∏–º–∏—Ç—É —Å–∏–º–≤–æ–ª–æ–≤ Telegram (~4096), –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–ø–∞—Å 3900.
    """
    if not lines:
        if header:
            await message.answer(header, parse_mode=parse_mode)
        return

    # –æ—Ç–ø—Ä–∞–≤–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º
    if header:
        await message.answer(header, parse_mode=parse_mode)

    MAX_CHARS = 3900  # –Ω–µ–±–æ–ª—å—à–æ–π –∑–∞–ø–∞—Å –∫ –ª–∏–º–∏—Ç—É Telegram
    buf: list[str] = []
    buf_len = 0
    buf_cnt = 0

    for line in lines:
        s = str(line)
        # –µ—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å–∞–º–∞ –ø–æ —Å–µ–±–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è ‚Äî –ø–æ—Ä–µ–∂–µ–º –≥—Ä—É–±–æ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        if len(s) > MAX_CHARS:
            # —Å–Ω–∞—á–∞–ª–∞ –≤—ã–ª—å–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–µ
            if buf:
                await message.answer("\n".join(buf), parse_mode=parse_mode)
                buf, buf_len, buf_cnt = [], 0, 0
            # –ø–æ—Ä–µ–∑–∞—Ç—å –æ–¥–Ω—É –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É
            for i in range(0, len(s), MAX_CHARS):
                await message.answer(s[i:i+MAX_CHARS], parse_mode=parse_mode)
            continue

        # –ø—Ä–æ–≤–µ—Ä—è–µ–º, –≤–ª–µ–∑–µ—Ç –ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ —Ç–µ–∫—É—â–∏–π –±—É—Ñ–µ—Ä
        if buf and (buf_len + 1 + len(s) > MAX_CHARS or buf_cnt >= chunk):
            await message.answer("\n".join(buf), parse_mode=parse_mode)
            buf, buf_len, buf_cnt = [], 0, 0

        # –¥–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É
        buf.append(s)
        buf_len += (len(s) + 1)  # +1 –∑–∞ –ø–µ—Ä–µ–≤–æ–¥ —Å—Ç—Ä–æ–∫–∏
        buf_cnt += 1

    # –¥–æ–±—Ä–æ—Å–∏–º –æ—Å—Ç–∞—Ç–æ–∫
    if buf:
        await message.answer("\n".join(buf), parse_mode=parse_mode)

def _format_mask(series: pd.Series, token: str) -> pd.Series:
    """
    –ë—É–ª–µ–≤–∞ –º–∞—Å–∫–∞ –ø–æ —Ñ–æ—Ä–º–∞—Ç—É:
      - 'city', '–≥–∏–¥', ... ‚Üí –≤—Å—ë, —á—Ç–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å CITY_FORMAT
      - 'billboard', 'bb'  ‚Üí BILLBOARD
      - –∏–Ω–∞—á–µ ‚Äî —Ç–æ—á–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ –≤–µ—Ä—Ö–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    –ü—Ä–æ–±–µ–ª—ã –∏ —Ä–µ–≥–∏—Å—Ç—Ä –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è.
    """
    col = series.astype(str).str.upper().str.strip()
    t = token.strip().upper()
    if t in {"CITY", "CITY_FORMAT", "CITYFORMAT", "CITYLIGHT", "–ì–ò–î", "–ì–ò–î–´"}:
        return col.str.startswith("CITY_FORMAT")
    if t in {"BILLBOARD", "BB"}:
        return col == "BILLBOARD"
    return col == t


def save_screens_cache(df: pd.DataFrame):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫—ç—à –Ω–∞ –¥–∏—Å–∫ –≤ data/screens_cache.*"""
    global LAST_SYNC_TS

    try:
        if df is None or df.empty:
            return False

        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º parquet –∏ csv
        df.to_parquet(CACHE_PARQUET, index=False)
        df.to_csv(CACHE_CSV, index=False, encoding="utf-8-sig")

        LAST_SYNC_TS = time.time()
        meta = {"ts": LAST_SYNC_TS, "rows": len(df)}
        CACHE_META.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")

        print(f"üíæ –ö—ç—à —Å–æ—Ö—Ä–∞–Ω—ë–Ω –Ω–∞ –¥–∏—Å–∫: {len(df)} —Å—Ç—Ä–æ–∫.")
        return True
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫—ç—à–∞: {e}")
        return False


def load_screens_cache() -> bool:
    """–ü—ã—Ç–∞–µ—Ç—Å—è –ø–æ–¥–Ω—è—Ç—å –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å –∏–∑ –∫—ç—à–∞. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True/False ‚Äî —É–¥–∞–ª–æ—Å—å –ª–∏."""
    global SCREENS, LAST_SYNC_TS

    df: pd.DataFrame | None = None

    # –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º parquet ‚Äî –±—ã—Å—Ç—Ä–µ–µ
    if CACHE_PARQUET.exists():
        try:
            df = pd.read_parquet(CACHE_PARQUET)
        except Exception:
            df = None

    # –µ—Å–ª–∏ parquet –Ω–µ —É–¥–∞–ª–æ—Å—å ‚Äî –ø—Ä–æ–±—É–µ–º csv
    if df is None and CACHE_CSV.exists():
        try:
            df = pd.read_csv(CACHE_CSV)
        except Exception:
            df = None

    if df is None or df.empty:
        return False

    SCREENS = df

    # —á–∏—Ç–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–≤—Ä–µ–º—è/–∫–æ–ª-–≤–æ), –µ—Å–ª–∏ –µ—Å—Ç—å
    try:
        meta = json.loads(CACHE_META.read_text(encoding="utf-8"))
        LAST_SYNC_TS = float(meta.get("ts")) if "ts" in meta else None
    except Exception:
        LAST_SYNC_TS = None

    return True

if load_screens_cache():
    logging.info(f"Loaded screens cache: {len(SCREENS)} rows, ts={LAST_SYNC_TS}")
else:
    logging.info("No local screens cache found.")

def parse_mix(val: str) -> list[tuple[str, str]]:
    """
    –†–∞–∑–±–æ—Ä —Å—Ç—Ä–æ–∫–∏ mix=... –Ω–∞ –ø–∞—Ä—ã (token, value_str).
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏: ',', ';', '|'
    –ü—Ä–∏–º–µ—Ä—ã:
      "BILLBOARD:90%,CITY:10%"
      "CITY_FORMAT_RC:5,CITY_FORMAT_WD:15"
    """
    if not isinstance(val, str) or not val.strip():
        return []
    s = val.replace("|", ",").replace(";", ",")
    items = []
    for part in s.split(","):
        part = part.strip()
        if not part or ":" not in part:
            continue
        token, v = part.split(":", 1)
        items.append((token.strip(), v.strip()))
    return items

def reach_score(item: dict, hours_per_day: int) -> float:
    # 1) –µ—Å–ª–∏ –µ—Å—Ç—å GRP ‚Äî –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ–º –µ–≥–æ (–∫–∞–∫ ¬´–æ—Ö–≤–∞—Ç–Ω–æ—Å—Ç—å¬ª —ç–∫—Ä–∞–Ω–∞)
    g = item.get("grp")
    if g is not None:
        try:
            return float(g)
        except Exception:
            pass

    # 2) –∏–Ω–∞—á–µ OTS –∫–∞–∫ —Å—É—Ä—Ä–æ–≥–∞—Ç
    ots = item.get("ots")
    if ots is not None:
        try:
            return float(ots)
        except Exception:
            pass

    # 3) –ø—Ä–µ–∂–Ω–∏–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ (fallback)
    a = (item.get("audience_per_day") or item.get("audiencePerDay") or 0) or 0
    if a:
        return float(a)

    traffic_h = item.get("traffic_per_hour") or item.get("trafficPerHour")
    vis = item.get("visibility_index") or item.get("visibilityIndex") or 1.0
    if traffic_h:
        try:
            return float(traffic_h) * hours_per_day * float(vis)
        except Exception:
            return float(traffic_h) * hours_per_day

    vpl = item.get("viewers_per_loop") or item.get("viewersPerLoop")
    lph = item.get("loops_per_hour") or item.get("loopsPerHour")
    if vpl and lph:
        try:
            return float(vpl) * float(lph) * hours_per_day
        except Exception:
            return float(vpl) * hours_per_day

    return 0.0

# -------- helpers --------

import re

def parse_kv(text: str) -> dict:
    """
    –†–∞–∑–±–∏—Ä–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –≤–∏–¥–∞ "key=val key2=val2" –∏–ª–∏ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ –∑–∞–ø—è—Ç—ã–º–∏/—Ç–æ—á–∫–∞–º–∏ —Å –∑–∞–ø—è—Ç–æ–π.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict —Å –∫–ª—é—á–∞–º–∏ –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ.
    """
    kv = {}
    # –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º "key=val key=val", –∞ —Ç–∞–∫–∂–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ , ; \n
    parts = re.split(r"[,\n;]\s*|\s+(?=\w+=)", text.strip())
    for part in parts:
        if "=" in part:
            k, v = part.split("=", 1)
            kv[k.strip().lower()] = v.strip()
    return kv

def _allocate_counts(total_n: int, mix_items: list[tuple[str, str]]) -> list[tuple[str, int]]:
    """
    –í—Ö–æ–¥: [('BILLBOARD','90%'), ('CITY','10%')] –∏–ª–∏ [('BILLBOARD','18'), ('CITY','2')]
    –í—ã—Ö–æ–¥: [('BILLBOARD', 18), ('CITY', 2)]
    –ü—Ä–∞–≤–∏–ª–∞:
      - –µ—Å–ª–∏ –µ—Å—Ç—å —Å—É—Ñ—Ñ–∏–∫—Å '%', —Å—á–∏—Ç–∞–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã (—Å –æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ–º –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –æ—Å—Ç–∞—Ç–∫–∞)
      - —á–∏—Å–ª–∞ –±–µ–∑ % —Ç—Ä–∞–∫—Ç—É—é—Ç—Å—è –∫–∞–∫ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —à—Ç—É–∫–∏
      - –¥–æ–ø—É—Å–∫–∞–µ—Ç—Å—è —Å–º–µ—à–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º: —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ + –ø—Ä–æ—Ü–µ–Ω—Ç—ã –Ω–∞ –æ—Å—Ç–∞—Ç–æ–∫
    """
    fixed: list[tuple[str, int]] = []
    perc:  list[tuple[str, float]] = []

    for token, v in mix_items:
        if v.endswith("%"):
            try:
                perc.append((token, float(v[:-1])))
            except:
                pass
        else:
            try:
                fixed.append((token, int(v)))
            except:
                pass

    # —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —á–∞—Å—Ç—å
    fixed_sum = sum(cnt for _, cnt in fixed)
    remaining = max(0, total_n - fixed_sum)

    # –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è —á–∞—Å—Ç—å
    out: list[tuple[str, int]] = fixed[:]
    if remaining > 0 and perc:
        p_total = sum(p for _, p in perc)
        if p_total <= 0:
            # –µ—Å–ª–∏ –ø—Ä–æ—Ü–µ–Ω—Ç—ã –∑–∞–¥–∞–Ω—ã, –Ω–æ —Å—É–º–º–∞ –Ω—É–ª–µ–≤–∞—è/–Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è ‚Äî –ø—Ä–æ—Å—Ç–æ –æ—Ç–¥–∞—ë–º –≤—Å—ë –ø–µ—Ä–≤–æ–º—É
            out.append((perc[0][0], remaining))
        else:
            # –±–∞–∑–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ + —Ä–∞–∑–¥–∞—á–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤ –ø–æ —É–±—ã–≤–∞–Ω–∏—é –¥—Ä–æ–±–Ω–æ–π —á–∞—Å—Ç–∏
            raw = [(tok, remaining * p / p_total) for tok, p in perc]
            base = [(tok, int(x)) for tok, x in raw]
            used = sum(cnt for _, cnt in base)
            rem  = remaining - used
            fracs = sorted(((x - int(x), tok) for tok, x in raw), reverse=True)
            extra = {}
            for i in range(rem):
                _, tok = fracs[i % len(fracs)]
                extra[tok] = extra.get(tok, 0) + 1
            # —Å–æ–±—Ä–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            for tok, cnt in base:
                out.append((tok, cnt + extra.get(tok, 0)))

    # –µ—Å–ª–∏ –ø—Ä–æ—Ü–µ–Ω—Ç—ã –µ—Å—Ç—å, –∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–µ—Ç –∏ remaining==0 (n –ø–µ—Ä–µ–∫—Ä—ã—Ç–æ —Ñ–∏–∫—Å–∞–º–∏) ‚Äî –ø—Ä–æ—Å—Ç–æ out —É–∂–µ –≥–æ—Ç–æ–≤
    # —É–±–µ–¥–∏–º—Å—è, —á—Ç–æ —Å—É–º–º–∞—Ä–Ω–æ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ–º total_n (–Ω–∞ –≤—Å—è–∫–∏–π)
    total = sum(cnt for _, cnt in out)
    if total > total_n:
        # –æ—Ç—Ä–µ–∂–µ–º –ª–∏—à–Ω–µ–µ —Å –∫–æ–Ω—Ü–∞
        delta = total - total_n
        trimmed = []
        for tok, cnt in out:
            take = max(0, cnt - delta)
            delta -= (cnt - take)
            trimmed.append((tok, take))
            if delta <= 0:
                trimmed.extend(out[len(trimmed):])
                break
        out = trimmed
    return out


def _select_with_mix(df_city: pd.DataFrame, n: int, mix_arg: str | None,
                     *, random_start: bool = True, seed: int | None = None) -> pd.DataFrame:
    """
    –î–µ–ª–∏—Ç –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –Ω–∞ –ø–æ–¥–Ω–∞–±–æ—Ä—ã –ø–æ —Ñ–æ—Ä–º–∞—Ç–∞–º —Å–æ–≥–ª–∞—Å–Ω–æ mix, —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –≤—ã–±–∏—Ä–∞–µ—Ç –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ,
    –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏ –¥–æ–±–∏—Ä–∞–µ—Ç –æ—Å—Ç–∞—Ç–æ–∫ –¢–û–õ–¨–ö–û –∏–∑ —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ mix.
    """
    # –±–µ–∑ mix ‚Üí –æ–±—ã—á–Ω—ã–π —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä
    if not mix_arg:
        return spread_select(df_city.reset_index(drop=True), n, random_start=random_start, seed=seed)

    items = parse_mix(mix_arg)
    if not items:
        return spread_select(df_city.reset_index(drop=True), n, random_start=random_start, seed=seed)

    # —Å–ø–∏—Å–æ–∫ —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –∏–∑ mix (–∫–∞–∫ —Ç–æ–∫–µ–Ω—ã)
    allowed_tokens = [tok for tok, _ in items]

    # —Å—É–∑–∏–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø—É–ª —Å—Ä–∞–∑—É —Ç–æ–ª—å–∫–æ –∫ —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∞–º
    if "format" not in df_city.columns:
        # –Ω–∞ –≤—Å—è–∫–∏–π ‚Äî –µ—Å–ª–∏ –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ format, –ø–∞–¥–∞–µ–º –≤ –æ–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
        base_pool = df_city.copy()
    else:
        mask_allowed = None
        col = df_city["format"]
        for tok in allowed_tokens:
            m = _format_mask(col, tok)
            mask_allowed = m if mask_allowed is None else (mask_allowed | m)
        base_pool = df_city[mask_allowed] if mask_allowed is not None else df_city.copy()

    if base_pool.empty:
        # –Ω–∏—á–µ–≥–æ –∏–∑ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ ‚Äî –≤–µ—Ä–Ω—ë–º –æ–±—ã—á–Ω—ã–π —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–π –∏–∑ –≤—Å–µ–≥–æ (—á—Ç–æ–±—ã –Ω–µ –ø—É—Å—Ç–æ)
        return spread_select(df_city.reset_index(drop=True), n, random_start=random_start, seed=seed)

    targets = _allocate_counts(n, items)  # [('BILLBOARD', 18), ('CITY', 2)]
    selected_parts: list[pd.DataFrame] = []
    used_ids: set[str] = set()

    pool = base_pool.copy()

    # –≤—ã–±–∏—Ä–∞–µ–º –ø–æ –∫–≤–æ—Ç–∞–º
    for token, need in targets:
        if need <= 0 or pool.empty:
            continue
        mask = _format_mask(pool["format"], token) if "format" in pool.columns else pd.Series([True]*len(pool), index=pool.index)
        subset = pool[mask]
        if subset.empty:
            continue
        pick_n = min(need, len(subset))
        picked = spread_select(subset.reset_index(drop=True), pick_n, random_start=random_start, seed=seed)
        selected_parts.append(picked)

        # –∏—Å–∫–ª—é—á–∏–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –∏–∑ –ø—É–ª–∞
        if "screen_id" in pool.columns and "screen_id" in picked.columns:
            chosen_ids = picked["screen_id"].astype(str).tolist()
            used_ids.update(chosen_ids)
            pool = pool[~pool["screen_id"].astype(str).isin(used_ids)]
        else:
            # fallback –ø–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º
            coords = set((float(a), float(b)) for a, b in picked[["lat","lon"]].itertuples(index=False, name=None))
            pool = pool[~((pool["lat"].astype(float).round(7).isin([x for x, _ in coords])) &
                          (pool["lon"].astype(float).round(7).isin([y for _, y in coords])))]
        if pool.empty:
            break

    combined = pd.concat(selected_parts, ignore_index=True) if selected_parts else base_pool.iloc[0:0]

    # –¥–æ–±–∏—Ä–∞–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–µ–µ –¢–û–õ–¨–ö–û –∏–∑ base_pool (—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã)
    remain = n - len(combined)
    if remain > 0 and not pool.empty:
        extra = spread_select(pool.reset_index(drop=True), min(remain, len(pool)), random_start=random_start, seed=seed)
        combined = pd.concat([combined, extra], ignore_index=True)

    return combined.head(n)
async def send_gid_xlsx(chat_id: int, ids: list[str], *, filename: str = "screen_ids.xlsx", caption: str = "GID —Å–ø–∏—Å–æ–∫ (XLSX)"):
    """–û—Ç–ø—Ä–∞–≤–∫–∞ XLSX —Å –æ–¥–Ω–∏–º —Å—Ç–æ–ª–±—Ü–æ–º GID –∏–∑ —Å–ø–∏—Å–∫–∞ screen_id."""
    df = pd.DataFrame({"GID": [str(x) for x in ids]})
    bio = io.BytesIO()
    with pd.ExcelWriter(bio, engine="openpyxl") as writer:
        df.to_excel(writer, index=False)
    bio.seek(0)
    await bot.send_document(
        chat_id,
        BufferedInputFile(bio.read(), filename=filename),
        caption=caption,
    )

async def send_gid_if_any(message: types.Message, df: pd.DataFrame, *, filename: str, caption: str):
    """–ï—Å–ª–∏ –≤ df –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∞ screen_id –∏ —Ç–∞–º –Ω–µ –ø—É—Å—Ç–æ ‚Äî –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º XLSX —Å –∫–æ–ª–æ–Ω–∫–æ–π GID."""
    if df is None or df.empty or "screen_id" not in df.columns:
        return
    ids = [s for s in (df["screen_id"].astype(str).tolist()) if str(s).strip() and str(s).lower() != "nan"]
    if ids:
        await send_gid_xlsx(message.chat.id, ids, filename=filename, caption=caption)

# --- ACCESS CONTROL HELPERS ---

def _owner_only(user_id: int) -> bool:
    return TELEGRAM_OWNER_ID == 0 or user_id == TELEGRAM_OWNER_ID

def _auth_headers_all_variants(token: str) -> list[tuple[str, dict]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ (label, headers). label ‚Äî —á—Ç–æ–±—ã –∫—Ä–∞—Å–∏–≤–æ –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å, —á—Ç–æ –º—ã –ø—Ä–æ–±–æ–≤–∞–ª–∏.
    """
    t = (token or "").strip()
    if not t:
        return [("no-auth", {})]
    return [
        ("Bearer",      {"Authorization": f"Bearer {t}"}),
        ("Token",       {"Authorization": f"Token {t}"}),
        ("X-API-Key",   {"X-API-Key": t}),
        ("x-api-key",   {"x-api-key": t}),
        ("X-Auth-Token",{"X-Auth-Token": t}),
        ("Auth-Token",  {"Auth-Token": t}),
        ("authToken",   {"authToken": t}),
        ("Cookie",      {"Cookie": f"authToken={t}"}),
    ]

from urllib.parse import urljoin
import json, aiohttp, logging

# --- PHOTO REPORTS (impression-shots/export) ---
import aiohttp

async def _fetch_impression_shots(
    campaign_id: int,
    per: int = 0,
    want_zip: bool = False,
    m: types.Message | None = None,
    dbg: bool = False,
):
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –ø–æ–ª—É—á–∏—Ç—å —Ñ–æ—Ç–æ–æ—Ç—á—ë—Ç—ã –∫–∞–º–ø–∞–Ω–∏–∏ —Ä–∞–∑–Ω—ã–º–∏ –ø—É—Ç—è–º–∏ –±—ç–∫–∞.
    - –ï—Å–ª–∏ want_zip=True, —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —ç–∫—Å–ø–æ—Ä—Ç ZIP (POST /campaigns/{id}/impression-shots/export)
      –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –º–∞—Ä–∫–µ—Ä {"__binary__": True, "__body__": bytes}.
    - –ò–Ω–∞—á–µ –ø—Ä–æ–±—É–µ–º JSON —Å–ø–∏—Å–∫–æ–º –∫–∞–¥–æ–≤.
    """
    base = (OBDSP_BASE or "https://proddsp.omniboard360.io").rstrip("/")
    headers = {
        "Authorization": f"Bearer {OBDSP_TOKEN}",
        "Accept": "application/json",
    }
    ssl_param = _make_ssl_param_for_aiohttp()
    timeout = aiohttp.ClientTimeout(total=180)

    async with aiohttp.ClientSession(timeout=timeout) as session:
        # 1) ZIP —ç–∫—Å–ø–æ—Ä—Ç (–µ—Å–ª–∏ –ø—Ä–æ—Å–∏–ª–∏)
        if want_zip:
            url = f"{base}/api/v1.0/campaigns/{campaign_id}/impression-shots/export"
            payload = {
                # –ø–æ –æ–ø—ã—Ç—É —ç—Ç–∏ –ø–æ–ª—è —Ç–∞–∫ –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è; –µ—Å–ª–∏ –Ω–∞ –±—ç–∫–µ –∏–Ω–∞—á–µ ‚Äî –Ω–∏–∂–µ –µ—Å—Ç—å GET-—Ñ–æ–ª–ª–±–µ–∫–∏
                "shotCountPerInventoryCreative": per if per > 0 else 0
            }
            if dbg and m:
                try: await m.answer(f"POST {url} (export, per={per})")
                except: pass
            async with session.post(url, headers=headers, json=payload, ssl=ssl_param) as resp:
                if resp.status == 200:
                    body = await resp.read()
                    # –≤–µ—Ä–Ω—ë–º –∫–∞–∫ ¬´–±–∏–Ω–∞—Ä–Ω—ã–π¬ª –æ—Ç–≤–µ—Ç
                    return [{"__binary__": True, "__body__": body}]
                elif resp.status not in (404, 405):
                    # 401/403/500 ‚Äî —Å—Ä–∞–∑—É –æ—à–∏–±–∫–∞
                    raise RuntimeError(f"API {resp.status}: {await resp.text()}")

        # 2) JSON: —Å–∞–º—ã–π –≤–µ—Ä–æ—è—Ç–Ω—ã–π –ø—É—Ç—å –±–µ–∑ clients
        q = {"shotCountPerInventoryCreative": per} if per > 0 else {}
        url = f"{base}/api/v1.0/campaigns/{campaign_id}/impression-shots"
        if dbg and m:
            try: await m.answer(f"GET {url} {q}")
            except: pass
        async with session.get(url, headers=headers, params=q, ssl=ssl_param) as resp:
            txt = await resp.text()
            if resp.status == 200:
                try:
                    data = await resp.json()
                except Exception:
                    raise RuntimeError(f"–ù–µ JSON: {txt[:400]}")
                # –±—ç–∫–∏ –∏–Ω–æ–≥–¥–∞ –∑–∞–≤–æ—Ä–∞—á–∏–≤–∞—é—Ç –≤ {content:[...]}
                if isinstance(data, dict) and "content" in data:
                    return data.get("content") or []
                return data if isinstance(data, list) else []
            elif resp.status not in (404, 405):
                raise RuntimeError(f"API {resp.status}: {txt[:400]}")

        # 3) JSON: —Å—Ç–∞—Ä—ã–π –ø—É—Ç—å —Å clients
        url = f"{base}/api/v1.0/clients/campaigns/{campaign_id}/impression-shots"
        if dbg and m:
            try: await m.answer(f"GET {url} {q}")
            except: pass
        async with session.get(url, headers=headers, params=q, ssl=ssl_param) as resp:
            txt = await resp.text()
            if resp.status == 200:
                try:
                    data = await resp.json()
                except Exception:
                    raise RuntimeError(f"–ù–µ JSON: {txt[:400]}")
                if isinstance(data, dict) and "content" in data:
                    return data.get("content") or []
                return data if isinstance(data, list) else []
            elif resp.status not in (404, 405):
                raise RuntimeError(f"API {resp.status}: {txt[:400]}")

        # 4) –°–æ–≤—Å–µ–º –æ–±—â–∏–π —Ñ–æ–ª–ª–±–µ–∫ –ø–æ query-–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
        url = f"{base}/api/v1.0/impression-shots"
        params = {"campaignId": campaign_id}
        if per > 0:
            params["shotCountPerInventoryCreative"] = per
        if dbg and m:
            try: await m.answer(f"GET {url} {params}")
            except: pass
        async with session.get(url, headers=headers, params=params, ssl=ssl_param) as resp:
            txt = await resp.text()
            if resp.status == 200:
                try:
                    data = await resp.json()
                except Exception:
                    raise RuntimeError(f"–ù–µ JSON: {txt[:400]}")
                if isinstance(data, dict) and "content" in data:
                    return data.get("content") or []
                return data if isinstance(data, list) else []

            raise RuntimeError(f"API {resp.status}: {txt[:400]}")

# ==== API helpers ====

def _make_ssl_param_for_aiohttp():
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      - False  -> –æ—Ç–∫–ª—é—á–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É (aiohttp –ø—Ä–∏–Ω–∏–º–∞–µ—Ç ssl=False)
      - ssl.SSLContext -> —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º CA (OBDSP_CA_BUNDLE) –∏–ª–∏ certifi
      - None  -> –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∫–æ—Ä–Ω–∏
    """
    if OBDSP_SSL_VERIFY in {"0", "false", "no", "off"}:
        return False  # –æ—Ç–∫–ª—é—á–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É (–Ω–∞ —Å–≤–æ–π —Å—Ç—Ä–∞—Ö –∏ —Ä–∏—Å–∫)

    # –ö–∞—Å—Ç–æ–º–Ω—ã–π –±–∞–Ω–¥–ª, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
    if OBDSP_CA_BUNDLE:
        ctx = ssl.create_default_context(cafile=OBDSP_CA_BUNDLE)
        return ctx

    # –ü–∞–∫–µ—Ç certifi, –µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
    if certifi is not None:
        try:
            ctx = ssl.create_default_context(cafile=certifi.where())
            return ctx
        except Exception:
            pass

    # –ò–Ω–∞—á–µ –ø—É—Å—Ç—å aiohttp –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∫–æ—Ä–Ω–∏
    return None

def _auth_headers() -> dict:
    """–§–æ—Ä–º–∏—Ä—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è API DSP."""
    t = (OBDSP_TOKEN or "").strip()
    scheme = (OBDSP_AUTH_SCHEME or "Bearer").strip().lower()
    if not t:
        return {}
    if scheme == "apikey":
        return {"X-API-Key": t}
    if scheme == "basic":
        return {"Authorization": f"Basic {t}"}
    if scheme == "token":
        return {"Authorization": f"Token {t}"}
    return {"Authorization": f"Bearer {t}"}

def _map_inventory_row(item: dict) -> dict:
    """
    –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –ø–æ–ª–µ–π API –∫ –Ω–∞—à–∏–º –∫–æ–ª–æ–Ω–∫–∞–º: screen_id,name,lat,lon,city,format,owner.
    –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–æ–ø—Ä–∞–≤—å –∞–ª–∏–∞—Å—ã –ø–æ–¥ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π JSON.
    """
    def pick(*keys, default=""):
        for k in keys:
            if k in item and item[k] is not None:
                return item[k]
        return default

    screen_id = str(pick("id", "screen_id", "guid", "gid", default="")).strip()
    name      = str(pick("name", "title", "screen_name", default="")).strip()
    lat       = pick("lat", "latitude", "geo_lat", default=None)
    lon       = pick("lon", "longitude", "geo_lon", default=None)
    city      = str(pick("city", "city_name", default="")).strip()
    format_   = str(pick("format", "screen_format", "type", default="")).strip()
    owner     = str(pick("owner", "vendor", "operator", "owner_name", default="")).strip()

    try:
        lat = float(lat)
        lon = float(lon)
    except (TypeError, ValueError):
        lat, lon = None, None

    return {
        "screen_id": screen_id,
        "name": name,
        "lat": lat,
        "lon": lon,
        "city": city,
        "format": format_,
        "owner": owner,
    }

# --- NORMALIZATION (API ‚Üí DataFrame) ---
def _normalize_api_to_df(items: list[dict]) -> pd.DataFrame:
    """
    –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Å—ã—Ä—ã–µ items –∏–∑ Omniboard /clients/inventories –≤ —É–¥–æ–±–Ω—ã–π DataFrame.
    –ù–∏–∫–∞–∫–æ–π —Ä–µ–∫—É—Ä—Å–∏–∏ —Ç—É—Ç –±—ã—Ç—å –Ω–µ –¥–æ–ª–∂–Ω–æ.
    """
    if not items:
        return pd.DataFrame(columns=[
            "id","screen_id","name","format","placement","installation",
            "owner_id","owner","city","address","lat","lon",
            "width_mm","height_mm","width_px","height_px",
            "phys_width_px","phys_height_px",
            "sspProvider","sspTypes","minBid","ots","grp","meta_format",
            "image_url","image_preview"
        ])

    def g(obj, path, default=None):
        try:
            cur = obj
            for k in path:
                if cur is None:
                    return default
                if isinstance(k, int):
                    cur = (cur or [])[k] if isinstance(cur, list) and len(cur) > k else None
                else:
                    cur = (cur or {}).get(k)
            return default if cur is None else cur
        except Exception:
            return default

    rows = []
    for it in items:
        rows.append({
            "id": it.get("id"),
            "screen_id": it.get("gid"),
            "name": it.get("name"),
            "format": it.get("type"),
            "placement": it.get("placement"),
            "installation": it.get("installation"),

            "owner_id": g(it, ["displayOwner","id"]),
            "owner":    g(it, ["displayOwner","name"]),

            "city":    g(it, ["location","city"]),
            "address": g(it, ["location","address"]),
            "lat":     g(it, ["location","latitude"]),
            "lon":     g(it, ["location","longitude"]),

            "width_mm":  g(it, ["surfaceDimensionMM","width"]),
            "height_mm": g(it, ["surfaceDimensionMM","height"]),
            "width_px":  g(it, ["screenResolutionPx","width"]),
            "height_px": g(it, ["screenResolutionPx","height"]),
            "phys_width_px":  g(it, ["physicalResolutionPx","width"]),
            "phys_height_px": g(it, ["physicalResolutionPx","height"]),

            "sspProvider": it.get("sspProvider"),
            "sspTypes":    ",".join(it.get("sspTypes") or []),

            "minBid": g(it, ["minBidInfo","minBid"]),
            "ots":    g(it, ["minBidInfo","ots"]),
            "grp":    g(it, ["metadata","grp"]),
            "meta_format": g(it, ["metadata","format"]),

            "image_url":     g(it, ["images", 0, "url"]),
            "image_preview": g(it, ["images", 0, "preview"]),
        })

    return pd.DataFrame(rows)


def _normalize_shots(raw: list[dict]) -> pd.DataFrame:
    """
    –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Å—ã—Ä–æ–π —Å–ø–∏—Å–æ–∫ –∫–∞–¥—Ä–æ–≤ –≤ —É–¥–æ–±–Ω—É—é —Ç–∞–±–ª–∏—Ü—É.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ–ª–µ–π, —Å—Ç–∞—Ä–∞—è—Å—å ¬´—É–≥–∞–¥–∞—Ç—å¬ª –≥–¥–µ gid/name/–∫–∞—Ä—Ç–∏–Ω–∫–∞/–≤—Ä–µ–º—è.
    """
    if not raw:
        return pd.DataFrame(columns=[
            "shot_id","campaign_id",
            "inventory_id","inventory_gid","inventory_name",
            "city","address","lat","lon",
            "creative_id","creative_name",
            "shot_time","image_url","preview_url"
        ])

    def g(o, path, default=None):
        try:
            cur = o
            for k in path:
                if cur is None:
                    return default
                cur = cur[k] if isinstance(k, int) else (cur or {}).get(k)
            return default if cur is None else cur
        except Exception:
            return default

    rows = []
    for it in raw:
        inv   = it.get("inventory") or {}
        loc   = inv.get("location") or {}
        img   = it.get("image") or {}              # –±—ã–≤–∞–µ—Ç image: {url, preview}
        # fallback: –∏–Ω–æ–≥–¥–∞ –≤ –∫–æ—Ä–Ω–µ –∫–∞–¥—Ä–∞ –º–æ–≥—É—Ç –ª–µ–∂–∞—Ç—å —Å—Å—ã–ª–∫–∏
        img_url     = img.get("url") or it.get("imageUrl") or it.get("url")
        preview_url = img.get("preview") or it.get("previewUrl") or it.get("thumbnailUrl")

        rows.append({
            "shot_id":       it.get("id"),
            "campaign_id":   g(it, ["campaign","id"]) or it.get("campaignId"),

            "inventory_id":  inv.get("id"),
            "inventory_gid": inv.get("gid"),
            "inventory_name":inv.get("name"),

            "city":          loc.get("city"),
            "address":       loc.get("address"),
            "lat":           loc.get("latitude"),
            "lon":           loc.get("longitude"),

            "creative_id":   g(it, ["creative","id"]) or it.get("creativeId"),
            "creative_name": g(it, ["creative","name"]),

            "shot_time":     it.get("shotTime") or it.get("time") or it.get("created"),
            "image_url":     img_url,
            "preview_url":   preview_url,
        })

    df = pd.DataFrame(rows)
    # –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –ø—Ä–∏–≤–µ–¥—ë–º –≤—Ä–µ–º—è, –µ—Å–ª–∏ –µ—Å—Ç—å
    if "shot_time" in df.columns:
        with pd.option_context("mode.chained_assignment", None):
            try:
                df["shot_time"] = pd.to_datetime(df["shot_time"], errors="coerce", utc=True).dt.tz_convert(None)
            except Exception:
                pass
    return df

# –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å–æ —Å—Ç–∞—Ä—ã–º –∏–º–µ–Ω–µ–º (—Å—Ç–∞—Ä—ã–π –∫–æ–¥ –º–æ–≥ –≤—ã–∑—ã–≤–∞—Ç—å —ç—Ç–æ –∏–º—è)
def _normalize_api_items(items: list[dict]) -> pd.DataFrame:
    return _normalize_api_to_df(items)


# ====== –•–≠–ù–î–õ–ï–†–´ ======
@dp.message(Command("start"))
async def cmd_start(m: types.Message):
    # –∫–æ—Ä–æ—Ç–∫–∏–π —Å—Ç–∞—Ç—É—Å
    if SCREENS is None or SCREENS.empty:
        status = "–≠–∫—Ä–∞–Ω–æ–≤ –µ—â—ë –Ω–µ—Ç ‚Äî –ø—Ä–∏—à–ª–∏—Ç–µ CSV/XLSX."
    else:
        status = f"–≠–∫—Ä–∞–Ω–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(SCREENS)}."
    await m.answer(
        "üëã –ü—Ä–∏–≤–µ—Ç! –Ø –≥–æ—Ç–æ–≤ –ø–æ–º–æ—á—å —Å –ø–æ–¥–±–æ—Ä–æ–º —ç–∫—Ä–∞–Ω–æ–≤.\n"
        f"{status}\n\n"
        "‚ñ∂Ô∏è –ù–∞–∂–º–∏—Ç–µ /help, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å –ø—Ä–∏–º–µ—Ä—ã –∫–æ–º–∞–Ω–¥.",
        reply_markup=make_main_menu()
    )
# ====== imports ======
import os, io, math, asyncio, logging, time, json, ssl
from pathlib import Path
from datetime import datetime
import random
import pandas as pd
import aiohttp

try:
    import certifi  # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
except Exception:
    certifi = None

# aiogram 3.x
from aiogram import Bot, Dispatcher, F, types, Router
from aiogram.types import Message, BufferedInputFile, BotCommand
from aiogram.filters import Command

# ====== logging ======
logging.basicConfig(level=logging.INFO)

# ====== SSL helpers ======
def _ssl_ctx_certifi() -> ssl.SSLContext:
    """–°–æ–∑–¥–∞—ë—Ç –±–µ–∑–æ–ø–∞—Å–Ω—ã–π SSL-–∫–æ–Ω—Ç–µ–∫—Å—Ç —Å CA –∏–∑ certifi, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω."""
    if certifi is not None:
        ctx = ssl.create_default_context(cafile=certifi.where())
    else:
        ctx = ssl.create_default_context()
    ctx.check_hostname = True
    ctx.verify_mode = ssl.CERT_REQUIRED
    return ctx

def _make_ssl_param_for_aiohttp():
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è aiohttp ssl=...
    –ï—Å–ª–∏ OBDSP_SSL_NO_VERIFY=1, –æ—Ç–∫–ª—é—á–∞–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫—É —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞ (–Ω–∞ —Å–≤–æ–π —Å—Ç—Ä–∞—Ö –∏ —Ä–∏—Å–∫).
    """
    no_verify = os.getenv("OBDSP_SSL_NO_VERIFY", "0").strip().lower() in {"1", "true", "yes", "on"}
    if no_verify:
        return False
    return _ssl_ctx_certifi()

# ====== ENV CONFIG ======
OBDSP_BASE = os.getenv("OBDSP_BASE", "https://obdsp.projects.eraga.net").strip()
OBDSP_TOKEN = os.getenv("OBDSP_TOKEN", "").strip()
OBDSP_AUTH_SCHEME = os.getenv("OBDSP_AUTH_SCHEME", "Bearer").strip()
OBDSP_CLIENT_ID = os.getenv("OBDSP_CLIENT_ID", "").strip()

try:
    TELEGRAM_OWNER_ID = int(os.getenv("TELEGRAM_OWNER_ID", "0"))
except Exception:
    TELEGRAM_OWNER_ID = 0

OBDSP_CA_BUNDLE = os.getenv("OBDSP_CA_BUNDLE", "").strip()
OBDSP_SSL_VERIFY = (os.getenv("OBDSP_SSL_VERIFY", "1") or "1").strip().lower()  # "1"/"0"/"true"/"false"
OBDSP_SSL_NO_VERIFY = os.getenv("OBDSP_SSL_NO_VERIFY", "0").strip().lower() in {"1", "true", "yes", "on"}

# ====== PATHS & STATE ======
BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR / "data"
DATA_DIR.mkdir(parents=True, exist_ok=True)

# –ï–î–ò–ù–´–ô –∫—ç—à-–∫–∞—Ç–∞–ª–æ–≥ (–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å env-–ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π)
CACHE_DIR = Path(os.getenv("SCREENS_CACHE_DIR", "/tmp/omnika_cache"))
CACHE_DIR.mkdir(parents=True, exist_ok=True)

CACHE_CSV  = CACHE_DIR / "screens_cache.csv"
CACHE_META = CACHE_DIR / "screens_cache.meta.json"

SCREENS: pd.DataFrame | None = None
LAST_RESULT: pd.DataFrame | None = None
LAST_SELECTION_NAME = "last"
MAX_PLAYS_PER_HOUR = 6
LAST_SYNC_TS: float | None = None

def _cache_diag() -> str:
    """–°—Ç—Ä–æ–∫–∞-–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–ª—è –ª–æ–≥–æ–≤/–æ—Ç–≤–µ—Ç–∞: –≥–¥–µ –ø–∏—à–µ–º –∏ –µ—Å—Ç—å –ª–∏ –ø—Ä–∞–≤–∞."""
    try:
        can_write_dir = os.access(CACHE_DIR, os.W_OK)
        parent = CACHE_DIR.parent
        return (
            f"BASE_DIR={BASE_DIR} | CACHE_DIR={CACHE_DIR} "
            f"| exists={CACHE_DIR.exists()} | writable={can_write_dir} "
            f"| parent_writable={os.access(parent, os.W_OK)}"
        )
    except Exception as e:
        return f"diag_error={e}"

def save_screens_cache(df: pd.DataFrame) -> bool:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫—ç—à –Ω–∞ –¥–∏—Å–∫ (CSV + meta)."""
    global LAST_SYNC_TS
    try:
        if df is None or df.empty:
            logging.warning("save_screens_cache: –ø—É—Å—Ç–æ–π df ‚Äî —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –Ω–µ—á–µ–≥–æ")
            return False

        # —Ç–µ—Å—Ç –∑–∞–ø–∏—Å–∏
        try:
            (CACHE_DIR / ".write_test").write_text("ok", encoding="utf-8")
        except Exception as e:
            logging.error(f"write_test failed: {e} | {_cache_diag()}")
            return False

        df.to_csv(CACHE_CSV, index=False, encoding="utf-8-sig")

        LAST_SYNC_TS = time.time()
        meta = {"ts": LAST_SYNC_TS, "rows": int(len(df))}
        CACHE_META.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")

        logging.info(f"üíæ –ö—ç—à —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {len(df)} —Å—Ç—Ä–æ–∫ ‚Üí {CACHE_CSV} | {_cache_diag()}")
        return True
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫—ç—à–∞: {e} | {_cache_diag()}", exc_info=True)
        return False

def load_screens_cache() -> bool:
    """–ü—ã—Ç–∞–µ—Ç—Å—è –ø–æ–¥–Ω—è—Ç—å –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å –∏–∑ CSV. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True/False."""
    global SCREENS, LAST_SYNC_TS
    try:
        if not CACHE_CSV.exists():
            logging.info(f"–ö—ç—à CSV –Ω–µ –Ω–∞–π–¥–µ–Ω: {CACHE_CSV} | {_cache_diag()}")
            return False

        df = pd.read_csv(CACHE_CSV)
        if df is None or df.empty:
            logging.warning(f"–ö—ç—à CSV –ø—É—Å—Ç–æ–π: {CACHE_CSV}")
            return False

        SCREENS = df

        if CACHE_META.exists():
            meta = json.loads(CACHE_META.read_text(encoding="utf-8"))
            LAST_SYNC_TS = float(meta.get("ts")) if "ts" in meta else None
        else:
            LAST_SYNC_TS = None

        logging.info(f"Loaded screens cache: {len(SCREENS)} rows, ts={LAST_SYNC_TS} | {_cache_diag()}")
        return True
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫—ç—à–∞: {e} | {_cache_diag()}", exc_info=True)
        return False

# ====== BOT (aiogram 3.x) ======
BOT_TOKEN = os.getenv("BOT_TOKEN")
if not BOT_TOKEN:
    raise SystemExit("Set BOT_TOKEN env var first")

bot = Bot(BOT_TOKEN)
dp = Dispatcher()
router = Router()

# ---------- –±–∞–∑–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã ----------
@router.message(Command("start"))
async def start_cmd(m: Message):
    await m.answer("–Ø –Ω–∞ –º–µ—Å—Ç–µ. –î–æ—Å—Ç—É–ø–Ω–æ: /cache_info, /sync_api")

@router.message(Command("ping"))
async def ping_cmd(m: Message):
    await m.answer("pong")

# ---------- –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∫—ç—à–∞ ----------
@router.message(Command("cache_info"))
async def cache_info(m: Message):
    try:
        lines = [
            f"CACHE_DIR: {CACHE_DIR}",
            f"exists: {CACHE_DIR.exists()}",
            f"writable: {os.access(CACHE_DIR, os.W_OK)}",
            f"CACHE_CSV exists: {CACHE_CSV.exists()}",
            f"CACHE_META exists: {CACHE_META.exists()}",
            f"diag: {_cache_diag()}",
        ]
        await m.answer("\n".join(lines))
    except Exception as e:
        await m.answer(f"cache_info error: {e}")

# –¥—É–±–ª–∏—Ä—É—é—â–∏–π –º–∞—Ç—á–∏–Ω–≥ –¥–ª—è –≥—Ä—É–ø–ø/—É–ø–æ–º–∏–Ω–∞–Ω–∏–π
@router.message(F.text.func(lambda t: isinstance(t, str) and t.strip().startswith(("/cache_info", "/cache_info@"))))
async def cache_info_fallback(m: Message):
    await cache_info(m)

# ---------- –°–ò–ù–•–†–û–ù–ò–ó–ê–¶–ò–Ø –ò–ù–í–ï–ù–¢–ê–†–Ø –ò–ó API ----------
# –í–ù–ò–ú–ê–ù–ò–ï: –Ω–∏–∂–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç–≤–æ–∏ —Ñ—É–Ω–∫—Ü–∏–∏ _owner_only, _fetch_inventories, _normalize_api_to_df
# (–æ–Ω–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –Ω–∏–∂–µ –ø–æ —Ñ–∞–π–ª—É –∏–ª–∏ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã).

@router.message(Command("sync_api"))
async def cmd_sync_api(m: types.Message):
    if not _owner_only(m.from_user.id):  # <- —Ç–≤–æ—è —Ñ—É–Ω–∫—Ü–∏—è
        await m.answer("‚õîÔ∏è –¢–æ–ª—å–∫–æ –≤–ª–∞–¥–µ–ª–µ—Ü –±–æ—Ç–∞ –º–æ–∂–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å —ç—Ç—É –∫–æ–º–∞–Ω–¥—É.")
        return

    # --- –ø–∞—Ä—Å–∏–º –æ–ø—Ü–∏–∏ ---
    text = (m.text or "").strip()
    parts = text.split()[1:]

    def _get_opt(name, cast, default):
        for p in parts:
            if p.startswith(name + "="):
                val = p.split("=", 1)[1]
                try:
                    return cast(val)
                except Exception:
                    return default
        return default

    def _as_list(s):
        return [x.strip() for x in str(s).split(",") if x.strip()] if s else []

    pages_limit = _get_opt("pages", int, None)
    page_size   = _get_opt("size", int, 500)
    total_limit = _get_opt("limit", int, None)

    # —Ñ–∏–ª—å—Ç—Ä—ã –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è
    city     = _get_opt("city", str, "").strip()
    formats  = _as_list(_get_opt("formats", str, "") or _get_opt("format", str, ""))
    owners   = _as_list(_get_opt("owners", str, "")  or _get_opt("owner", str, ""))

    # –ª—é–±—ã–µ –¥–æ–ø. api.* -> –ø—Ä—è–º–æ –≤ query
    raw_api = {}
    for p in parts:
        if p.startswith("api.") and "=" in p:
            k, v = p.split("=", 1)
            raw_api[k[4:]] = v

    filters = {
        "city": city,
        "formats": formats,
        "owners": owners,
        "api_params": raw_api,
    }

    pretty = []
    if city:    pretty.append(f"city={city}")
    if formats: pretty.append(f"formats={','.join(formats)}")
    if owners:  pretty.append(f"owners={','.join(owners)}")
    if raw_api: pretty.append("+" + "&".join(f"{k}={v}" for k, v in raw_api.items()))
    hint = (" (—Ñ–∏–ª—å—Ç—Ä—ã: " + ", ".join(pretty) + ")") if pretty else ""
    await m.answer("‚è≥ –¢—è–Ω—É –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å –∏–∑ –≤–Ω–µ—à–Ω–µ–≥–æ API‚Ä¶" + hint)

    # --- —Ç—è–Ω–µ–º, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º, —Å–æ—Ö—Ä–∞–Ω—è–µ–º ---
    try:
        items = await _fetch_inventories(   # <- —Ç–≤–æ—è —Ñ—É–Ω–∫—Ü–∏—è
            pages_limit=pages_limit,
            page_size=page_size,
            total_limit=total_limit,
            m=m,
            filters=filters,
        )
    except Exception as e:
        logging.exception("sync_api failed")
        await m.answer(f"üö´ –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∏–Ω–∫–Ω—É—Ç—å: {e}")
        return

    if not items:
        await m.answer("API –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫.")
        return

    # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è -> DataFrame
    df = _normalize_api_to_df(items)  # <- —Ç–≤–æ—è —Ñ—É–Ω–∫—Ü–∏—è
    if df.empty:
        await m.answer("–°–ø–∏—Å–æ–∫ –ø—Ä–∏—à—ë–ª, –Ω–æ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—É—Å—Ç–æ (–ø—Ä–æ–≤–µ—Ä—å –º–∞–ø–ø–∏–Ω–≥ –ø–æ–ª–µ–π).")
        return

    # –≤ –ø–∞–º—è—Ç—å
    global SCREENS
    SCREENS = df

    # —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à –Ω–∞ –¥–∏—Å–∫
    try:
        if save_screens_cache(df):
            await m.answer(f"üíæ –ö—ç—à —Å–æ—Ö—Ä–∞–Ω—ë–Ω –Ω–∞ –¥–∏—Å–∫: {len(df)} —Å—Ç—Ä–æ–∫.")
        else:
            await m.answer("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à –Ω–∞ –¥–∏—Å–∫.")
    except Exception as e:
        await m.answer(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫—ç—à–∞: {e}")

    # --- –æ—Ç–ø—Ä–∞–≤–∫–∞ CSV ---
    try:
        csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
        await m.bot.send_document(
            m.chat.id,
            BufferedInputFile(csv_bytes, filename="inventories_sync.csv"),
            caption=f"–ò–Ω–≤–µ–Ω—Ç–∞—Ä—å –∏–∑ API: {len(df)} —Å—Ç—Ä–æ–∫ (CSV)"
        )
    except Exception as e:
        await m.answer(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å CSV: {e}")

    # --- –æ—Ç–ø—Ä–∞–≤–∫–∞ XLSX ---
    try:
        xlsx_buf = io.BytesIO()
        with pd.ExcelWriter(xlsx_buf, engine="openpyxl") as writer:
            df.to_excel(writer, index=False, sheet_name="inventories")
        xlsx_buf.seek(0)
        await m.bot.send_document(
            m.chat.id,
            BufferedInputFile(xlsx_buf.getvalue(), filename="inventories_sync.xlsx"),
            caption=f"–ò–Ω–≤–µ–Ω—Ç–∞—Ä—å –∏–∑ API: {len(df)} —Å—Ç—Ä–æ–∫ (XLSX)"
        )
    except Exception as e:
        await m.answer(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å XLSX: {e} (–ø—Ä–æ–≤–µ—Ä—å, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ openpyxl)")

    await m.answer(f"‚úÖ –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –æ–∫: {len(df)} —ç–∫—Ä–∞–Ω–æ–≤.")

# –§–æ–ª–ª–±–µ–∫-–º–∞—Ç—á–∏–Ω–≥ –ø–æ ¬´—Å—ã—Ä–æ–º—É¬ª —Ç–µ–∫—Å—Ç—É (–≥—Ä—É–ø–ø—ã/—É–ø–æ–º–∏–Ω–∞–Ω–∏—è)
@router.message(F.text.func(lambda t: isinstance(t, str) and t.strip().startswith(("/sync_api", "/sync_api@"))))
async def cmd_sync_api_fallback(m: types.Message):
    await cmd_sync_api(m)

# –õ–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—á–µ–≥–æ (—á—Ç–æ–±—ã –ø–æ–Ω–∏–º–∞—Ç—å, —á—Ç–æ –µ—â—ë –ø—Ä–∏–ª–µ—Ç–∞–µ—Ç)
@router.message()
async def _log_unhandled_messages(m: types.Message):
    logging.info(f"Unhandled message caught by router: "
                 f"type={m.content_type}, chat={m.chat.type}, text={getattr(m, 'text', None)!r}")

@router.chat_member()
async def _log_chat_member(ev: types.ChatMemberUpdated):
    logging.info(f"ChatMember update: chat={ev.chat.id}, old={ev.old_chat_member.status}, new={ev.new_chat_member.status}")

# ---------- —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —Ä–æ—É—Ç–µ—Ä–∞ –∏ –∑–∞–ø—É—Å–∫ ----------
dp.include_router(router)

async def main():
    # –ø—Ä–æ–±—É–µ–º –ø–æ–¥–Ω—è—Ç—å –∫—ç—à –Ω–∞ —Å—Ç–∞—Ä—Ç–µ (–Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ)
    try:
        load_screens_cache()
    except Exception as e:
        logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫—ç—à –Ω–∞ —Å—Ç–∞—Ä—Ç–µ: {e}")

    # –ü–æ–¥—Å–∫–∞–∑–∫–∏ –∫–æ–º–∞–Ω–¥ –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ Telegram
    await bot.set_my_commands([
        BotCommand(command="start", description="–ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –±–æ—Ç –∂–∏–≤"),
        BotCommand(command="ping", description="–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–≤–µ—Ç–∞"),
        BotCommand(command="cache_info", description="–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∫—ç—à–∞"),
        BotCommand(command="sync_api", description="–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∏–Ω–≤–µ–Ω—Ç–∞—Ä—è –∏–∑ API"),
    ])

    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())

@dp.message(Command("shots"))
async def cmd_shots(m: types.Message):
    if not _owner_only(m.from_user.id):
        await m.answer("‚õîÔ∏è –¢–æ–ª—å–∫–æ –≤–ª–∞–¥–µ–ª–µ—Ü –±–æ—Ç–∞ –º–æ–∂–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å —ç—Ç—É –∫–æ–º–∞–Ω–¥—É.")
        return

    # --- –ø–∞—Ä—Å–∏–º –æ–ø—Ü–∏–∏ ---
    text = (m.text or "").strip()
    parts = text.split()[1:]

    def _get_opt(name, cast, default):
        for p in parts:
            if p.startswith(name + "="):
                v = p.split("=", 1)[1]
                try:
                    return cast(v)
                except:
                    return default
        return default

    def _get_str(name, default=""):
        for p in parts:
            if p.startswith(name + "="):
                return p.split("=", 1)[1]
        return default

    campaign_id = _get_opt("campaign", int, None)
    per         = _get_opt("per", int, 0)   # shotCountPerInventoryCreative
    limit       = _get_opt("limit", int, None)
    want_zip    = str(_get_str("zip", "0")).lower() in {"1","true","yes","on"}
    fields_req  = _get_str("fields", "").strip()
    dbg         = str(_get_str("dbg", "0")).lower() in {"1","true","yes","on"}

    if not campaign_id:
        await m.answer("–§–æ—Ä–º–∞—Ç: /shots campaign=<ID> [per=0] [limit=100] [zip=1] [fields=...]")
        return

    await m.answer(f"‚è≥ –°–æ–±–∏—Ä–∞—é —Ñ–æ—Ç–æ–æ—Ç—á—ë—Ç –ø–æ –∫–∞–º–ø–∞–Ω–∏–∏ {campaign_id}‚Ä¶")

    # --- –∑–∞–ø—Ä–æ—Å –≤ API ---
    try:
        shots = await _fetch_impression_shots(
            campaign_id,
            per=per,
            want_zip=want_zip,   # ‚Üê –¥–æ–±–∞–≤–∏–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∫—É ZIP
            m=m,
            dbg=False            # –º–æ–∂–Ω–æ True, –µ—Å–ª–∏ —Ö–æ—á–µ—à—å –≤–∏–¥–µ—Ç—å, –∫–∞–∫–∏–µ URL –æ–Ω –ø—Ä–æ–±—É–µ—Ç
        )
    except Exception as e:
        await m.answer(f"üö´ –û—à–∏–±–∫–∞ API: {e}")
        return

    # --- –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ —Ç–∞–±–ª–∏—Ü—É ---
    df = _normalize_shots(shots)
    if limit and not df.empty and len(df) > limit:
        df = df.head(limit)

    if df.empty:
        await m.answer("–§–æ—Ç–æ–æ—Ç—á—ë—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return

    # --- –æ—Ç–ø—Ä–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---
    if fields_req:
        cols = [c.strip() for c in fields_req.split(",") if c.strip()]
        cols = [c for c in cols if c in df.columns]
        if not cols:
            await m.answer("–ü–æ–ª—è –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω—ã. –î–æ—Å—Ç—É–ø–Ω—ã–µ: " + ", ".join(df.columns))
            return
        view = df[cols].copy()
        csv_bytes = view.to_csv(index=False).encode("utf-8-sig")
        await bot.send_document(
            m.chat.id,
            BufferedInputFile(csv_bytes, filename=f"shots_{campaign_id}.csv"),
            caption=f"–ö–∞–¥—Ä—ã –∫–∞–º–ø–∞–Ω–∏–∏ {campaign_id} (–ø–æ–ª—è: {', '.join(cols)})"
        )
    else:
        # –ü–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä: CSV + XLSX
        try:
            csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
            await bot.send_document(
                m.chat.id,
                BufferedInputFile(csv_bytes, filename=f"shots_{campaign_id}.csv"),
                caption=f"–§–æ—Ç–æ–æ—Ç—á—ë—Ç –∫–∞–º–ø–∞–Ω–∏–∏ {campaign_id}: {len(df)} —Å—Ç—Ä–æ–∫ (CSV)"
            )
        except Exception as e:
            await m.answer(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å CSV: {e}")

        try:
            import io as _io
            xbuf = _io.BytesIO()
            with pd.ExcelWriter(xbuf, engine="openpyxl") as w:
                df.to_excel(w, index=False, sheet_name="shots")
            xbuf.seek(0)
            await bot.send_document(
                m.chat.id,
                BufferedInputFile(xbuf.getvalue(), filename=f"shots_{campaign_id}.xlsx"),
                caption=f"–§–æ—Ç–æ–æ—Ç—á—ë—Ç –∫–∞–º–ø–∞–Ω–∏–∏ {campaign_id}: {len(df)} —Å—Ç—Ä–æ–∫ (XLSX)"
            )
        except Exception as e:
            await m.answer(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å XLSX: {e} (–ø—Ä–æ–≤–µ—Ä—å openpyxl)")

    # --- –ø–æ –∑–∞–ø—Ä–æ—Å—É —Å–æ–±–µ—Ä—ë–º ZIP —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ ---
    if want_zip:
        import io as _io, zipfile, aiohttp, asyncio
        urls = [u for u in (df["image_url"].dropna().tolist() or []) if isinstance(u, str) and u.startswith("http")]
        if not urls:
            await m.answer("–ù–µ—Ç —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, zip –Ω–µ —Å–æ–±—Ä–∞–Ω.")
            return

        await m.answer(f"üì¶ –°–∫–∞—á–∏–≤–∞—é {len(urls)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π‚Ä¶")
        ssl_param = _make_ssl_param_for_aiohttp()
        timeout = aiohttp.ClientTimeout(total=300)

        zip_buf = _io.BytesIO()
        async with aiohttp.ClientSession(timeout=timeout) as session, zipfile.ZipFile(zip_buf, "w", zipfile.ZIP_DEFLATED) as zf:
            sem = asyncio.Semaphore(8)

            async def grab(i, url):
                async with sem:
                    try:
                        async with session.get(url, ssl=ssl_param) as r:
                            if r.status == 200:
                                content = await r.read()
                                zf.writestr(f"shot_{i:05d}.jpg", content)
                    except Exception:
                        pass

            await asyncio.gather(*[grab(i, u) for i, u in enumerate(urls, 1)])

        zip_buf.seek(0)
        await bot.send_document(
            m.chat.id,
            BufferedInputFile(zip_buf.getvalue(), filename=f"shots_{campaign_id}.zip"),
            caption="ZIP —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏"
        )

@dp.message(Command("status"))
async def cmd_status(m: types.Message):
    global SCREENS

    base = (OBDSP_BASE or "").strip()
    tok  = (OBDSP_TOKEN or "").strip()
    screens_count = len(SCREENS) if SCREENS is not None else 0

    text = [
        "üìä *OmniDSP Bot Status*",
        f"‚Ä¢ API Base: `{base or '‚Äî'}`",
        f"‚Ä¢ Token: {'‚úÖ' if tok else '‚ùå –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}",
        f"‚Ä¢ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —ç–∫—Ä–∞–Ω–æ–≤: *{screens_count}*",
    ]

    if screens_count:
        text.append(f"‚Ä¢ –ü—Ä–∏–º–µ—Ä –≥–æ—Ä–æ–¥–æ–≤: {', '.join(SCREENS['city'].dropna().astype(str).unique()[:5])}")

    await m.answer("\n".join(text), parse_mode="Markdown")

@dp.message(Command("reload_cache"))
async def cmd_reload_cache(m: types.Message):
    if not _owner_only(m.from_user.id):
        return
    ok = load_screens_cache()
    if ok:
        await m.answer(f"üîÑ –ö—ç—à –ø–æ–¥–≥—Ä—É–∂–µ–Ω: {len(SCREENS)} —Å—Ç—Ä–æ–∫.")
    else:
        await m.answer("‚ùå –ö—ç—à –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç.")

@dp.message(Command("clear_cache"))
async def cmd_clear_cache(m: types.Message):
    if not _owner_only(m.from_user.id):
        return
    removed = []
    for p in (CACHE_PARQUET, CACHE_CSV, CACHE_META):
        try:
            if p.exists():
                p.unlink()
                removed.append(p.name)
        except Exception:
            pass
    await m.answer("üóë –ö—ç—à –æ—á–∏—â–µ–Ω: " + (", ".join(removed) if removed else "–Ω–∏—á–µ–≥–æ –Ω–µ –±—ã–ª–æ"))


@dp.message(Command("diag_whoami"))
async def diag_whoami(m: types.Message):
    import aiohttp, json
    try:
        base = (OBDSP_BASE or "https://proddsp.omniboard360.io").strip().rstrip("/")
        tok  = (OBDSP_TOKEN or "").strip().strip('"').strip("'")
        if not tok:
            await m.answer("OBDSP_TOKEN –ø—É—Å—Ç. –ó–∞–¥–∞–π –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è.")
            return

        url = f"{base}/api/v1.0/users/current"
        headers = {
            "Authorization": f"Bearer {tok}",
            "Accept": "application/json",
        }
        ssl_param = _make_ssl_param_for_aiohttp()
        timeout = aiohttp.ClientTimeout(total=30)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(url, headers=headers, ssl=_make_ssl_param_for_aiohttp()) as resp:
                text = await resp.text()
                await m.answer(f"GET {url}\nstatus={resp.status}\nbody={text[:800]}")
    except Exception as e:
        await m.answer(f"–û—à–∏–±–∫–∞: {e}")

@dp.message(Command("diag_env"))
async def diag_env(m: types.Message):
    tok = (OBDSP_TOKEN or "").strip()
    base = (OBDSP_BASE or "").strip()
    sslv = (os.getenv("OBDSP_SSL_VERIFY","") or "").strip()
    shown = f"{tok[:6]}‚Ä¶{tok[-6:]}" if tok else "(empty)"
    await m.answer(
        "ENV:\n"
        f"BASE={base}\n"
        f"TOKEN_LEN={len(tok)} TOKEN={shown}\n"
        f"OBDSP_SSL_VERIFY={sslv}\n"
    )

@dp.message(Command("diag_whoami_force"))
async def diag_whoami_force(m: types.Message):
    import aiohttp
    try:
        base = (OBDSP_BASE or "https://proddsp.omniboard360.io").rstrip("/")
        tok  = (OBDSP_TOKEN or "").strip().strip('"').strip("'")
        if not tok:
            await m.answer("OBDSP_TOKEN –ø—É—Å—Ç –≤–Ω—É—Ç—Ä–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞.")
            return
        url = f"{base}/api/v1.0/users/current"
        headers = {
            "Authorization": f"Bearer {tok}",
            "Accept": "application/json",
        }
        ssl_param = _make_ssl_param_for_aiohttp()
        timeout = aiohttp.ClientTimeout(total=20)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(url, headers=headers, ssl=_make_ssl_param_for_aiohttp()) as resp:
                text = await resp.text()
                await m.answer(
                    f"GET {url}\n"
                    f"sent_header=Authorization: Bearer <{len(tok)} chars>\n"
                    f"status={resp.status}\n"
                    f"body={text[:900]}"
                )
    except Exception as e:
        await m.answer(f"–û—à–∏–±–∫–∞: {e}")

@dp.message(Command("help"))
async def cmd_help(m: types.Message):
    await m.answer(HELP, reply_markup=make_main_menu())

@dp.message(Command("diag_url"))
async def cmd_diag_url(m: types.Message):
    base = (OBDSP_BASE or "").strip().rstrip("/")
    # –í–ê–ñ–ù–û: –±–µ–∑ clientId –≤ –ø—É—Ç–∏
    root = f"{base}/api/v1.0/clients/inventories"
    await m.answer(f"GET {root}\n(–ø—Ä–∏–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã) {root}?page=0&size=1")

@dp.message(Command("examples"))
async def cmd_examples(m: types.Message):
    text = (
        "üîç –ü—Ä–∏–º–µ—Ä—ã:\n"
        "‚Ä¢ /near 55.714349 37.553834 2\n"
        "‚Ä¢ /near 55.714349 37.553834 2 fields=screen_id\n"
        "‚Ä¢ /pick_city –ú–æ—Å–∫–≤–∞ 20 fields=screen_id\n"
        "‚Ä¢ /pick_city –ú–æ—Å–∫–≤–∞ 20 format=city fields=screen_id\n"
        "‚Ä¢ /pick_city –ú–æ—Å–∫–≤–∞ 20 format=billboard,supersite mix=billboard:70%,supersite:30% fields=screen_id\n"
        "‚Ä¢ /pick_at 55.75 37.62 25 15\n"
    )
    await m.answer(text, reply_markup=make_main_menu())

@dp.message(Command("radius"))
async def set_radius(m: types.Message):
    try:
        r = float(m.text.split()[1])
        if r <= 0 or r > 50: raise ValueError
        USER_RADIUS[m.from_user.id] = r
        await m.answer(f"–†–∞–¥–∏—É—Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {r:.2f} –∫–º")
    except:
        await m.answer("–£–∫–∞–∂–∏ —Ä–∞–¥–∏—É—Å –≤ –∫–º: /radius 2")

@dp.message(Command("near"))
async def cmd_near(m: types.Message):
    global LAST_RESULT, SCREENS
    if SCREENS is None or SCREENS.empty:
        await m.answer("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —ç–∫—Ä–∞–Ω–æ–≤ (CSV/XLSX).")
        return

    parts = m.text.strip().split()
    if len(parts) < 3:
        await m.answer("–§–æ—Ä–º–∞—Ç: /near lat lon [radius_km] [fields=screen_id]")
        return

    try:
        lat = float(parts[1]); lon = float(parts[2])
        radius = USER_RADIUS.get(m.from_user.id, DEFAULT_RADIUS)
        # 4-–π –∞—Ä–≥—É–º–µ–Ω—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å —á–∏—Å–ª–æ–º —Ä–∞–¥–∏—É—Å–∞ (–µ—Å–ª–∏ –±–µ–∑ '=')
        tail_from = 3
        if len(parts) >= 4 and "=" not in parts[3]:
            radius = float(parts[3].strip("[](){}"))
            tail_from = 4
        # –ø–∞—Ä—Å–∏–º –≤–æ–∑–º–æ–∂–Ω—ã–µ key=value (–≤–∫–ª—é—á–∞—è fields=...)
        kwargs = {}
        for p in parts[tail_from:]:
            if "=" in p:
                k, v = p.split("=", 1)
                kwargs[k.strip().lower()] = v.strip().strip('"').strip("'")
    except Exception:
        await m.answer("–ü—Ä–∏–º–µ—Ä: /near 55.714349 37.553834 2 fields=screen_id")
        return

    # –°—á–∏—Ç–∞–µ–º –∫—Ä—É–≥
    res = find_within_radius(SCREENS, (lat, lon), radius)
    if res is None or res.empty:
        await m.answer(f"–í —Ä–∞–¥–∏—É—Å–µ {radius} –∫–º –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        return

    LAST_RESULT = res

    # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å–∏–ª–∏ —Ç–æ–ª—å–∫–æ GUID'—ã
    if kwargs.get("fields", "").lower() == "screen_id":
        ids = [str(x) for x in res.get("screen_id", pd.Series([""]*len(res))).tolist()]
        if not ids:
            await m.answer(f"–ù–∞–π–¥–µ–Ω–æ {len(res)} —ç–∫—Ä., –Ω–æ –∫–æ–ª–æ–Ω–∫–∞ screen_id –ø—É—Å—Ç–∞—è.")
            return
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø–∞—á–∫–∏ –∏ —à–ª—ë–º –≤—Å—ë
        header = f"–ù–∞–π–¥–µ–Ω–æ {len(ids)} screen_id:"
        # send_lines(message, lines, header=None, chunk=60) ‚Äî –Ω–∞—à —Ö–µ–ª–ø–µ—Ä
        await send_lines(m, ids, header=header, chunk=60)
        return

    # –ò–Ω–∞—á–µ ¬´—á–µ–ª–æ–≤–µ—á–Ω—ã–π¬ª —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö (–±–µ–∑ —É—Å–µ—á–µ–Ω–∏—è)
    lines = []
    for _, r in res.iterrows():
        sid = r.get("screen_id", "")
        name = r.get("name", "")
        dist = r.get("distance_km", "")
        fmt  = r.get("format", "")
        own  = r.get("owner", "")
        lines.append(f"‚Ä¢ {sid} ‚Äî {name} ({dist} –∫–º) [{fmt} / {own}]")

    if not lines:
        # –§–æ–ª–±—ç–∫ –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –≤–¥—Ä—É–≥ –≤—Å—ë –ø—É—Å—Ç–æ
        await m.answer(f"–ù–∞–π–¥–µ–Ω–æ: {len(res)} —ç–∫—Ä. –≤ —Ä–∞–¥–∏—É—Å–µ {radius} –∫–º, –Ω–æ —Å–ø–∏—Å–æ–∫ –ø—É—Å—Ç. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–ª–æ–Ω–∫–∏.")
        # –ü–æ–∫–∞–∂–µ–º –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ –∫–∞–∫ CSV-–≤—Å—Ç–∞–≤–∫—É
        try:
            sample = res.head(5).to_csv(index=False)
            await m.answer(f"–ü–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ –¥–∞–Ω–Ω—ã—Ö:\n```\n{sample}\n```", parse_mode="Markdown")
        except Exception:
            pass
        return

    await send_lines(m, lines, header=f"–ù–∞–π–¥–µ–Ω–æ: {len(res)} —ç–∫—Ä. –≤ —Ä–∞–¥–∏—É—Å–µ {radius} –∫–º", chunk=60)

@dp.message(Command("sync_api"))
async def cmd_sync_api(m: types.Message):
    if not _owner_only(m.from_user.id):
        await m.answer("‚õîÔ∏è –¢–æ–ª—å–∫–æ –≤–ª–∞–¥–µ–ª–µ—Ü –±–æ—Ç–∞ –º–æ–∂–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å —ç—Ç—É –∫–æ–º–∞–Ω–¥—É.")
        return

    # --- —Ä–∞–∑–±–æ—Ä –æ–ø—Ü–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞ ---
    # –ø—Ä–∏–º–µ—Ä: /sync_api pages=3 size=500 limit=2000 city=–ú–æ—Å–∫–≤–∞ type=BILLBOARD ownerId=42
    text = (m.text or "").strip()
    parts = text.split()[1:]  # –≤—Å—ë –ø–æ—Å–ª–µ /sync_api

    def _get_opt(name, cast, default):
        for p in parts:
            if p.startswith(name + "="):
                val = p.split("=", 1)[1]
                try:
                    return cast(val)
                except:
                    return default
        return default

    pages_limit = _get_opt("pages", int, None)
    page_size   = _get_opt("size", int, 500)
    total_limit = _get_opt("limit", int, None)

    # —Å–µ—Ä–≤–µ—Ä–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã Omniboard:
    city         = _get_opt("city", str, None)           # –ø—Ä–∏–º–µ—Ä: –ú–æ—Å–∫–≤–∞
    typ          = _get_opt("type", str, None)           # –ø—Ä–∏–º–µ—Ä: BILLBOARD
    owner_id     = _get_opt("ownerId", str, None)        # –ø—Ä–∏–º–µ—Ä: 42 (—Å—Ç—Ä–æ–∫–æ–π —Ç–æ–∂–µ –æ–∫)
    placement    = _get_opt("placement", str, None)      # OUTDOOR / INDOOR ...
    installation = _get_opt("installation", str, None)   # STATIC / DIGITAL ...

    api_filters = {
        "city": city,
        "type": typ,
        "ownerId": owner_id,
        "placement": placement,
        "installation": installation,
    }

    await m.answer("‚è≥ –¢—è–Ω—É –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å –∏–∑ –≤–Ω–µ—à–Ω–µ–≥–æ API‚Ä¶")

    try:
        items = await _fetch_inventories(
            pages_limit=pages_limit,
            page_size=page_size,
            total_limit=total_limit,
            m=m,
            filters=api_filters,   # <--- –í–ê–ñ–ù–û: –ø–µ—Ä–µ–¥–∞—ë–º —Ñ–∏–ª—å—Ç—Ä—ã –Ω–∞ —Å–µ—Ä–≤–µ—Ä
        )
    except Exception as e:
        logging.exception("sync_api failed")
        await m.answer(f"üö´ –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∏–Ω–∫–Ω—É—Ç—å: {e}")
        return

    if not items:
        await m.answer("API –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫.")
        return

    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ –¥–∏—Å–∫ (–ø–æ–ª–µ–∑–Ω–æ) –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∞–π–ª—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
    try:
        df.to_excel("screens_from_api.xlsx", index=False)
    except Exception:
        pass

    # CSV
    try:
        csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
        await bot.send_document(
            m.chat.id,
            BufferedInputFile(csv_bytes, filename="inventories_sync.csv"),
            caption=f"–ò–Ω–≤–µ–Ω—Ç–∞—Ä—å –∏–∑ API: {len(df)} —Å—Ç—Ä–æ–∫ (CSV)"
        )
    except Exception as e:
        await m.answer(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å CSV: {e}")

    # XLSX
    try:
        xlsx_buf = io.BytesIO()
        with pd.ExcelWriter(xlsx_buf, engine="openpyxl") as writer:
            df.to_excel(writer, index=False, sheet_name="inventories")
        xlsx_buf.seek(0)
        await bot.send_document(
            m.chat.id,
            BufferedInputFile(xlsx_buf.getvalue(), filename="inventories_sync.xlsx"),
            caption=f"–ò–Ω–≤–µ–Ω—Ç–∞—Ä—å –∏–∑ API: {len(df)} —Å—Ç—Ä–æ–∫ (XLSX)"
        )
    except Exception as e:
        await m.answer(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å XLSX: {e} (–ø—Ä–æ–≤–µ—Ä—å –ø–∞–∫–µ—Ç openpyxl)")

    await m.answer("–ì–æ—Ç–æ–≤–æ! –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è /near, /pick_city –∏ –¥—Ä. –ø–æ –¥–∞–Ω–Ω—ã–º –∏–∑ API ‚ú®")

@dp.message(Command("pick_city"))
async def pick_city(m: types.Message):
    global LAST_RESULT, SCREENS
    if SCREENS is None or SCREENS.empty:
        await m.answer("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å (CSV/XLSX –∏–ª–∏ /sync_api).")
        return

    parts = (m.text or "").strip().split()
    if len(parts) < 3:
        await m.answer("–§–æ—Ä–º–∞—Ç: /pick_city –ì–æ—Ä–æ–¥ N [format=...] [owner=...] [fields=...] [shuffle=1] [fixed=1] [seed=42]")
        return

    # –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∏ –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    pos, keyvals = [], []
    for p in parts[1:]:
        (keyvals if "=" in p else pos).append(p)
    if not pos:
        await m.answer("–ù—É–∂–Ω—ã –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: –ì–æ—Ä–æ–¥ N")
        return

    try:
        n = int(pos[-1])
        city = " ".join(pos[:-1]) if len(pos) > 1 else ""
        kwargs = parse_kwargs(keyvals)
        shuffle_flag = str(kwargs.get("shuffle", "0")).lower() in {"1", "true", "yes", "on"}
        fixed        = str(kwargs.get("fixed",   "0")).lower() in {"1", "true", "yes", "on"}
        seed         = int(kwargs["seed"]) if str(kwargs.get("seed","")).isdigit() else None
    except Exception:
        await m.answer("–ü—Ä–∏–º–µ—Ä: /pick_city –ú–æ—Å–∫–≤–∞ 20 format=BILLBOARD fields=screen_id shuffle=1")
        return

    if "city" not in SCREENS.columns:
        await m.answer("–í –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç —Å—Ç–æ–ª–±—Ü–∞ city. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /near –∏–ª–∏ /sync_api —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π.")
        return

    # —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≥–æ—Ä–æ–¥—É + –¥–æ–ø. —Ñ–∏–ª—å—Ç—Ä—ã
    subset = SCREENS[SCREENS["city"].astype(str).str.strip().str.lower() == city.strip().lower()]
    subset = apply_filters(subset, kwargs) if not subset.empty and kwargs else subset

    if subset.empty:
        await m.answer(f"–ù–µ –Ω–∞—à—ë–ª —ç–∫—Ä–∞–Ω–æ–≤ –≤ –≥–æ—Ä–æ–¥–µ: {city} (—Å —É—á—ë—Ç–æ–º —Ñ–∏–ª—å—Ç—Ä–æ–≤).")
        return

    # –ª—ë–≥–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–¥ k-center
    if shuffle_flag:
        subset = subset.sample(frac=1, random_state=None).reset_index(drop=True)

    # —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä
    res = spread_select(
        subset.reset_index(drop=True),
        n,
        random_start=not fixed,
        seed=seed
    )
    LAST_RESULT = res

    # –≤—ã–≤–æ–¥
    fields = parse_fields(kwargs.get("fields","")) if "fields" in kwargs else []

    if fields:
        view = res[fields]

        # —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–π –æ—Ç–≤–µ—Ç –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –ø–æ–ª—è–º
        if fields == ["screen_id"]:
            ids = [str(x) for x in view["screen_id"].tolist()]
            await send_lines(m, ids, header=f"–í—ã–±—Ä–∞–Ω–æ {len(ids)} screen_id –ø–æ –≥–æ—Ä–æ–¥—É ¬´{city}¬ª:")
        else:
            lines = [" | ".join(str(row[c]) for c in fields) for _, row in view.iterrows()]
            await send_lines(m, lines, header=f"–í—ã–±—Ä–∞–Ω–æ {len(view)} —ç–∫—Ä–∞–Ω–æ–≤ –ø–æ –≥–æ—Ä–æ–¥—É ¬´{city}¬ª (–ø–æ–ª—è: {', '.join(fields)}):")
    else:
        # –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–π —Å–ø–∏—Å–æ–∫
        lines = []
        for _, r in res.iterrows():
            nm  = r.get("name","") or r.get("screen_id","")
            fmt = r.get("format","") or ""
            own = r.get("owner","") or ""
            md  = r.get("min_dist_to_others_km", None)
            tail = f"(–º–∏–Ω. –¥–æ —Å–æ—Å–µ–¥–∞ {md} –∫–º)" if md is not None else ""
            lines.append(f"‚Ä¢ {r.get('screen_id','')} ‚Äî {nm} [{r['lat']:.5f},{r['lon']:.5f}] [{fmt} / {own}] {tail}".strip())
        await send_lines(m, lines, header=f"–í—ã–±—Ä–∞–Ω–æ {len(res)} —ç–∫—Ä–∞–Ω–æ–≤ –ø–æ –≥–æ—Ä–æ–¥—É ¬´{city}¬ª (—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ):")

    # –≤—Å–µ–≥–¥–∞ –ø—Ä–∏–∫–ª–∞–¥—ã–≤–∞–µ–º XLSX —Å –∫–æ–ª–æ–Ω–∫–æ–π GID (screen_id)
    await send_gid_if_any(
        m,
        res,
        filename="city_screen_ids.xlsx",
        caption=f"GID –ø–æ –≥–æ—Ä–æ–¥—É ¬´{city}¬ª (XLSX)"
    )

@dp.message(Command("pick_at"))
async def pick_at(m: types.Message):
    global LAST_RESULT, SCREENS
    if SCREENS is None or SCREENS.empty:
        await m.answer("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —ç–∫—Ä–∞–Ω–æ–≤ (CSV/XLSX).")
        return
    parts = m.text.strip().split()
    if len(parts) < 4:
        await m.answer("–§–æ—Ä–º–∞—Ç: /pick_at lat lon N [radius_km]")
        return
    try:
        lat, lon = float(parts[1]), float(parts[2])
        n = int(parts[3])
        radius = float(parts[4]) if len(parts) >= 5 and "=" not in parts[4] else 20.0
    except:
        await m.answer("–ü—Ä–∏–º–µ—Ä: /pick_at 55.75 37.62 30 15")
        return
    circle = find_within_radius(SCREENS, (lat, lon), radius)
    if circle.empty:
        await m.answer(f"–í —Ä–∞–¥–∏—É—Å–µ {radius} –∫–º –Ω–µ—Ç —ç–∫—Ä–∞–Ω–æ–≤.")
        return

 # –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏, –∫–∞–∫ –º—ã –¥–µ–ª–∞–ª–∏ —Ä–∞–Ω–µ–µ
    fixed = str(kwargs.get("fixed", "0")).lower() in {"1", "true", "yes", "on"}
    seed = int(kwargs["seed"]) if "seed" in kwargs and kwargs["seed"].isdigit() else None

    # –Ω–æ–≤—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä mix=...
    mix_arg = kwargs.get("mix") or kwargs.get("mix_formats")

    res = _select_with_mix(
        subset.reset_index(drop=True),
        n,
        mix_arg,
        random_start=not fixed,
        seed=seed
    )
    LAST_RESULT = res

    lines = []
    for _, r in res.iterrows():
        nm = r.get("name","") or r.get("screen_id","")
        fmt = r.get("format",""); own = r.get("owner","")
        lines.append(f"‚Ä¢ {r.get('screen_id','')} ‚Äî {nm} [{r['lat']:.5f},{r['lon']:.5f}] [{fmt} / {own}] (–º–∏–Ω. –¥–æ —Å–æ—Å–µ–¥–∞ {r['min_dist_to_others_km']} –∫–º)")
    await send_lines(m, lines, header=f"–í—ã–±—Ä–∞–Ω–æ {len(res)} —ç–∫—Ä–∞–Ω–æ–≤ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –≤ —Ä–∞–¥–∏—É—Å–µ {radius} –∫–º:")

    await send_gid_if_any(
        m,
        res,
        filename="picked_at_screen_ids.xlsx",
        caption="GID (XLSX)"
    )

@dp.message(Command("export_last"))
async def export_last(m: types.Message):
    global LAST_RESULT
    if LAST_RESULT is None or LAST_RESULT.empty:
        await m.answer("–ü–æ–∫–∞ –Ω–µ—á–µ–≥–æ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å. –°–Ω–∞—á–∞–ª–∞ —Å–¥–µ–ª–∞–π—Ç–µ –≤—ã–±–æ—Ä–∫—É (/near, /pick_city, /pick_at).")
        return
    csv_bytes = LAST_RESULT.to_csv(index=False).encode("utf-8-sig")
    await bot.send_document(
        m.chat.id,
        BufferedInputFile(csv_bytes, filename="selection.csv"),
        caption="–ü–æ—Å–ª–µ–¥–Ω—è—è –≤—ã–±–æ—Ä–∫–∞ (CSV)",
    )

@dp.message(Command("diag_env"))
async def cmd_diag_env(m: types.Message):
    base = OBDSP_BASE
    cid  = OBDSP_CLIENT_ID
    tok_len = len(OBDSP_TOKEN or "")
    await m.answer(f"BASE={base}\nCLIENT_ID={cid}\nTOKEN_LEN={tok_len}")

@dp.message(F.content_type.in_({"document"}))
async def on_file(m: types.Message):
    """–ü—Ä–∏–Ω–∏–º–∞–µ–º CSV/XLSX –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å."""
    try:
        file = await bot.get_file(m.document.file_id)
        file_bytes = await bot.download_file(file.file_path)
        data = file_bytes.read()

        # —á–∏—Ç–∞–µ–º CSV/XLSX
        if m.document.file_name.lower().endswith(".xlsx"):
            df = pd.read_excel(io.BytesIO(data))
        else:
            try:
                df = pd.read_csv(io.BytesIO(data), encoding="utf-8-sig")
            except:
                df = pd.read_csv(io.BytesIO(data))

        # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–ª–æ–Ω–æ–∫
        rename_map = {
            "Screen_ID": "screen_id", "ScreenId": "screen_id", "id": "screen_id", "ID": "screen_id",
            "Name": "name", "–ù–∞–∑–≤–∞–Ω–∏–µ": "name",
            "Latitude": "lat", "Lat": "lat", "–®–∏—Ä–æ—Ç–∞": "lat",
            "Longitude": "lon", "Lon": "lon", "–î–æ–ª–≥–æ—Ç–∞": "lon",
            "City": "city", "–ì–æ—Ä–æ–¥": "city",
            "Format": "format", "–§–æ—Ä–º–∞—Ç": "format",
            "Owner": "owner", "–í–ª–∞–¥–µ–ª–µ—Ü": "owner", "–û–ø–µ—Ä–∞—Ç–æ—Ä": "owner"
        }
        df = df.rename(columns=rename_map)

        if not {"lat","lon"}.issubset(df.columns):
            await m.answer("–ù—É–∂–Ω—ã –∫–æ–ª–æ–Ω–∫–∏ –º–∏–Ω–∏–º—É–º: lat, lon. (–û–ø—Ü.: screen_id, name, city, format, owner)")
            return

        # –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤
        df["lat"] = pd.to_numeric(df["lat"], errors="coerce")
        df["lon"] = pd.to_numeric(df["lon"], errors="coerce")
        df = df.dropna(subset=["lat","lon"])

        # –∑–∞–ø–æ–ª–Ω—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ
        for col in ["screen_id","name","city","format","owner"]:
            if col not in df.columns:
                df[col] = ""

        global SCREENS
        SCREENS = df[["screen_id","name","lat","lon","city","format","owner"]].reset_index(drop=True)
        await m.answer(
            f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —ç–∫—Ä–∞–Ω–æ–≤: {len(SCREENS)}.\n"
            "–¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ: –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –≥–µ–æ–ª–æ–∫–∞—Ü–∏—é üìç, /near lat lon [R], /pick_city –ì–æ—Ä–æ–¥ N, /pick_at lat lon N [R]."
        )
    except Exception as e:
        await m.answer(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª: {e}")

@dp.message(F.text)
async def fallback_text(m: types.Message):
    t = (m.text or "").strip()
    if t.startswith("/"):
        # –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞ ‚Äî –ø–æ–∫–∞–∂–µ–º help
        await m.answer("–Ø –≤–∞—Å –ø–æ–Ω—è–ª, –Ω–æ —Ç–∞–∫–æ–π –∫–æ–º–∞–Ω–¥—ã –Ω–µ—Ç. –ù–∞–∂–º–∏—Ç–µ /help –¥–ª—è —Å–ø–∏—Å–∫–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π.", reply_markup=make_main_menu())
    else:
        # —Å–≤–æ–±–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç ‚Äî –º—è–≥–∫–æ –Ω–∞–ø—Ä–∞–≤–∏–º
        await m.answer(
            "–ß—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å, –ø—Ä–∏—à–ª–∏—Ç–µ —Ñ–∞–π–ª CSV/XLSX —Å —ç–∫—Ä–∞–Ω–∞–º–∏, –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ /near, /pick_city, /pick_at.\n"
            "–ù–∞–∂–º–∏—Ç–µ /help, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å –ø—Ä–∏–º–µ—Ä—ã.",
            reply_markup=make_main_menu()
        )

# ====== –ó–ê–ü–£–°–ö ======
async def main():
    # –≤—ã–∫–ª—é—á–∞–µ–º webhook –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π, —á—Ç–æ–±—ã –Ω–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤–∞–ª —Å polling
    await bot.delete_webhook(drop_pending_updates=True)
    me = await bot.get_me()
    logging.info(f"‚úÖ –ë–æ—Ç @{me.username} –∑–∞–ø—É—â–µ–Ω –∏ –∂–¥—ë—Ç —Å–æ–æ–±—â–µ–Ω–∏–π‚Ä¶")
    await dp.start_polling(bot, allowed_updates=["message"])

if __name__ == "__main__":
    asyncio.run(main())